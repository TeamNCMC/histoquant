var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>Info</p> <p>The documentation is under construction.</p> <p><code>cuisto</code> is a Python package aiming at quantifying histological data.</p> <p>After ABBA registration of 2D histological slices and QuPath objects' detection, <code>cuisto</code> is used to :</p> <ul> <li>compute metrics, such as objects density in each brain regions,</li> <li>compute objects distributions in three three axes (rostro-caudal, dorso-ventral and medio-lateral),</li> <li>compute averages and sem across animals,</li> <li>displaying all the above.</li> </ul> <p>This documentation contains <code>cuisto</code> installation instructions, ABBA installation instructions, guides to prepare images for the pipeline, detect objects with QuPath, register 2D slices on a 3D atlas with ABBA, along with examples.</p> <p>In theory, <code>cuisto</code> should work with any measurements table with the required columns, but has been designed with ABBA and QuPath in mind.</p> <p>Due to the IT environment of the laboratory, this documentation is very Windows-oriented but most of it should be applicable to Linux and MacOS as well by slightly adapting terminal commands.</p> <p></p>"},{"location":"index.html#documentation-navigation","title":"Documentation navigation","text":"<p>The documentation outline is on the left panel, you can click on items to browse it. In each page, you'll get the table of contents on the right panel.</p>"},{"location":"index.html#useful-external-resources","title":"Useful external resources","text":"<ul> <li>Project repository : https://github.com/TeamNCMC/cuisto</li> <li>QuPath documentation : https://qupath.readthedocs.io/en/stable/</li> <li>Aligning Big Brain and Atlases (ABBA) documentation : https://abba-documentation.readthedocs.io/en/latest/</li> <li>Brainglobe : https://brainglobe.info/</li> <li>BraiAn, a similar but published and way more feature-packed project : https://silvalab.codeberg.page/BraiAn/</li> <li>Image.sc community forum : https://forum.image.sc/</li> <li>Introduction to Bioimage Analysis, an interactive book written by QuPath's creator : https://bioimagebook.github.io/index.html</li> </ul>"},{"location":"index.html#credits","title":"Credits","text":"<p><code>cuisto</code> has been primarly developed by Guillaume Le Goc in Julien Bouvier's lab at NeuroPSI. The clever name was found by Aur\u00e9lie Bodeau.</p> <p>The documentation itself is built with MkDocs using the Material theme.</p>"},{"location":"api-compute.html","title":"cuisto.compute","text":"<p>compute module, part of cuisto.</p> <p>Contains actual computation functions.</p>"},{"location":"api-compute.html#cuisto.compute.get_distribution","title":"<code>get_distribution(df, col, hue, hue_filter, per_commonnorm, binlim, nbins=100)</code>","text":"<p>Computes distribution of objects.</p> <p>A global distribution using only <code>col</code> is computed, then it computes a distribution distinguishing values in the <code>hue</code> column. For the latter, it is possible to use a subset of the data ony, based on another column using <code>hue_filter</code>. This another column is determined with <code>hue</code>, if the latter is \"hemisphere\", then <code>hue_filter</code> is used in the \"channel\" color and vice-versa. <code>per_commonnorm</code> controls how they are normalized, either as a whole (True) or independantly (False).</p> <p>Use cases : (1) single-channel, two hemispheres : <code>col=x</code>, <code>hue=hemisphere</code>, <code>hue_filter=\"\"</code>, <code>per_commonorm=True</code>. Computes a distribution for each hemisphere, the sum of the area of both is equal to 1. (2) three-channels, one hemisphere : <code>col=x</code>, hue=<code>channel</code>, <code>hue_filter=\"Ipsi.\", per_commonnorm=False</code>. Computes a distribution for each channel only for points in the ipsilateral hemisphere. Each curve will have an area of 1.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <code>col</code> <code>str</code> <p>Key in <code>df</code>, used to compute the distributions.</p> required <code>hue</code> <code>str</code> <p>Key in <code>df</code>. Criterion for additional distributions.</p> required <code>hue_filter</code> <code>str</code> <p>Further filtering for \"per\" distribution. - hue = channel : value is the name of one of the hemisphere - hue = hemisphere : value can be the name of a channel, a list of such or \"all\"</p> required <code>per_commonnorm</code> <code>bool</code> <p>Use common normalization for all hues (per argument).</p> required <code>binlim</code> <code>list or tuple</code> <p>First bin left edge and last bin right edge.</p> required <code>nbins</code> <code>int</code> <p>Number of bins. Default is 100.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>df_distribution</code> <code>DataFrame</code> <p>DataFrame with <code>bins</code>, <code>distribution</code>, <code>count</code> and their per-hemisphere or per-channel variants.</p> Source code in <code>cuisto/compute.py</code> <pre><code>def get_distribution(\n    df: pd.DataFrame,\n    col: str,\n    hue: str,\n    hue_filter: dict,\n    per_commonnorm: bool,\n    binlim: tuple | list,\n    nbins=100,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes distribution of objects.\n\n    A global distribution using only `col` is computed, then it computes a distribution\n    distinguishing values in the `hue` column. For the latter, it is possible to use a\n    subset of the data ony, based on another column using `hue_filter`. This another\n    column is determined with `hue`, if the latter is \"hemisphere\", then `hue_filter` is\n    used in the \"channel\" color and vice-versa.\n    `per_commonnorm` controls how they are normalized, either as a whole (True) or\n    independantly (False).\n\n    Use cases :\n    (1) single-channel, two hemispheres : `col=x`, `hue=hemisphere`, `hue_filter=\"\"`,\n    `per_commonorm=True`. Computes a distribution for each hemisphere, the sum of the\n    area of both is equal to 1.\n    (2) three-channels, one hemisphere : `col=x`, hue=`channel`,\n    `hue_filter=\"Ipsi.\", per_commonnorm=False`. Computes a distribution for each channel\n    only for points in the ipsilateral hemisphere. Each curve will have an area of 1.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n    col : str\n        Key in `df`, used to compute the distributions.\n    hue : str\n        Key in `df`. Criterion for additional distributions.\n    hue_filter : str\n        Further filtering for \"per\" distribution.\n        - hue = channel : value is the name of one of the hemisphere\n        - hue = hemisphere : value can be the name of a channel, a list of such or \"all\"\n    per_commonnorm : bool\n        Use common normalization for all hues (per argument).\n    binlim : list or tuple\n        First bin left edge and last bin right edge.\n    nbins : int, optional\n        Number of bins. Default is 100.\n\n    Returns\n    -------\n    df_distribution : pandas.DataFrame\n        DataFrame with `bins`, `distribution`, `count` and their per-hemisphere or\n        per-channel variants.\n\n    \"\"\"\n\n    # - Preparation\n    bin_edges = np.linspace(*binlim, nbins + 1)  # create bins\n    df_distribution = []  # prepare list of distributions\n\n    # - Both hemispheres, all channels\n    # get raw count per bins (histogram)\n    count, bin_edges = np.histogram(df[col], bin_edges)\n    # get normalized count (pdf)\n    distribution, _ = np.histogram(df[col], bin_edges, density=True)\n    # get bin centers rather than edges to plot them\n    bin_centers = bin_edges[:-1] + np.diff(bin_edges) / 2\n\n    # make a DataFrame out of that\n    df_distribution.append(\n        pd.DataFrame(\n            {\n                \"bins\": bin_centers,\n                \"distribution\": distribution,\n                \"count\": count,\n                \"hemisphere\": \"both\",\n                \"channel\": \"all\",\n                \"axis\": col,  # keep track of what col. was used\n            }\n        )\n    )\n\n    # - Per additional criterion\n    # select data\n    df_sub = select_hemisphere_channel(df, hue, hue_filter, False)\n    hue_values = df[hue].unique()  # get grouping values\n    # total number of datapoints in the subset used for additional distribution\n    length_total = len(df_sub)\n\n    for value in hue_values:\n        # select part and coordinates\n        df_part = df_sub.loc[df_sub[hue] == value, col]\n\n        # get raw count per bins (histogram)\n        count, bin_edges = np.histogram(df_part, bin_edges)\n        # get normalized count (pdf)\n        distribution, _ = np.histogram(df_part, bin_edges, density=True)\n\n        if per_commonnorm:\n            # re-normalize so that the sum of areas of all sub-parts is 1\n            length_part = len(df_part)  # number of datapoints in that hemisphere\n            distribution *= length_part / length_total\n\n        # get bin centers rather than edges to plot them\n        bin_centers = bin_edges[:-1] + np.diff(bin_edges) / 2\n\n        # make a DataFrame out of that\n        df_distribution.append(\n            pd.DataFrame(\n                {\n                    \"bins\": bin_centers,\n                    \"distribution\": distribution,\n                    \"count\": count,\n                    hue: value,\n                    \"channel\" if hue == \"hemisphere\" else \"hemisphere\": hue_filter,\n                    \"axis\": col,  # keep track of what col. was used\n                }\n            )\n        )\n\n    return pd.concat(df_distribution)\n</code></pre>"},{"location":"api-compute.html#cuisto.compute.get_regions_metrics","title":"<code>get_regions_metrics(df_annotations, object_type, channel_names, meas_base_name, metrics_names)</code>","text":"<p>Get a new DataFrame with cumulated axons segments length in each brain regions.</p> <p>This is the quantification per brain regions for fibers-like objects, eg. axons. The returned DataFrame has columns \"cum. length \u00b5m\", \"cum. length mm\", \"density \u00b5m^-1\", \"density mm^-1\", \"coverage index\".</p> <p>Parameters:</p> Name Type Description Default <code>df_annotations</code> <code>DataFrame</code> <p>DataFrame with an entry for each brain regions, with columns \"Area \u00b5m^2\", \"Name\", \"hemisphere\", and \"{object_type: channel} Length \u00b5m\".</p> required <code>object_type</code> <code>str</code> <p>Object type (primary classification).</p> required <code>channel_names</code> <code>dict</code> <p>Map between original channel names to something else.</p> required <code>meas_base_name</code> <code>str</code> required <code>metrics_names</code> <code>dict</code> required <p>Returns:</p> Name Type Description <code>df_regions</code> <code>DataFrame</code> <p>DataFrame with brain regions name, area and metrics.</p> Source code in <code>cuisto/compute.py</code> <pre><code>def get_regions_metrics(\n    df_annotations: pd.DataFrame,\n    object_type: str,\n    channel_names: dict,\n    meas_base_name: str,\n    metrics_names: dict,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Get a new DataFrame with cumulated axons segments length in each brain regions.\n\n    This is the quantification per brain regions for fibers-like objects, eg. axons. The\n    returned DataFrame has columns \"cum. length \u00b5m\", \"cum. length mm\", \"density \u00b5m^-1\",\n    \"density mm^-1\", \"coverage index\".\n\n    Parameters\n    ----------\n    df_annotations : pandas.DataFrame\n        DataFrame with an entry for each brain regions, with columns \"Area \u00b5m^2\",\n        \"Name\", \"hemisphere\", and \"{object_type: channel} Length \u00b5m\".\n    object_type : str\n        Object type (primary classification).\n    channel_names : dict\n        Map between original channel names to something else.\n    meas_base_name : str\n    metrics_names : dict\n\n    Returns\n    -------\n    df_regions : pandas.DataFrame\n        DataFrame with brain regions name, area and metrics.\n\n    \"\"\"\n    # get columns names\n    cols = df_annotations.columns\n    # get columns with fibers lengths\n    cols_colors = cols[\n        cols.str.startswith(object_type) &amp; cols.str.endswith(meas_base_name)\n    ]\n    # select relevant data\n    cols_to_select = pd.Index([\"Name\", \"hemisphere\", \"Area \u00b5m^2\"]).append(cols_colors)\n    # sum lengths and areas of each brain regions\n    df_regions = (\n        df_annotations[cols_to_select]\n        .groupby([\"Name\", \"hemisphere\"])\n        .sum()\n        .reset_index()\n    )\n\n    # get measurement for both hemispheres (sum)\n    df_both = df_annotations[cols_to_select].groupby([\"Name\"]).sum().reset_index()\n    df_both[\"hemisphere\"] = \"both\"\n    df_regions = (\n        pd.concat([df_regions, df_both], ignore_index=True)\n        .sort_values(by=\"Name\")\n        .reset_index()\n        .drop(columns=\"index\")\n    )\n\n    # rename measurement columns to lower case\n    df_regions = df_regions.rename(\n        columns={\n            k: k.replace(meas_base_name, meas_base_name.lower()) for k in cols_colors\n        }\n    )\n\n    # update names\n    meas_base_name = meas_base_name.lower()\n    cols = df_regions.columns\n    cols_colors = cols[\n        cols.str.startswith(object_type) &amp; cols.str.endswith(meas_base_name)\n    ]\n\n    # convert area in mm^2\n    df_regions[\"Area mm^2\"] = df_regions[\"Area \u00b5m^2\"] / 1e6\n\n    # prepare metrics\n    if \"\u00b5m\" in meas_base_name:\n        # fibers : convert to mm\n        cols_to_convert = pd.Index([col for col in cols_colors if \"\u00b5m\" in col])\n        df_regions[cols_to_convert.str.replace(\"\u00b5m\", \"mm\")] = (\n            df_regions[cols_to_convert] / 1000\n        )\n        metrics = [meas_base_name, meas_base_name.replace(\"\u00b5m\", \"mm\")]\n    else:\n        # objects : count\n        metrics = [meas_base_name]\n\n    # density = measurement / area\n    metric = metrics_names[\"density \u00b5m^-2\"]\n    df_regions[cols_colors.str.replace(meas_base_name, metric)] = df_regions[\n        cols_colors\n    ].divide(df_regions[\"Area \u00b5m^2\"], axis=0)\n    metrics.append(metric)\n    metric = metrics_names[\"density mm^-2\"]\n    df_regions[cols_colors.str.replace(meas_base_name, metric)] = df_regions[\n        cols_colors\n    ].divide(df_regions[\"Area mm^2\"], axis=0)\n    metrics.append(metric)\n\n    # coverage index = measurement\u00b2 / area\n    metric = metrics_names[\"coverage index\"]\n    df_regions[cols_colors.str.replace(meas_base_name, metric)] = (\n        df_regions[cols_colors].pow(2).divide(df_regions[\"Area \u00b5m^2\"], axis=0)\n    )\n    metrics.append(metric)\n\n    # prepare relative metrics columns\n    metric = metrics_names[\"relative measurement\"]\n    cols_rel_meas = cols_colors.str.replace(meas_base_name, metric)\n    df_regions[cols_rel_meas] = np.nan\n    metrics.append(metric)\n    metric = metrics_names[\"relative density\"]\n    cols_dens = cols_colors.str.replace(meas_base_name, metrics_names[\"density mm^-2\"])\n    cols_rel_dens = cols_colors.str.replace(meas_base_name, metric)\n    df_regions[cols_rel_dens] = np.nan\n    metrics.append(metric)\n    # relative metrics should be defined within each hemispheres (left, right, both)\n    for hemisphere in df_regions[\"hemisphere\"].unique():\n        row_indexer = df_regions[\"hemisphere\"] == hemisphere\n\n        # relative measurement = measurement / total measurement\n        df_regions.loc[row_indexer, cols_rel_meas] = (\n            df_regions.loc[row_indexer, cols_colors]\n            .divide(df_regions.loc[row_indexer, cols_colors].sum())\n            .to_numpy()\n        )\n\n        # relative density = density / total density\n        df_regions.loc[row_indexer, cols_rel_dens] = (\n            df_regions.loc[\n                row_indexer,\n                cols_dens,\n            ]\n            .divide(df_regions.loc[row_indexer, cols_dens].sum())\n            .to_numpy()\n        )\n\n    # collect channel names\n    channels = (\n        cols_colors.str.replace(object_type + \": \", \"\")\n        .str.replace(\" \" + meas_base_name, \"\")\n        .values.tolist()\n    )\n    # collect measurements columns names\n    cols_metrics = df_regions.columns.difference(\n        pd.Index([\"Name\", \"hemisphere\", \"Area \u00b5m^2\", \"Area mm^2\"])\n    )\n    for metric in metrics:\n        cols_to_cat = [f\"{object_type}: {cn} {metric}\" for cn in channels]\n        # make sure it's part of available metrics\n        if not set(cols_to_cat) &lt;= set(cols_metrics):\n            raise ValueError(f\"{cols_to_cat} not in DataFrame.\")\n        # group all colors in the same colors\n        df_regions[metric] = df_regions[cols_to_cat].values.tolist()\n        # remove original data\n        df_regions = df_regions.drop(columns=cols_to_cat)\n\n    # add a color tag, given their names in the configuration file\n    df_regions[\"channel\"] = len(df_regions) * [[channel_names[k] for k in channels]]\n    metrics.append(\"channel\")\n\n    # explode the dataframe so that each color has an entry\n    df_regions = df_regions.explode(metrics)\n\n    return df_regions\n</code></pre>"},{"location":"api-compute.html#cuisto.compute.normalize_starter_cells","title":"<code>normalize_starter_cells(df, cols, animal, info_file, channel_names)</code>","text":"<p>Normalize data by the number of starter cells.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Contains the data to be normalized.</p> required <code>cols</code> <code>list - like</code> <p>Columns to divide by the number of starter cells.</p> required <code>animal</code> <code>str</code> <p>Animal ID to parse the number of starter cells.</p> required <code>info_file</code> <code>str</code> <p>Full path to the TOML file with informations.</p> required <code>channel_names</code> <code>dict</code> <p>Map between original channel names to something else.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Same <code>df</code> with normalized count.</p> Source code in <code>cuisto/compute.py</code> <pre><code>def normalize_starter_cells(\n    df: pd.DataFrame, cols: list[str], animal: str, info_file: str, channel_names: dict\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Normalize data by the number of starter cells.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Contains the data to be normalized.\n    cols : list-like\n        Columns to divide by the number of starter cells.\n    animal : str\n        Animal ID to parse the number of starter cells.\n    info_file : str\n        Full path to the TOML file with informations.\n    channel_names : dict\n        Map between original channel names to something else.\n\n    Returns\n    -------\n    pd.DataFrame\n        Same `df` with normalized count.\n\n    \"\"\"\n    for channel in df[\"channel\"].unique():\n        # inverse mapping channel colors : names\n        reverse_channels = {v: k for k, v in channel_names.items()}\n        nstarters = get_starter_cells(animal, reverse_channels[channel], info_file)\n\n        for col in cols:\n            df.loc[df[\"channel\"] == channel, col] = (\n                df.loc[df[\"channel\"] == channel, col] / nstarters\n            )\n\n    return df\n</code></pre>"},{"location":"api-config-config.html","title":"Api config config","text":"<p><code>object_type</code> : name of QuPath base classification (eg. without the \": subclass\" part) <code>segmentation_tag</code> : type of segmentation, matches directory name, used only in the full pipeline</p> atlas <p>Information related to the atlas used</p> <p><code>name</code> : brainglobe-atlasapi atlas name <code>type</code> : \"brain\" or \"cord\" (eg. registration done in ABBA or abba_python). This will determine whether to flip Left/Right when determining detections hemisphere based on their coordinates. Also adapts the axes in the 2D heatmaps. <code>midline</code> : midline Z coordinates (left/right limit) in microns to determine detections hemisphere based on their coordinates. <code>outline_structures</code> : structures to show an outline of in heatmaps  </p> channels <p>Information related to imaging channels</p> names <p>Must contain all classifications derived from \"object_type\" you want to process. In the form <code>subclassification name = name to display on the plots</code></p> <p><code>\"marker+\"</code> : classification name = name to display <code>\"marker-\"</code> : add any number of sub-classification</p> colors <p>Must have same keys as \"names\" keys, in the form <code>subclassification name = color</code>, with color specified as a matplotlib named color, an RGB list or an hex code.</p> <p><code>\"marker+\"</code> : classification name = matplotlib color <code>\"marker-\"</code> : must have the same entries as \"names\".</p> hemispheres <p>Information related to hemispheres, same structure as channels</p> names <p><code>Left</code> : Left = name to display <code>Right</code> : Right = name to display</p> colors <p>Must have same keys as names' keys</p> <p><code>Left</code> : ff516e\"  # Left = matplotlib color (either #hex, color name or RGB list) <code>Right</code> : 960010\"  # Right = matplotlib color</p> distributions <p>Spatial distributions parameters</p> <p><code>stereo</code> : use stereotaxic coordinates (as in Paxinos, only for mouse brain CCFv3) <code>ap_lim</code> : bins limits for anterio-posterior in mm <code>ap_nbins</code> : number of bins for anterio-posterior <code>dv_lim</code> : bins limits for dorso-ventral in mm <code>dv_nbins</code> : number of bins for dorso-ventral <code>ml_lim</code> : bins limits for medio-lateral in mm <code>ml_nbins</code> : number of bins for medio-lateral <code>hue</code> : color curves with this parameter, must be \"hemisphere\" or \"channel\" <code>hue_filter</code> : use only a subset of data</p> <ul> <li>If hue=hemisphere : it should be a channel name, a list of such or \"all\"  </li> <li>If hue=channel : it should be a hemisphere name or \"both\"</li> </ul> <p><code>common_norm</code> : use a global normalization (eg. the sum of areas under all curves is 1). Otherwise, normalize each hue individually</p> display <p>Display parameters</p> <p><code>show_injection</code> : add a patch showing the extent of injection sites. Uses corresponding channel colors. Requires the information TOML configuration file set up <code>cmap</code> : matplotlib color map for 2D heatmaps <code>cmap_nbins</code> : number of bins for 2D heatmaps <code>cmap_lim</code> : color limits for 2D heatmaps</p> regions <p>Distributions per regions parameters</p> <p><code>base_measurement</code> : the name of the measurement in QuPath to derive others from. Usually \"Count\" or \"Length \u00b5m\" <code>hue</code> : color bars with this parameter, must be \"hemisphere\" or \"channel\" <code>hue_filter</code> : use only a subset of data</p> <ul> <li>If hue=hemisphere : it should be a channel name, a list of such or \"all\"</li> <li>If hue=channel : it should be a hemisphere name or \"both\"</li> </ul> <p><code>hue_mirror</code> : plot two hue_filter in mirror instead of discarding the others. For example, if hue=channel and hue_filter=\"both\", plots the two hemisphere in mirror. <code>normalize_starter_cells</code> : normalize non-relative metrics by the number of starter cells</p> metrics <p>Names of metrics. The keys are used internally in cuisto as is so should NOT be modified. The values will only chang etheir names in the ouput file</p> <p><code>\"density \u00b5m^-2\"</code> : relevant name <code>\"density mm^-2\"</code> : relevant name <code>\"coverage index\"</code> : relevant name <code>\"relative measurement\"</code> : relevant name <code>\"relative density\"</code> : relevant name</p> display <p><code>nregions</code> : number of regions to display (sorted by max.) <code>orientation</code> : orientation of the bars (\"h\" or \"v\") <code>order</code> : order the regions by \"ontology\" or by \"max\". Set to \"max\" to provide a custom order <code>dodge</code> : enforce the bar not being stacked <code>log_scale</code> : use log. scale for metrics</p> metrics <p>name of metrics to display</p> <p><code>\"count\"</code> : real_name = display_name, with real_name the \"values\" in [regions.metrics] <code>\"density mm^-2\"</code></p> files <p>Full path to information TOML files and atlas outlines for 2D heatmaps.</p> <p><code>blacklist</code> <code>fusion</code> <code>outlines</code> <code>infos</code> </p>"},{"location":"api-config.html","title":"cuisto.config","text":"<p>config module, part of cuisto.</p> <p>Contains the Config class.</p>"},{"location":"api-config.html#cuisto.config.Config","title":"<code>Config(config_file)</code>","text":"<p>The configuration class.</p> <p>Reads input configuration file and provides its constant.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Full path to the configuration file to load.</p> required <p>Returns:</p> Name Type Description <code>cfg</code> <code>Config object.</code> <p>Constructor.</p> Source code in <code>cuisto/config.py</code> <pre><code>def __init__(self, config_file):\n    \"\"\"Constructor.\"\"\"\n    with open(config_file, \"rb\") as fid:\n        cfg = tomllib.load(fid)\n\n        for key in cfg:\n            setattr(self, key, cfg[key])\n\n    self.config_file = config_file\n    self.bg_atlas = BrainGlobeAtlas(self.atlas[\"name\"], check_latest=False)\n    self.get_blacklist()\n    self.get_leaves_list()\n</code></pre>"},{"location":"api-config.html#cuisto.config.Config.get_blacklist","title":"<code>get_blacklist()</code>","text":"<p>Wraps cuisto.utils.get_blacklist.</p> Source code in <code>cuisto/config.py</code> <pre><code>def get_blacklist(self):\n    \"\"\"Wraps cuisto.utils.get_blacklist.\"\"\"\n\n    self.atlas[\"blacklist\"] = utils.get_blacklist(\n        self.files[\"blacklist\"], self.bg_atlas\n    )\n</code></pre>"},{"location":"api-config.html#cuisto.config.Config.get_hue_palette","title":"<code>get_hue_palette(mode)</code>","text":"<p>Get color palette given hue.</p> <p>Maps hue to colors in channels or hemispheres.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>(hemisphere, channel)</code> <code>\"hemisphere\"</code> <p>Returns:</p> Name Type Description <code>palette</code> <code>dict</code> <p>Maps a hue level to a color, usable in seaborn.</p> Source code in <code>cuisto/config.py</code> <pre><code>def get_hue_palette(self, mode: str) -&gt; dict:\n    \"\"\"\n    Get color palette given hue.\n\n    Maps hue to colors in channels or hemispheres.\n\n    Parameters\n    ----------\n    mode : {\"hemisphere\", \"channel\"}\n\n    Returns\n    -------\n    palette : dict\n        Maps a hue level to a color, usable in seaborn.\n\n    \"\"\"\n    params = getattr(self, mode)\n\n    if params[\"hue\"] == \"channel\":\n        # replace channels by their new names\n        palette = {\n            self.channels[\"names\"][k]: v for k, v in self.channels[\"colors\"].items()\n        }\n    elif params[\"hue\"] == \"hemisphere\":\n        # replace hemispheres by their new names\n        palette = {\n            self.hemispheres[\"names\"][k]: v\n            for k, v in self.hemispheres[\"colors\"].items()\n        }\n    else:\n        palette = None\n        warnings.warn(f\"hue={self.regions[\"display\"][\"hue\"]} not supported.\")\n\n    return palette\n</code></pre>"},{"location":"api-config.html#cuisto.config.Config.get_injection_sites","title":"<code>get_injection_sites(animals)</code>","text":"<p>Get list of injection sites coordinates for each animals, for each channels.</p> <p>Parameters:</p> Name Type Description Default <code>animals</code> <code>list of str</code> <p>List of animals.</p> required <p>Returns:</p> Name Type Description <code>injection_sites</code> <code>dict</code> <p>{\"x\": {channel0: [x]}, \"y\": {channel1: [y]}}</p> Source code in <code>cuisto/config.py</code> <pre><code>def get_injection_sites(self, animals: list[str]) -&gt; dict:\n    \"\"\"\n    Get list of injection sites coordinates for each animals, for each channels.\n\n    Parameters\n    ----------\n    animals : list of str\n        List of animals.\n\n    Returns\n    -------\n    injection_sites : dict\n        {\"x\": {channel0: [x]}, \"y\": {channel1: [y]}}\n\n    \"\"\"\n    injection_sites = {\n        axis: {channel: [] for channel in self.channels[\"names\"].keys()}\n        for axis in [\"x\", \"y\", \"z\"]\n    }\n\n    for animal in animals:\n        for channel in self.channels[\"names\"].keys():\n            injx, injy, injz = utils.get_injection_site(\n                animal,\n                self.files[\"infos\"],\n                channel,\n                stereo=self.distributions[\"stereo\"],\n            )\n            if injx is not None:\n                injection_sites[\"x\"][channel].append(injx)\n            if injy is not None:\n                injection_sites[\"y\"][channel].append(injy)\n            if injz is not None:\n                injection_sites[\"z\"][channel].append(injz)\n\n    return injection_sites\n</code></pre>"},{"location":"api-config.html#cuisto.config.Config.get_leaves_list","title":"<code>get_leaves_list()</code>","text":"<p>Wraps utils.get_leaves_list.</p> Source code in <code>cuisto/config.py</code> <pre><code>def get_leaves_list(self):\n    \"\"\"Wraps utils.get_leaves_list.\"\"\"\n\n    self.atlas[\"leaveslist\"] = utils.get_leaves_list(self.bg_atlas)\n</code></pre>"},{"location":"api-display.html","title":"cuisto.display","text":"<p>display module, part of cuisto.</p> <p>Contains display functions, essentially wrapping matplotlib and seaborn functions.</p>"},{"location":"api-display.html#cuisto.display.add_data_coverage","title":"<code>add_data_coverage(df, ax, colors=None, **kwargs)</code>","text":"<p>Add lines below the plot to represent data coverage.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with <code>X_min</code> and <code>X_max</code> on rows for each animals (on columns).</p> required <code>ax</code> <code>Axes</code> <p>Handle to axes where to add the patch.</p> required <code>colors</code> <code>list or str or None</code> <p>Colors for the patches, as a RGB list or hex list. Should be the same size as the number of patches to plot, eg. the number of columns in <code>df</code>. If None, default seaborn colors are used. If only one element, used for each animal.</p> <code>None</code> <code>**kwargs</code> <code>passed to patches.Rectangle()</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Handle to updated axes.</p> Source code in <code>cuisto/display.py</code> <pre><code>def add_data_coverage(\n    df: pd.DataFrame, ax: plt.Axes, colors: list | str | None = None, **kwargs\n) -&gt; plt.Axes:\n    \"\"\"\n    Add lines below the plot to represent data coverage.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame with `X_min` and `X_max` on rows for each animals (on columns).\n    ax : Axes\n        Handle to axes where to add the patch.\n    colors : list or str or None, optional\n        Colors for the patches, as a RGB list or hex list. Should be the same size as\n        the number of patches to plot, eg. the number of columns in `df`. If None,\n        default seaborn colors are used. If only one element, used for each animal.\n    **kwargs : passed to patches.Rectangle()\n\n    Returns\n    -------\n    ax : Axes\n        Handle to updated axes.\n\n    \"\"\"\n    # get colors\n    ncolumns = len(df.columns)\n    if not colors:\n        colors = sns.color_palette(n_colors=ncolumns)\n    elif isinstance(colors, str) or (isinstance(colors, list) &amp; (len(colors) == 3)):\n        colors = [colors] * ncolumns\n    elif len(colors) != ncolumns:\n        warnings.warn(f\"Wrong number of colors ({len(colors)}), using default colors.\")\n        colors = sns.color_palette(n_colors=ncolumns)\n\n    # get patch height depending on current axis limits\n    ymin, ymax = ax.get_ylim()\n    height = (ymax - ymin) * 0.02\n\n    for animal, color in zip(df.columns, colors):\n        # get patch coordinates\n        ymin, ymax = ax.get_ylim()\n        ylength = ymax - ymin\n        ybottom = ymin - 0.02 * ylength\n        xleft = df.loc[\"X_min\", animal]\n        xright = df.loc[\"X_max\", animal]\n\n        # plot patch\n        ax.add_patch(\n            patches.Rectangle(\n                (xleft, ybottom),\n                xright - xleft,\n                height,\n                label=animal,\n                color=color,\n                **kwargs,\n            )\n        )\n\n        ax.autoscale(tight=True)  # set new axes limits\n\n    ax.autoscale()  # reset scale\n\n    return ax\n</code></pre>"},{"location":"api-display.html#cuisto.display.add_injection_patch","title":"<code>add_injection_patch(X, ax, **kwargs)</code>","text":"<p>Add a patch representing the injection sites.</p> <p>The patch will span from the minimal coordinate to the maximal. If plotted in stereotaxic coordinates, coordinates should be converted beforehand.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>list</code> <p>Coordinates in mm for each animals. Can be empty to not plot anything.</p> required <code>ax</code> <code>Axes</code> <p>Handle to axes where to add the patch.</p> required <code>**kwargs</code> <code>passed to Axes.axvspan</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Handle to updated Axes.</p> Source code in <code>cuisto/display.py</code> <pre><code>def add_injection_patch(X: list, ax: plt.Axes, **kwargs) -&gt; plt.Axes:\n    \"\"\"\n    Add a patch representing the injection sites.\n\n    The patch will span from the minimal coordinate to the maximal.\n    If plotted in stereotaxic coordinates, coordinates should be converted beforehand.\n\n    Parameters\n    ----------\n    X : list\n        Coordinates in mm for each animals. Can be empty to not plot anything.\n    ax : Axes\n        Handle to axes where to add the patch.\n    **kwargs : passed to Axes.axvspan\n\n    Returns\n    -------\n    ax : Axes\n        Handle to updated Axes.\n\n    \"\"\"\n    # plot patch\n    if len(X) &gt; 0:\n        ax.axvspan(min(X), max(X), **kwargs)\n\n    return ax\n</code></pre>"},{"location":"api-display.html#cuisto.display.draw_structure_outline","title":"<code>draw_structure_outline(view='sagittal', structures=['root'], outline_file='', ax=None, microns=False, **kwargs)</code>","text":"<p>Plot brain regions outlines in given projection.</p> <p>This requires a file containing the structures outlines.</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>str</code> <p>Projection, \"sagittal\", \"coronal\" or \"top\". Default is \"sagittal\".</p> <code>'sagittal'</code> <code>structures</code> <code>list[str]</code> <p>List of structures acronyms whose outlines will be drawn. Default is [\"root\"].</p> <code>['root']</code> <code>outline_file</code> <code>str</code> <p>Full path the outlines HDF5 file.</p> <code>''</code> <code>ax</code> <code>Axes or None</code> <p>Axes where to plot the outlines. If None, get current axes (the default).</p> <code>None</code> <code>microns</code> <code>bool</code> <p>If False (default), converts the coordinates in mm.</p> <code>False</code> <code>**kwargs</code> <code>passed to pyplot.plot()</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> Source code in <code>cuisto/display.py</code> <pre><code>def draw_structure_outline(\n    view: str = \"sagittal\",\n    structures: list[str] = [\"root\"],\n    outline_file: str = \"\",\n    ax: plt.Axes | None = None,\n    microns: bool = False,\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plot brain regions outlines in given projection.\n\n    This requires a file containing the structures outlines.\n\n    Parameters\n    ----------\n    view : str\n        Projection, \"sagittal\", \"coronal\" or \"top\". Default is \"sagittal\".\n    structures : list[str]\n        List of structures acronyms whose outlines will be drawn. Default is [\"root\"].\n    outline_file : str\n        Full path the outlines HDF5 file.\n    ax : plt.Axes or None, optional\n        Axes where to plot the outlines. If None, get current axes (the default).\n    microns : bool, optional\n        If False (default), converts the coordinates in mm.\n    **kwargs : passed to pyplot.plot()\n\n    Returns\n    -------\n    ax : plt.Axes\n\n    \"\"\"\n    # get axes\n    if not ax:\n        ax = plt.gca()\n\n    # get units\n    if microns:\n        conv = 1\n    else:\n        conv = 1 / 1000\n\n    with h5py.File(outline_file) as f:\n        if view == \"sagittal\":\n            for structure in structures:\n                dsets = f[\"sagittal\"][structure]\n\n                for dset in dsets.values():\n                    ax.plot(dset[:, 0] * conv, dset[:, 1] * conv, **kwargs)\n\n        if view == \"coronal\":\n            for structure in structures:\n                dsets = f[\"coronal\"][structure]\n\n                for dset in dsets.values():\n                    ax.plot(dset[:, 0] * conv, dset[:, 1] * conv, **kwargs)\n\n        if view == \"top\":\n            for structure in structures:\n                dsets = f[\"top\"][structure]\n\n                for dset in dsets.values():\n                    ax.plot(dset[:, 0] * conv, dset[:, 1] * conv, **kwargs)\n\n    return ax\n</code></pre>"},{"location":"api-display.html#cuisto.display.nice_bar_plot","title":"<code>nice_bar_plot(df, x='', y=[''], hue='', ylabel=[''], orient='h', nx=None, ordering=None, names_list=None, hue_mirror=False, log_scale=False, bar_kws={}, pts_kws={})</code>","text":"<p>Nice bar plot of per-region objects distribution.</p> <p>This is used for objects distribution across brain regions. Shows the <code>y</code> metric (count, aeral density, cumulated length...) in each <code>x</code> categories (brain regions). <code>orient</code> controls wether the bars are shown horizontally (default) or vertically. Input <code>df</code> must have an additional \"hemisphere\" column. All <code>y</code> are plotted in the same figure as different subplots. <code>nx</code> controls the number of displayed regions.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <code>x</code> <code>str</code> <p>Key in <code>df</code>.</p> <code>''</code> <code>y</code> <code>str</code> <p>Key in <code>df</code>.</p> <code>''</code> <code>hue</code> <code>str</code> <p>Key in <code>df</code>.</p> <code>''</code> <code>ylabel</code> <code>list of str</code> <p>Y axis labels.</p> <code>['']</code> <code>orient</code> <code>h or v</code> <p>\"h\" for horizontal bars (default) or \"v\" for vertical bars.</p> <code>'h'</code> <code>nx</code> <code>None or int</code> <p>Number of <code>x</code> to show in the plot. Default is None (no limit).</p> <code>None</code> <code>ordering</code> <code>None or list[str] or max</code> <p>Sorted list of acronyms. Data will be sorted follwowing this order, if \"max\", sorted by descending values, if None, not sorted (default).</p> <code>None</code> <code>names_list</code> <code>list or None</code> <p>List of names to display. If None (default), takes the most prominent overall ones.</p> <code>None</code> <code>hue_mirror</code> <code>bool</code> <p>If there are 2 groups, plot in mirror. Default is False.</p> <code>False</code> <code>log_scale</code> <code>bool</code> <p>Set the metrics in log scale. Default is False.</p> <code>False</code> <code>bar_kws</code> <code>dict</code> <p>Passed to seaborn.barplot().</p> <code>{}</code> <code>pts_kws</code> <code>dict</code> <p>Passed to seaborn.stripplot().</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>figs</code> <code>list</code> <p>List of figures.</p> Source code in <code>cuisto/display.py</code> <pre><code>def nice_bar_plot(\n    df: pd.DataFrame,\n    x: str = \"\",\n    y: list[str] = [\"\"],\n    hue: str = \"\",\n    ylabel: list[str] = [\"\"],\n    orient=\"h\",\n    nx: None | int = None,\n    ordering: None | list[str] | str = None,\n    names_list: None | list = None,\n    hue_mirror: bool = False,\n    log_scale: bool = False,\n    bar_kws: dict = {},\n    pts_kws: dict = {},\n) -&gt; list[plt.Axes]:\n    \"\"\"\n    Nice bar plot of per-region objects distribution.\n\n    This is used for objects distribution across brain regions. Shows the `y` metric\n    (count, aeral density, cumulated length...) in each `x` categories (brain regions).\n    `orient` controls wether the bars are shown horizontally (default) or vertically.\n    Input `df` must have an additional \"hemisphere\" column. All `y` are plotted in the\n    same figure as different subplots. `nx` controls the number of displayed regions.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n    x, y, hue : str\n        Key in `df`.\n    ylabel : list of str\n        Y axis labels.\n    orient : \"h\" or \"v\", optional\n        \"h\" for horizontal bars (default) or \"v\" for vertical bars.\n    nx : None or int, optional\n        Number of `x` to show in the plot. Default is None (no limit).\n    ordering : None or list[str] or \"max\", optional\n        Sorted list of acronyms. Data will be sorted follwowing this order, if \"max\",\n        sorted by descending values, if None, not sorted (default).\n    names_list : list or None, optional\n        List of names to display. If None (default), takes the most prominent overall\n        ones.\n    hue_mirror : bool, optional\n        If there are 2 groups, plot in mirror. Default is False.\n    log_scale : bool, optional\n        Set the metrics in log scale. Default is False.\n    bar_kws : dict\n        Passed to seaborn.barplot().\n    pts_kws : dict\n        Passed to seaborn.stripplot().\n\n    Returns\n    -------\n    figs : list\n        List of figures.\n\n    \"\"\"\n    figs = []\n    # loop for each features\n    for yi, ylabeli in zip(y, ylabel):\n        # prepare data\n        # get nx first most prominent regions\n        if not names_list:\n            names_list_plt = (\n                df.groupby([\"Name\"])[yi].mean().sort_values(ascending=False).index[0:nx]\n            )\n        else:\n            names_list_plt = names_list\n        dfplt = df[df[\"Name\"].isin(names_list_plt)]  # limit to those regions\n        # limit hierarchy list if provided\n        if isinstance(ordering, list):\n            order = [el for el in ordering if el in names_list_plt]\n        elif ordering == \"max\":\n            order = names_list_plt\n        else:\n            order = None\n\n        # reorder keys depending on orientation and create axes\n        if orient == \"h\":\n            xp = yi\n            yp = x\n            if hue_mirror:\n                nrows = 1\n                ncols = 2\n                sharex = None\n                sharey = \"all\"\n            else:\n                nrows = 1\n                ncols = 1\n                sharex = None\n                sharey = None\n        elif orient == \"v\":\n            xp = x\n            yp = yi\n            if hue_mirror:\n                nrows = 2\n                ncols = 1\n                sharex = \"all\"\n                sharey = None\n            else:\n                nrows = 1\n                ncols = 1\n                sharex = None\n                sharey = None\n        fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey)\n\n        if hue_mirror:\n            # two graphs\n            ax1, ax2 = axs\n            # determine what will be mirrored\n            if hue == \"channel\":\n                hue_filter = \"hemisphere\"\n            elif hue == \"hemisphere\":\n                hue_filter = \"channel\"\n            # select the two types (should be left/right or two channels)\n            hue_filters = dfplt[hue_filter].unique()[0:2]\n            hue_filters.sort()  # make sure it will be always in the same order\n\n            # plot\n            for filt, ax in zip(hue_filters, [ax1, ax2]):\n                dfplt2 = dfplt[dfplt[hue_filter] == filt]\n                ax = sns.barplot(\n                    dfplt2,\n                    x=xp,\n                    y=yp,\n                    hue=hue,\n                    estimator=\"mean\",\n                    errorbar=\"se\",\n                    orient=orient,\n                    order=order,\n                    ax=ax,\n                    **bar_kws,\n                )\n                # add points\n                ax = sns.stripplot(\n                    dfplt2, x=xp, y=yp, hue=hue, legend=False, ax=ax, **pts_kws\n                )\n\n                # cosmetics\n                if orient == \"h\":\n                    ax.set_title(f\"{hue_filter}: {filt}\")\n                    ax.set_ylabel(None)\n                    ax.set_ylim((nx + 0.5, -0.5))\n                    if log_scale:\n                        ax.set_xscale(\"log\")\n\n                elif orient == \"v\":\n                    if ax == ax1:\n                        # top title\n                        ax1.set_title(f\"{hue_filter}: {filt}\")\n                        ax.set_xlabel(None)\n                    elif ax == ax2:\n                        # use xlabel as bottom title\n                        ax2.set_xlabel(\n                            f\"{hue_filter}: {filt}\", fontsize=ax1.title.get_fontsize()\n                        )\n                    ax.set_xlim((-0.5, nx + 0.5))\n                    if log_scale:\n                        ax.set_yscale(\"log\")\n\n                    for label in ax.get_xticklabels():\n                        label.set_verticalalignment(\"center\")\n                        label.set_horizontalalignment(\"center\")\n\n            # tune axes cosmetics\n            if orient == \"h\":\n                ax1.set_xlabel(ylabeli)\n                ax2.set_xlabel(ylabeli)\n                ax1.set_xlim(\n                    ax1.get_xlim()[0], max((ax1.get_xlim()[1], ax2.get_xlim()[1]))\n                )\n                ax2.set_xlim(\n                    ax2.get_xlim()[0], max((ax1.get_xlim()[1], ax2.get_xlim()[1]))\n                )\n                ax1.invert_xaxis()\n                sns.despine(ax=ax1, left=True, top=True, right=False, bottom=False)\n                sns.despine(ax=ax2, left=False, top=True, right=True, bottom=False)\n                ax1.yaxis.tick_right()\n                ax1.tick_params(axis=\"y\", pad=20)\n                for label in ax1.get_yticklabels():\n                    label.set_verticalalignment(\"center\")\n                    label.set_horizontalalignment(\"center\")\n            elif orient == \"v\":\n                ax2.set_ylabel(ylabeli)\n                ax1.set_ylim(\n                    ax1.get_ylim()[0], max((ax1.get_ylim()[1], ax2.get_ylim()[1]))\n                )\n                ax2.set_ylim(\n                    ax2.get_ylim()[0], max((ax1.get_ylim()[1], ax2.get_ylim()[1]))\n                )\n                ax2.invert_yaxis()\n                sns.despine(ax=ax1, left=False, top=True, right=True, bottom=False)\n                sns.despine(ax=ax2, left=False, top=False, right=True, bottom=True)\n                for label in ax2.get_xticklabels():\n                    label.set_verticalalignment(\"center\")\n                    label.set_horizontalalignment(\"center\")\n                ax2.tick_params(axis=\"x\", labelrotation=90, pad=20)\n\n        else:\n            # one graph\n            ax = axs\n            # plot\n            ax = sns.barplot(\n                dfplt,\n                x=xp,\n                y=yp,\n                hue=hue,\n                estimator=\"mean\",\n                errorbar=\"se\",\n                orient=orient,\n                order=order,\n                ax=ax,\n                **bar_kws,\n            )\n            # add points\n            ax = sns.stripplot(\n                dfplt, x=xp, y=yp, hue=hue, legend=False, ax=ax, **pts_kws\n            )\n\n            # cosmetics\n            if orient == \"h\":\n                ax.set_xlabel(ylabeli)\n                ax.set_ylabel(None)\n                ax.set_ylim((nx + 0.5, -0.5))\n                if log_scale:\n                    ax.set_xscale(\"log\")\n            elif orient == \"v\":\n                ax.set_xlabel(None)\n                ax.set_ylabel(ylabeli)\n                ax.set_xlim((-0.5, nx + 0.5))\n                if log_scale:\n                    ax.set_yscale(\"log\")\n\n        fig.tight_layout(pad=0)\n        figs.append(fig)\n\n    return figs\n</code></pre>"},{"location":"api-display.html#cuisto.display.nice_distribution_plot","title":"<code>nice_distribution_plot(df, x='', y='', hue=None, xlabel='', ylabel='', injections_sites={}, channel_colors={}, channel_names={}, ax=None, **kwargs)</code>","text":"<p>Nice plot of 1D distribution of objects.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <code>x</code> <code>str</code> <p>Keys in <code>df</code>.</p> <code>''</code> <code>y</code> <code>str</code> <p>Keys in <code>df</code>.</p> <code>''</code> <code>hue</code> <code>str or None</code> <p>Key in <code>df</code>. If None, no hue is used.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>X and Y axes labels.</p> <code>''</code> <code>ylabel</code> <code>str</code> <p>X and Y axes labels.</p> <code>''</code> <code>injections_sites</code> <code>dict</code> <p>List of injection sites 1D coordinates in a dict with the channel name as key. If empty, injection site is not plotted (default).</p> <code>{}</code> <code>channel_colors</code> <code>dict</code> <p>Required if injections_sites is not empty, dict mapping channel names to a color.</p> <code>{}</code> <code>channel_names</code> <code>dict</code> <p>Required if injections_sites is not empty, dict mapping channel names to a display name.</p> <code>{}</code> <code>ax</code> <code>Axes or None</code> <p>Axes in which to plot the figure, if None, a new figure is created (default).</p> <code>None</code> <code>**kwargs</code> <code>passed to seaborn.lineplot()</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>matplotlib axes</code> <p>Handle to axes.</p> Source code in <code>cuisto/display.py</code> <pre><code>def nice_distribution_plot(\n    df: pd.DataFrame,\n    x: str = \"\",\n    y: str = \"\",\n    hue: str | None = None,\n    xlabel: str = \"\",\n    ylabel: str = \"\",\n    injections_sites: dict = {},\n    channel_colors: dict = {},\n    channel_names: dict = {},\n    ax: plt.Axes | None = None,\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"\n    Nice plot of 1D distribution of objects.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n    x, y : str\n        Keys in `df`.\n    hue : str or None, optional\n        Key in `df`. If None, no hue is used.\n    xlabel, ylabel : str\n        X and Y axes labels.\n    injections_sites : dict, optional\n        List of injection sites 1D coordinates in a dict with the channel name as key.\n        If empty, injection site is not plotted (default).\n    channel_colors : dict, optional\n        Required if injections_sites is not empty, dict mapping channel names to a\n        color.\n    channel_names : dict, optional\n        Required if injections_sites is not empty, dict mapping channel names to a\n        display name.\n    ax : Axes or None, optional\n        Axes in which to plot the figure, if None, a new figure is created (default).\n    **kwargs : passed to seaborn.lineplot()\n\n    Returns\n    -------\n    ax : matplotlib axes\n        Handle to axes.\n\n    \"\"\"\n    if not ax:\n        # create figure\n        _, ax = plt.subplots(figsize=(10, 6))\n\n    ax = sns.lineplot(\n        df,\n        x=x,\n        y=y,\n        hue=hue,\n        estimator=\"mean\",\n        errorbar=\"se\",\n        ax=ax,\n        **kwargs,\n    )\n\n    for channel in injections_sites.keys():\n        ax = add_injection_patch(\n            injections_sites[channel],\n            ax,\n            color=channel_colors[channel],\n            edgecolor=None,\n            alpha=0.25,\n            label=channel_names[channel] + \": inj. site\",\n        )\n\n    ax.legend()\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    return ax\n</code></pre>"},{"location":"api-display.html#cuisto.display.nice_heatmap","title":"<code>nice_heatmap(df, animals, x='', y='', xlabel='', ylabel='', invertx=False, inverty=False, **kwargs)</code>","text":"<p>Nice plots of 2D distribution of boutons as a heatmap per animal.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <code>animals</code> <code>list-like of str</code> <p>List of animals.</p> required <code>x</code> <code>str</code> <p>Keys in <code>df</code>.</p> <code>''</code> <code>y</code> <code>str</code> <p>Keys in <code>df</code>.</p> <code>''</code> <code>xlabel</code> <code>str</code> <p>Labels of x and y axes.</p> <code>''</code> <code>ylabel</code> <code>str</code> <p>Labels of x and y axes.</p> <code>''</code> <code>invertx</code> <code>bool</code> <p>Wether to inverse the x or y axes. Default is False.</p> <code>False</code> <code>inverty</code> <code>bool</code> <p>Wether to inverse the x or y axes. Default is False.</p> <code>False</code> <code>**kwargs</code> <code>passed to seaborn.histplot()</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes or list of Axes</code> <p>Handle to axes.</p> Source code in <code>cuisto/display.py</code> <pre><code>def nice_heatmap(\n    df: pd.DataFrame,\n    animals: tuple[str] | list[str],\n    x: str = \"\",\n    y: str = \"\",\n    xlabel: str = \"\",\n    ylabel: str = \"\",\n    invertx: bool = False,\n    inverty: bool = False,\n    **kwargs,\n) -&gt; list[plt.Axes] | plt.Axes:\n    \"\"\"\n    Nice plots of 2D distribution of boutons as a heatmap per animal.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n    animals : list-like of str\n        List of animals.\n    x, y : str\n        Keys in `df`.\n    xlabel, ylabel : str\n        Labels of x and y axes.\n    invertx, inverty : bool, optional\n        Wether to inverse the x or y axes. Default is False.\n    **kwargs : passed to seaborn.histplot()\n\n    Returns\n    -------\n    ax : Axes or list of Axes\n        Handle to axes.\n\n    \"\"\"\n\n    # 2D distribution, per animal\n    _, axs = plt.subplots(len(animals), 1, sharex=\"all\")\n\n    for animal, ax in zip(animals, axs):\n        ax = sns.histplot(\n            df[df[\"animal\"] == animal],\n            x=x,\n            y=y,\n            ax=ax,\n            **kwargs,\n        )\n        ax.set_xlabel(xlabel)\n        ax.set_ylabel(ylabel)\n        ax.set_title(animal)\n\n        if inverty:\n            ax.invert_yaxis()\n\n    if invertx:\n        axs[-1].invert_xaxis()  # only once since all x axes are shared\n\n    return axs\n</code></pre>"},{"location":"api-display.html#cuisto.display.nice_joint_plot","title":"<code>nice_joint_plot(df, x='', y='', xlabel='', ylabel='', invertx=False, inverty=False, outline_kws={}, ax=None, **kwargs)</code>","text":"<p>Joint distribution.</p> <p>Used to display a 2D heatmap of objects. This is more qualitative than quantitative, for display purposes.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <code>x</code> <code>str</code> <p>Keys in <code>df</code>.</p> <code>''</code> <code>y</code> <code>str</code> <p>Keys in <code>df</code>.</p> <code>''</code> <code>xlabel</code> <code>str</code> <p>Label of x and y axes.</p> <code>''</code> <code>ylabel</code> <code>str</code> <p>Label of x and y axes.</p> <code>''</code> <code>invertx</code> <code>bool</code> <p>Whether to inverse the x or y axes. Default is False for both.</p> <code>False</code> <code>inverty</code> <code>bool</code> <p>Whether to inverse the x or y axes. Default is False for both.</p> <code>False</code> <code>outline_kws</code> <code>dict</code> <p>Passed to draw_structure_outline().</p> <code>{}</code> <code>ax</code> <code>Axes or None</code> <p>Axes to plot in. If None, draws in current axes (default).</p> <code>None</code> <code>**kwargs</code> <p>Passed to seaborn.histplot.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> Source code in <code>cuisto/display.py</code> <pre><code>def nice_joint_plot(\n    df: pd.DataFrame,\n    x: str = \"\",\n    y: str = \"\",\n    xlabel: str = \"\",\n    ylabel: str = \"\",\n    invertx: bool = False,\n    inverty: bool = False,\n    outline_kws: dict = {},\n    ax: plt.Axes | None = None,\n    **kwargs,\n) -&gt; plt.Figure:\n    \"\"\"\n    Joint distribution.\n\n    Used to display a 2D heatmap of objects. This is more qualitative than quantitative,\n    for display purposes.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n    x, y : str\n        Keys in `df`.\n    xlabel, ylabel : str\n        Label of x and y axes.\n    invertx, inverty : bool, optional\n        Whether to inverse the x or y axes. Default is False for both.\n    outline_kws : dict\n        Passed to draw_structure_outline().\n    ax : plt.Axes or None, optional\n        Axes to plot in. If None, draws in current axes (default).\n    **kwargs\n        Passed to seaborn.histplot.\n\n    Returns\n    -------\n    ax : plt.Axes\n\n    \"\"\"\n    if not ax:\n        ax = plt.gca()\n\n    # plot outline\n    draw_structure_outline(ax=ax, **outline_kws)\n\n    # plot joint distribution\n    sns.histplot(\n        df,\n        x=x,\n        y=y,\n        ax=ax,\n        **kwargs,\n    )\n\n    # adjust axes\n    if invertx:\n        ax.invert_xaxis()\n    if inverty:\n        ax.invert_yaxis()\n\n    # labels\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    return ax\n</code></pre>"},{"location":"api-display.html#cuisto.display.plot_1D_distributions","title":"<code>plot_1D_distributions(dfs_distributions, cfg, df_coordinates=None)</code>","text":"<p>Wraps nice_distribution_plot().</p> Source code in <code>cuisto/display.py</code> <pre><code>def plot_1D_distributions(\n    dfs_distributions: list[pd.DataFrame],\n    cfg,\n    df_coordinates: pd.DataFrame = None,\n):\n    \"\"\"\n    Wraps nice_distribution_plot().\n    \"\"\"\n    # prepare figures\n    fig, axs_dist = plt.subplots(1, 3, sharey=True, figsize=(13, 6))\n    xlabels = [\n        \"Rostro-caudal position (mm)\",\n        \"Dorso-ventral position (mm)\",\n        \"Medio-lateral position (mm)\",\n    ]\n\n    # get animals\n    animals = []\n    for df in dfs_distributions:\n        animals.extend(df[\"animal\"].unique())\n    animals = set(animals)\n\n    # get injection sites\n    if cfg.distributions[\"display\"][\"show_injection\"]:\n        injection_sites = cfg.get_injection_sites(animals)\n    else:\n        injection_sites = {k: {} for k in range(3)}\n\n    # get color palette based on hue\n    hue = cfg.distributions[\"hue\"]\n    palette = cfg.get_hue_palette(\"distributions\")\n\n    # loop through each axis\n    for df_dist, ax_dist, xlabel, inj_sites in zip(\n        dfs_distributions, axs_dist, xlabels, injection_sites.values()\n    ):\n        # select data\n        if cfg.distributions[\"hue\"] == \"hemisphere\":\n            dfplt = df_dist[df_dist[\"hemisphere\"] != \"both\"]\n        elif cfg.distributions[\"hue\"] == \"channel\":\n            dfplt = df_dist[df_dist[\"channel\"] != \"all\"]\n\n        # plot\n        ax_dist = nice_distribution_plot(\n            dfplt,\n            x=\"bins\",\n            y=\"distribution\",\n            hue=hue,\n            xlabel=xlabel,\n            ylabel=\"normalized distribution\",\n            injections_sites=inj_sites,\n            channel_colors=cfg.channels[\"colors\"],\n            channel_names=cfg.channels[\"names\"],\n            linewidth=2,\n            palette=palette,\n            ax=ax_dist,\n        )\n\n        # add data coverage\n        if (\"Atlas_AP\" in df_dist[\"axis\"].unique()) &amp; (df_coordinates is not None):\n            df_coverage = utils.get_data_coverage(df_coordinates)\n            ax_dist = add_data_coverage(df_coverage, ax_dist, edgecolor=None, alpha=0.5)\n            ax_dist.legend()\n        else:\n            ax_dist.legend().remove()\n\n    # - Distributions, per animal\n    if len(animals) &gt; 1:\n        _, axs_dist = plt.subplots(1, 3, sharey=True)\n\n        # loop through each axis\n        for df_dist, ax_dist, xlabel, inj_sites in zip(\n            dfs_distributions, axs_dist, xlabels, injection_sites.values()\n        ):\n            # select data\n            df_dist_plot = df_dist[df_dist[\"hemisphere\"] == \"both\"]\n\n            # plot\n            ax_dist = nice_distribution_plot(\n                df_dist_plot,\n                x=\"bins\",\n                y=\"distribution\",\n                hue=\"animal\",\n                xlabel=xlabel,\n                ylabel=\"normalized distribution\",\n                injections_sites=inj_sites,\n                channel_colors=cfg.channels[\"colors\"],\n                channel_names=cfg.channels[\"names\"],\n                linewidth=2,\n                ax=ax_dist,\n            )\n\n    return fig\n</code></pre>"},{"location":"api-display.html#cuisto.display.plot_2D_distributions","title":"<code>plot_2D_distributions(df, cfg)</code>","text":"<p>Wraps nice_joint_plot().</p> Source code in <code>cuisto/display.py</code> <pre><code>def plot_2D_distributions(df: pd.DataFrame, cfg):\n    \"\"\"\n    Wraps nice_joint_plot().\n    \"\"\"\n    # -- 2D heatmap, all animals pooled\n    # prepare figure\n    fig_heatmap = plt.figure(figsize=(12, 9))\n\n    ax_sag = fig_heatmap.add_subplot(2, 2, 1)\n    ax_cor = fig_heatmap.add_subplot(2, 2, 2, sharey=ax_sag)\n    ax_top = fig_heatmap.add_subplot(2, 2, 3, sharex=ax_sag)\n    ax_cbar = fig_heatmap.add_subplot(2, 2, 4, box_aspect=15)\n\n    # prepare options\n    map_options = dict(\n        bins=cfg.distributions[\"display\"][\"cmap_nbins\"],\n        cmap=cfg.distributions[\"display\"][\"cmap\"],\n        rasterized=True,\n        thresh=10,\n        stat=\"count\",\n        vmin=cfg.distributions[\"display\"][\"cmap_lim\"][0],\n        vmax=cfg.distributions[\"display\"][\"cmap_lim\"][1],\n    )\n    outline_kws = dict(\n        structures=cfg.atlas[\"outline_structures\"],\n        outline_file=cfg.files[\"outlines\"],\n        linewidth=1.5,\n        color=\"k\",\n    )\n    cbar_kws = dict(label=\"count\")\n\n    # determine which axes are going to be inverted\n    if cfg.atlas[\"type\"] == \"brain\":\n        cor_invertx = True\n        cor_inverty = False\n        top_invertx = True\n        top_inverty = False\n    elif cfg.atlas[\"type\"] == \"cord\":\n        cor_invertx = False\n        cor_inverty = False\n        top_invertx = True\n        top_inverty = True\n\n    # - sagittal\n    # no need to invert axes because they are shared with the two other views\n    outline_kws[\"view\"] = \"sagittal\"\n    nice_joint_plot(\n        df,\n        x=\"Atlas_X\",\n        y=\"Atlas_Y\",\n        xlabel=\"Rostro-caudal (mm)\",\n        ylabel=\"Dorso-ventral (mm)\",\n        outline_kws=outline_kws,\n        ax=ax_sag,\n        **map_options,\n    )\n\n    # - coronal\n    outline_kws[\"view\"] = \"coronal\"\n    nice_joint_plot(\n        df,\n        x=\"Atlas_Z\",\n        y=\"Atlas_Y\",\n        xlabel=\"Medio-lateral (mm)\",\n        ylabel=\"Dorso-ventral (mm)\",\n        invertx=cor_invertx,\n        inverty=cor_inverty,\n        outline_kws=outline_kws,\n        ax=ax_cor,\n        **map_options,\n    )\n    ax_cor.invert_yaxis()\n\n    # - top\n    outline_kws[\"view\"] = \"top\"\n    nice_joint_plot(\n        df,\n        x=\"Atlas_X\",\n        y=\"Atlas_Z\",\n        xlabel=\"Rostro-caudal (mm)\",\n        ylabel=\"Medio-lateral (mm)\",\n        invertx=top_invertx,\n        inverty=top_inverty,\n        outline_kws=outline_kws,\n        ax=ax_top,\n        cbar=True,\n        cbar_ax=ax_cbar,\n        cbar_kws=cbar_kws,\n        **map_options,\n    )\n    fig_heatmap.suptitle(\"sagittal, coronal and top-view projections\")\n\n    # -- 2D heatmap per animals\n    # get animals\n    animals = df[\"animal\"].unique()\n    if len(animals) &gt; 1:\n        # Rostro-caudal, dorso-ventral (sagittal)\n        _ = nice_heatmap(\n            df,\n            animals,\n            x=\"Atlas_X\",\n            y=\"Atlas_Y\",\n            xlabel=\"Rostro-caudal (mm)\",\n            ylabel=\"Dorso-ventral (mm)\",\n            invertx=True,\n            inverty=True,\n            cmap=\"OrRd\",\n            rasterized=True,\n            cbar=True,\n        )\n\n        # Medio-lateral, dorso-ventral (coronal)\n        _ = nice_heatmap(\n            df,\n            animals,\n            x=\"Atlas_Z\",\n            y=\"Atlas_Y\",\n            xlabel=\"Medio-lateral (mm)\",\n            ylabel=\"Dorso-ventral (mm)\",\n            inverty=True,\n            invertx=True,\n            cmap=\"OrRd\",\n            rasterized=True,\n        )\n\n    return fig_heatmap\n</code></pre>"},{"location":"api-display.html#cuisto.display.plot_regions","title":"<code>plot_regions(df, cfg, **kwargs)</code>","text":"<p>Wraps nice_bar_plot().</p> Source code in <code>cuisto/display.py</code> <pre><code>def plot_regions(df: pd.DataFrame, cfg, **kwargs):\n    \"\"\"\n    Wraps nice_bar_plot().\n    \"\"\"\n    # get regions order\n    if cfg.regions[\"display\"][\"order\"] == \"ontology\":\n        regions_order = [d[\"acronym\"] for d in cfg.bg_atlas.structures_list]\n    elif cfg.regions[\"display\"][\"order\"] == \"max\":\n        regions_order = \"max\"\n    else:\n        regions_order = None\n\n    # determine metrics to be plotted and color palette based on hue\n    metrics = [*cfg.regions[\"display\"][\"metrics\"].keys()]\n    hue = cfg.regions[\"hue\"]\n    palette = cfg.get_hue_palette(\"regions\")\n\n    # select data\n    dfplt = utils.select_hemisphere_channel(\n        df, hue, cfg.regions[\"hue_filter\"], cfg.regions[\"hue_mirror\"]\n    )\n\n    # prepare options\n    bar_kws = dict(\n        err_kws={\"linewidth\": 1.5},\n        dodge=cfg.regions[\"display\"][\"dodge\"],\n        palette=palette,\n    )\n    pts_kws = dict(\n        size=4,\n        edgecolor=\"auto\",\n        linewidth=0.75,\n        dodge=cfg.regions[\"display\"][\"dodge\"],\n        palette=palette,\n    )\n    # draw\n    figs = nice_bar_plot(\n        dfplt,\n        x=\"Name\",\n        y=metrics,\n        hue=hue,\n        ylabel=[*cfg.regions[\"display\"][\"metrics\"].values()],\n        orient=cfg.regions[\"display\"][\"orientation\"],\n        nx=cfg.regions[\"display\"][\"nregions\"],\n        ordering=regions_order,\n        hue_mirror=cfg.regions[\"hue_mirror\"],\n        log_scale=cfg.regions[\"display\"][\"log_scale\"],\n        bar_kws=bar_kws,\n        pts_kws=pts_kws,\n        **kwargs,\n    )\n\n    return figs\n</code></pre>"},{"location":"api-io.html","title":"cuisto.io","text":"<p>io module, part of cuisto.</p> <p>Contains loading and saving functions.</p>"},{"location":"api-io.html#cuisto.io.cat_csv_dir","title":"<code>cat_csv_dir(directory, **kwargs)</code>","text":"<p>Scans a directory for csv files and concatenate them into a single DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory to scan.</p> required <code>**kwargs</code> <code>passed to pandas.read_csv()</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>All CSV files concatenated in a single DataFrame.</p> Source code in <code>cuisto/io.py</code> <pre><code>def cat_csv_dir(directory, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"\n    Scans a directory for csv files and concatenate them into a single DataFrame.\n\n    Parameters\n    ----------\n    directory : str\n        Path to the directory to scan.\n    **kwargs : passed to pandas.read_csv()\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        All CSV files concatenated in a single DataFrame.\n\n    \"\"\"\n    return pd.concat(\n        pd.read_csv(\n            os.path.join(directory, filename),\n            **kwargs,\n        )\n        for filename in os.listdir(directory)\n        if (filename.endswith(\".csv\"))\n        and not check_empty_file(os.path.join(directory, filename), threshold=1)\n    )\n</code></pre>"},{"location":"api-io.html#cuisto.io.cat_data_dir","title":"<code>cat_data_dir(directory, segtype, **kwargs)</code>","text":"<p>Wraps either cat_csv_dir() or cat_json_dir() depending on <code>segtype</code>.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory to scan.</p> required <code>segtype</code> <code>str</code> <p>\"synaptophysin\" or \"fibers\".</p> required <code>**kwargs</code> <code>passed to cat_csv_dir() or cat_json_dir().</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>All files concatenated in a single DataFrame.</p> Source code in <code>cuisto/io.py</code> <pre><code>def cat_data_dir(directory: str, segtype: str, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"\n    Wraps either cat_csv_dir() or cat_json_dir() depending on `segtype`.\n\n    Parameters\n    ----------\n    directory : str\n        Path to the directory to scan.\n    segtype : str\n        \"synaptophysin\" or \"fibers\".\n    **kwargs : passed to cat_csv_dir() or cat_json_dir().\n\n    Returns\n    -------\n    df : pd.DataFrame\n        All files concatenated in a single DataFrame.\n\n    \"\"\"\n    if segtype in CSV_KW:\n        # remove kwargs for json\n        kwargs.pop(\"hemisphere_names\", None)\n        kwargs.pop(\"atlas\", None)\n        return cat_csv_dir(directory, **kwargs)\n    elif segtype in JSON_KW:\n        kwargs = {k: kwargs[k] for k in [\"hemisphere_names\", \"atlas\"] if k in kwargs}\n        return cat_json_dir(directory, **kwargs)\n    else:\n        raise ValueError(\n            f\"'{segtype}' not supported, unable to determine if CSV or JSON.\"\n        )\n</code></pre>"},{"location":"api-io.html#cuisto.io.cat_json_dir","title":"<code>cat_json_dir(directory, hemisphere_names, atlas)</code>","text":"<p>Scans a directory for json files and concatenate them in a single DataFrame.</p> <p>The json files must be generated with 'workflow_import_export.groovy\" from a QuPath project.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory to scan.</p> required <code>hemisphere_names</code> <code>dict</code> <p>Maps between hemisphere names in the json files (\"Right\" and \"Left\") to something else (eg. \"Ipsi.\" and \"Contra.\").</p> required <code>atlas</code> <code>BrainGlobeAtlas</code> <p>Atlas to read regions from.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>All JSON files concatenated in a single DataFrame.</p> Source code in <code>cuisto/io.py</code> <pre><code>def cat_json_dir(\n    directory: str, hemisphere_names: dict, atlas: BrainGlobeAtlas\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Scans a directory for json files and concatenate them in a single DataFrame.\n\n    The json files must be generated with 'workflow_import_export.groovy\" from a QuPath\n    project.\n\n    Parameters\n    ----------\n    directory : str\n        Path to the directory to scan.\n    hemisphere_names : dict\n        Maps between hemisphere names in the json files (\"Right\" and \"Left\") to\n        something else (eg. \"Ipsi.\" and \"Contra.\").\n    atlas : BrainGlobeAtlas\n        Atlas to read regions from.\n\n    Returns\n    -------\n    df : pd.DataFrame\n        All JSON files concatenated in a single DataFrame.\n\n    \"\"\"\n    # list files\n    files_list = [\n        os.path.join(directory, filename)\n        for filename in os.listdir(directory)\n        if (filename.endswith(\".json\"))\n    ]\n\n    data = []  # prepare list of DataFrame\n    for filename in files_list:\n        with open(filename, \"rb\") as fid:\n            df = pd.DataFrame.from_dict(\n                orjson.loads(fid.read())[\"paths\"], orient=\"index\"\n            )\n            df[\"Image\"] = os.path.basename(filename).split(\"_detections\")[0]\n            data.append(df)\n\n    df = (\n        pd.concat(data)\n        .explode(\n            [\"x\", \"y\", \"z\", \"hemisphere\"]\n        )  # get an entry for each point of segments\n        .reset_index()\n        .rename(\n            columns=dict(\n                x=\"Atlas_X\",\n                y=\"Atlas_Y\",\n                z=\"Atlas_Z\",\n                index=\"Object ID\",\n                classification=\"Classification\",\n            )\n        )\n        .set_index(\"Object ID\")\n    )\n\n    # change hemisphere names\n    df[\"hemisphere\"] = df[\"hemisphere\"].map(hemisphere_names)\n\n    # add object type\n    df[\"Object type\"] = \"Detection\"\n\n    # add brain regions\n    df = utils.add_brain_region(df, atlas, col=\"Parent\")\n\n    return df\n</code></pre>"},{"location":"api-io.html#cuisto.io.check_empty_file","title":"<code>check_empty_file(filename, threshold=1)</code>","text":"<p>Checks if a file is empty.</p> <p>Empty is defined as a file whose number of lines is lower than or equal to <code>threshold</code> (to allow for headers).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Full path to the file to check.</p> required <code>threshold</code> <code>int</code> <p>If number of lines is lower than or equal to this value, it is considered as empty. Default is 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>empty</code> <code>bool</code> <p>True if the file is empty as defined above.</p> Source code in <code>cuisto/io.py</code> <pre><code>def check_empty_file(filename: str, threshold: int = 1) -&gt; bool:\n    \"\"\"\n    Checks if a file is empty.\n\n    Empty is defined as a file whose number of lines is lower than or equal to\n    `threshold` (to allow for headers).\n\n    Parameters\n    ----------\n    filename : str\n        Full path to the file to check.\n    threshold : int, optional\n        If number of lines is lower than or equal to this value, it is considered as\n        empty. Default is 1.\n\n    Returns\n    -------\n    empty : bool\n        True if the file is empty as defined above.\n\n    \"\"\"\n    with open(filename, \"rb\") as fid:\n        nlines = sum(1 for _ in fid)\n\n    if nlines &lt;= threshold:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api-io.html#cuisto.io.get_measurements_directory","title":"<code>get_measurements_directory(wdir, animal, kind, segtype)</code>","text":"<p>Get the directory with detections or annotations measurements for given animal ID.</p> <p>Parameters:</p> Name Type Description Default <code>wdir</code> <code>str</code> <p>Base working directory.</p> required <code>animal</code> <code>str</code> <p>Animal ID.</p> required <code>kind</code> <code>str</code> <p>\"annotation\" or \"detection\".</p> required <code>segtype</code> <code>str</code> <p>Type of segmentation, eg. \"synaptophysin\".</p> required <p>Returns:</p> Name Type Description <code>directory</code> <code>str</code> <p>Path to detections or annotations directory.</p> Source code in <code>cuisto/io.py</code> <pre><code>def get_measurements_directory(wdir, animal: str, kind: str, segtype: str) -&gt; str:\n    \"\"\"\n    Get the directory with detections or annotations measurements for given animal ID.\n\n    Parameters\n    ----------\n    wdir : str\n        Base working directory.\n    animal : str\n        Animal ID.\n    kind : str\n        \"annotation\" or \"detection\".\n    segtype : str\n        Type of segmentation, eg. \"synaptophysin\".\n\n    Returns\n    -------\n    directory : str\n        Path to detections or annotations directory.\n\n    \"\"\"\n    bdir = os.path.join(wdir, animal, animal.lower() + \"_segmentation\", segtype)\n\n    if (kind == \"detection\") or (kind == \"detections\"):\n        return os.path.join(bdir, \"detections\")\n    elif (kind == \"annotation\") or (kind == \"annotations\"):\n        return os.path.join(bdir, \"annotations\")\n    else:\n        raise ValueError(\n            f\"kind = '{kind}' not supported. Choose 'detection' or 'annotation'.\"\n        )\n</code></pre>"},{"location":"api-io.html#cuisto.io.load_dfs","title":"<code>load_dfs(filepath, fmt, identifiers=['df_regions', 'df_coordinates', 'df_distribution_ap', 'df_distribution_dv', 'df_distribution_ml'])</code>","text":"<p>Load DataFrames from file.</p> <p>If <code>fmt</code> is \"h5\" (\"xslx\"), identifiers are interpreted as h5 group identifier (sheet name, respectively). If <code>fmt</code> is \"pickle\", \"csv\" or \"tsv\", identifiers are appended to <code>filename</code>. Path to the file can't have a dot (\".\") in it.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Full path to the file(s), without extension.</p> required <code>fmt</code> <code>(h5, csv, pickle, xlsx)</code> <p>File(s) format.</p> <code>\"h5\"</code> <code>identifiers</code> <code>list of str</code> <p>List of identifiers to load from files. Defaults to the ones saved in cuisto.process.process_animals().</p> <code>['df_regions', 'df_coordinates', 'df_distribution_ap', 'df_distribution_dv', 'df_distribution_ml']</code> <p>Returns:</p> Type Description <code>All requested DataFrames.</code> Source code in <code>cuisto/io.py</code> <pre><code>def load_dfs(\n    filepath: str,\n    fmt: str,\n    identifiers: list[str] = [\n        \"df_regions\",\n        \"df_coordinates\",\n        \"df_distribution_ap\",\n        \"df_distribution_dv\",\n        \"df_distribution_ml\",\n    ],\n):\n    \"\"\"\n    Load DataFrames from file.\n\n    If `fmt` is \"h5\" (\"xslx\"), identifiers are interpreted as h5 group identifier (sheet\n    name, respectively).\n    If `fmt` is \"pickle\", \"csv\" or \"tsv\", identifiers are appended to `filename`.\n    Path to the file can't have a dot (\".\") in it.\n\n    Parameters\n    ----------\n    filepath : str\n        Full path to the file(s), without extension.\n    fmt : {\"h5\", \"csv\", \"pickle\", \"xlsx\"}\n        File(s) format.\n    identifiers : list of str, optional\n        List of identifiers to load from files. Defaults to the ones saved in\n        cuisto.process.process_animals().\n\n    Returns\n    -------\n    All requested DataFrames.\n\n    \"\"\"\n    # ensure filename without extension\n    base_path = os.path.splitext(filepath)[0]\n    full_path = base_path + \".\" + fmt\n\n    res = []\n    if (fmt == \"h5\") or (fmt == \"hdf\") or (fmt == \"hdf5\"):\n        for identifier in identifiers:\n            res.append(pd.read_hdf(full_path, identifier))\n    elif fmt == \"xlsx\":\n        for identifier in identifiers:\n            res.append(pd.read_excel(full_path, sheet_name=identifier))\n    else:\n        for identifier in identifiers:\n            id_path = f\"{base_path}_{identifier}.{fmt}\"\n            if (fmt == \"pickle\") or (fmt == \"pkl\"):\n                res.append(pd.read_pickle(id_path))\n            elif fmt == \"csv\":\n                res.append(pd.read_csv(id_path))\n            elif fmt == \"tsv\":\n                res.append(pd.read_csv(id_path, sep=\"\\t\"))\n            else:\n                raise ValueError(f\"{fmt} is not supported.\")\n\n    return res\n</code></pre>"},{"location":"api-io.html#cuisto.io.save_dfs","title":"<code>save_dfs(out_dir, filename, dfs)</code>","text":"<p>Save DataFrames to file.</p> <p>File format is inferred from file name extension.</p> <p>Parameters:</p> Name Type Description Default <code>out_dir</code> <code>str</code> <p>Output directory.</p> required <code>filename</code> <code>_type_</code> <p>File name.</p> required <code>dfs</code> <code>dict</code> <p>DataFrames to save, as {identifier: df}. If HDF5 or xlsx, all df are saved in the same file, otherwise identifier is appended to the file name.</p> required Source code in <code>cuisto/io.py</code> <pre><code>def save_dfs(out_dir: str, filename, dfs: dict):\n    \"\"\"\n    Save DataFrames to file.\n\n    File format is inferred from file name extension.\n\n    Parameters\n    ----------\n    out_dir : str\n        Output directory.\n    filename : _type_\n        File name.\n    dfs : dict\n        DataFrames to save, as {identifier: df}. If HDF5 or xlsx, all df are saved in\n        the same file, otherwise identifier is appended to the file name.\n\n    \"\"\"\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n\n    basename, ext = os.path.splitext(filename)\n    if ext in [\".h5\", \".hdf\", \".hdf5\"]:\n        path = os.path.join(out_dir, filename)\n        for identifier, df in dfs.items():\n            df.to_hdf(path, key=identifier)\n    elif ext == \".xlsx\":\n        for identifier, df in dfs.items():\n            df.to_excel(path, sheet_name=identifier)\n    else:\n        for identifier, df in dfs.items():\n            path = os.path.join(out_dir, f\"{basename}_{identifier}{ext}\")\n            if ext in [\".pickle\", \".pkl\"]:\n                df.to_pickle(path)\n            elif ext == \".csv\":\n                df.to_csv(path)\n            elif ext == \".tsv\":\n                df.to_csv(path, sep=\"\\t\")\n            else:\n                raise ValueError(f\"{filename} has an unsupported extension.\")\n</code></pre>"},{"location":"api-process.html","title":"cuisto.process","text":"<p>process module, part of cuisto.</p> <p>Wraps other functions for a click&amp;play behaviour. Relies on the configuration file.</p>"},{"location":"api-process.html#cuisto.process.process_animal","title":"<code>process_animal(animal, df_annotations, df_detections, cfg, compute_distributions=True)</code>","text":"<p>Quantify objects for one animal.</p> <p>Fetch required files and compute objects' distributions in brain regions, spatial distributions and gather Atlas coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>animal</code> <code>str</code> <p>Animal ID.</p> required <code>df_annotations</code> <code>DataFrame</code> <p>DataFrames of QuPath Annotations and Detections.</p> required <code>df_detections</code> <code>DataFrame</code> <p>DataFrames of QuPath Annotations and Detections.</p> required <code>cfg</code> <code>Config</code> <p>The configuration loaded from TOML configuration file.</p> required <code>compute_distributions</code> <code>bool</code> <p>If False, do not compute the 1D distributions and return an empty list.Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_regions</code> <code>DataFrame</code> <p>Metrics in brain regions. One entry for each hemisphere of each brain regions.</p> <code>df_distribution</code> <code>list of pandas.DataFrame</code> <p>Rostro-caudal distribution, as raw count and probability density function, in each axis.</p> <code>df_coordinates</code> <code>DataFrame</code> <p>Atlas coordinates of each points.</p> Source code in <code>cuisto/process.py</code> <pre><code>def process_animal(\n    animal: str,\n    df_annotations: pd.DataFrame,\n    df_detections: pd.DataFrame,\n    cfg,\n    compute_distributions: bool = True,\n) -&gt; tuple[pd.DataFrame, list[pd.DataFrame], pd.DataFrame]:\n    \"\"\"\n    Quantify objects for one animal.\n\n    Fetch required files and compute objects' distributions in brain regions, spatial\n    distributions and gather Atlas coordinates.\n\n    Parameters\n    ----------\n    animal : str\n        Animal ID.\n    df_annotations, df_detections : pd.DataFrame\n        DataFrames of QuPath Annotations and Detections.\n    cfg : cuisto.Config\n        The configuration loaded from TOML configuration file.\n    compute_distributions : bool, optional\n        If False, do not compute the 1D distributions and return an empty list.Default\n        is True.\n\n    Returns\n    -------\n    df_regions : pandas.DataFrame\n        Metrics in brain regions. One entry for each hemisphere of each brain regions.\n    df_distribution : list of pandas.DataFrame\n        Rostro-caudal distribution, as raw count and probability density function, in\n        each axis.\n    df_coordinates : pandas.DataFrame\n        Atlas coordinates of each points.\n\n    \"\"\"\n    # - Annotations data cleanup\n    # filter regions\n    df_annotations = utils.filter_df_regions(\n        df_annotations, [\"Root\", \"root\"], mode=\"remove\", col=\"Name\"\n    )\n    df_annotations = utils.filter_df_regions(\n        df_annotations, cfg.atlas[\"blacklist\"], mode=\"remove\", col=\"Name\"\n    )\n    # add hemisphere\n    df_annotations = utils.add_hemisphere(df_annotations, cfg.hemispheres[\"names\"])\n    # remove objects in non-leaf regions\n    df_annotations = utils.filter_df_regions(\n        df_annotations, cfg.atlas[\"leaveslist\"], mode=\"keep\", col=\"Name\"\n    )\n    # merge regions\n    df_annotations = utils.merge_regions(\n        df_annotations, col=\"Name\", fusion_file=cfg.files[\"fusion\"]\n    )\n    if compute_distributions:\n        # - Detections data cleanup\n        # remove objects not in selected classifications\n        df_detections = utils.filter_df_classifications(\n            df_detections, cfg.object_type, mode=\"keep\", col=\"Classification\"\n        )\n        # remove objects from blacklisted regions and \"Root\"\n        df_detections = utils.filter_df_regions(\n            df_detections, cfg.atlas[\"blacklist\"], mode=\"remove\", col=\"Parent\"\n        )\n        # add hemisphere\n        df_detections = utils.add_hemisphere(\n            df_detections,\n            cfg.hemispheres[\"names\"],\n            cfg.atlas[\"midline\"],\n            col=\"Atlas_Z\",\n            atlas_type=cfg.atlas[\"type\"],\n        )\n        # add detection channel\n        df_detections = utils.add_channel(\n            df_detections, cfg.object_type, cfg.channels[\"names\"]\n        )\n        # convert coordinates to mm\n        df_detections[[\"Atlas_X\", \"Atlas_Y\", \"Atlas_Z\"]] = df_detections[\n            [\"Atlas_X\", \"Atlas_Y\", \"Atlas_Z\"]\n        ].divide(1000)\n        # convert to sterotaxic coordinates\n        if cfg.distributions[\"stereo\"]:\n            (\n                df_detections[\"Atlas_AP\"],\n                df_detections[\"Atlas_DV\"],\n                df_detections[\"Atlas_ML\"],\n            ) = utils.ccf_to_stereo(\n                df_detections[\"Atlas_X\"],\n                df_detections[\"Atlas_Y\"],\n                df_detections[\"Atlas_Z\"],\n            )\n        else:\n            (\n                df_detections[\"Atlas_AP\"],\n                df_detections[\"Atlas_DV\"],\n                df_detections[\"Atlas_ML\"],\n            ) = (\n                df_detections[\"Atlas_X\"],\n                df_detections[\"Atlas_Y\"],\n                df_detections[\"Atlas_Z\"],\n            )\n\n    # - Computations\n    # get regions distributions\n    df_regions = compute.get_regions_metrics(\n        df_annotations,\n        cfg.object_type,\n        cfg.channels[\"names\"],\n        cfg.regions[\"base_measurement\"],\n        cfg.regions[\"metrics\"],\n    )\n    colstonorm = [v for v in cfg.regions[\"metrics\"].values() if \"relative\" not in v]\n\n    # normalize by starter cells\n    if cfg.regions[\"normalize_starter_cells\"]:\n        df_regions = compute.normalize_starter_cells(\n            df_regions, colstonorm, animal, cfg.files[\"infos\"], cfg.channels[\"names\"]\n        )\n\n    # get AP, DV, ML distributions in stereotaxic coordinates\n    if compute_distributions:\n        dfs_distributions = [\n            compute.get_distribution(\n                df_detections,\n                axis,\n                cfg.distributions[\"hue\"],\n                cfg.distributions[\"hue_filter\"],\n                cfg.distributions[\"common_norm\"],\n                stereo_lim,\n                nbins=nbins,\n            )\n            for axis, stereo_lim, nbins in zip(\n                [\"Atlas_AP\", \"Atlas_DV\", \"Atlas_ML\"],\n                [\n                    cfg.distributions[\"ap_lim\"],\n                    cfg.distributions[\"dv_lim\"],\n                    cfg.distributions[\"ml_lim\"],\n                ],\n                [\n                    cfg.distributions[\"ap_nbins\"],\n                    cfg.distributions[\"dv_nbins\"],\n                    cfg.distributions[\"dv_nbins\"],\n                ],\n            )\n        ]\n    else:\n        dfs_distributions = []\n\n    # add animal tag to each DataFrame\n    df_detections[\"animal\"] = animal\n    df_regions[\"animal\"] = animal\n    for df in dfs_distributions:\n        df[\"animal\"] = animal\n\n    return df_regions, dfs_distributions, df_detections\n</code></pre>"},{"location":"api-process.html#cuisto.process.process_animals","title":"<code>process_animals(wdir, animals, cfg, out_fmt=None, compute_distributions=True)</code>","text":"<p>Get data from all animals and plot.</p> <p>Parameters:</p> Name Type Description Default <code>wdir</code> <code>str</code> <p>Base working directory, containing <code>animals</code> folders.</p> required <code>animals</code> <code>list-like of str</code> <p>List of animals ID.</p> required <code>cfg</code> <p>Configuration object.</p> required <code>out_fmt</code> <code>(None, h5, csv, tsv, xslx, pickle)</code> <p>Output file(s) format, if None, nothing is saved (default).</p> <code>None</code> <code>compute_distributions</code> <code>bool</code> <p>If False, do not compute the 1D distributions and return an empty list.Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_regions</code> <code>DataFrame</code> <p>Metrics in brain regions. One entry for each hemisphere of each brain regions.</p> <code>df_distribution</code> <code>list of pandas.DataFrame</code> <p>Rostro-caudal distribution, as raw count and probability density function, in each axis.</p> <code>df_coordinates</code> <code>DataFrame</code> <p>Atlas coordinates of each points.</p> Source code in <code>cuisto/process.py</code> <pre><code>def process_animals(\n    wdir: str,\n    animals: list[str] | tuple[str],\n    cfg,\n    out_fmt: str | None = None,\n    compute_distributions: bool = True,\n) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Get data from all animals and plot.\n\n    Parameters\n    ----------\n    wdir : str\n        Base working directory, containing `animals` folders.\n    animals : list-like of str\n        List of animals ID.\n    cfg: cuisto.Config\n        Configuration object.\n    out_fmt : {None, \"h5\", \"csv\", \"tsv\", \"xslx\", \"pickle\"}\n        Output file(s) format, if None, nothing is saved (default).\n    compute_distributions : bool, optional\n        If False, do not compute the 1D distributions and return an empty list.Default\n        is True.\n\n\n    Returns\n    -------\n    df_regions : pandas.DataFrame\n        Metrics in brain regions. One entry for each hemisphere of each brain regions.\n    df_distribution : list of pandas.DataFrame\n        Rostro-caudal distribution, as raw count and probability density function, in\n        each axis.\n    df_coordinates : pandas.DataFrame\n        Atlas coordinates of each points.\n\n    \"\"\"\n\n    # -- Preparation\n    df_regions = []\n    dfs_distributions = []\n    df_coordinates = []\n\n    # -- Processing\n    pbar = tqdm(animals)\n\n    for animal in pbar:\n        pbar.set_description(f\"Processing {animal}\")\n\n        # combine all detections and annotations from this animal\n        df_annotations = io.cat_csv_dir(\n            io.get_measurements_directory(\n                wdir, animal, \"annotation\", cfg.segmentation_tag\n            ),\n            index_col=\"Object ID\",\n            sep=\"\\t\",\n        )\n        if compute_distributions:\n            df_detections = io.cat_data_dir(\n                io.get_measurements_directory(\n                    wdir, animal, \"detection\", cfg.segmentation_tag\n                ),\n                cfg.segmentation_tag,\n                index_col=\"Object ID\",\n                sep=\"\\t\",\n                hemisphere_names=cfg.hemispheres[\"names\"],\n                atlas=cfg.bg_atlas,\n            )\n        else:\n            df_detections = pd.DataFrame()\n\n        # get results\n        df_reg, dfs_dis, df_coo = process_animal(\n            animal,\n            df_annotations,\n            df_detections,\n            cfg,\n            compute_distributions=compute_distributions,\n        )\n\n        # collect results\n        df_regions.append(df_reg)\n        dfs_distributions.append(dfs_dis)\n        df_coordinates.append(df_coo)\n\n    # concatenate all results\n    df_regions = pd.concat(df_regions, ignore_index=True)\n    dfs_distributions = [\n        pd.concat(dfs_list, ignore_index=True) for dfs_list in zip(*dfs_distributions)\n    ]\n    df_coordinates = pd.concat(df_coordinates, ignore_index=True)\n\n    # -- Saving\n    if out_fmt:\n        outdir = os.path.join(wdir, \"quantification\")\n        outfile = f\"{cfg.object_type.lower()}_{cfg.atlas[\"type\"]}_{'-'.join(animals)}.{out_fmt}\"\n        dfs = dict(\n            df_regions=df_regions,\n            df_coordinates=df_coordinates,\n            df_distribution_ap=dfs_distributions[0],\n            df_distribution_dv=dfs_distributions[1],\n            df_distribution_ml=dfs_distributions[2],\n        )\n        io.save_dfs(outdir, outfile, dfs)\n\n    return df_regions, dfs_distributions, df_coordinates\n</code></pre>"},{"location":"api-script-qupath-script-runner.html","title":"qupath_script_runner","text":"<p>Template to show how to run groovy script with QuPath, multi-threaded.</p>"},{"location":"api-script-qupath-script-runner.html#scripts.qupath_script_template.EXCLUDE_LIST","title":"<code>EXCLUDE_LIST = []</code>  <code>module-attribute</code>","text":"<p>Images names to NOT run the script on.</p>"},{"location":"api-script-qupath-script-runner.html#scripts.qupath_script_template.NTHREADS","title":"<code>NTHREADS = 5</code>  <code>module-attribute</code>","text":"<p>Number of threads to use.</p>"},{"location":"api-script-qupath-script-runner.html#scripts.qupath_script_template.QPROJ_PATH","title":"<code>QPROJ_PATH = '/path/to/qupath/project.qproj'</code>  <code>module-attribute</code>","text":"<p>Full path to the QuPath project.</p>"},{"location":"api-script-qupath-script-runner.html#scripts.qupath_script_template.QUIET","title":"<code>QUIET = True</code>  <code>module-attribute</code>","text":"<p>Use QuPath in quiet mode, eg. with minimal verbosity.</p>"},{"location":"api-script-qupath-script-runner.html#scripts.qupath_script_template.QUPATH_EXE","title":"<code>QUPATH_EXE = '/path/to/the/qupath/QuPath-0.5.1 (console).exe'</code>  <code>module-attribute</code>","text":"<p>Path to the QuPath executable (console mode).</p>"},{"location":"api-script-qupath-script-runner.html#scripts.qupath_script_template.SAVE","title":"<code>SAVE = True</code>  <code>module-attribute</code>","text":"<p>Whether to save the project after the script ran on an image.</p>"},{"location":"api-script-qupath-script-runner.html#scripts.qupath_script_template.SCRIPT_PATH","title":"<code>SCRIPT_PATH = '/path/to/the/script.groovy'</code>  <code>module-attribute</code>","text":"<p>Path to the groovy script.</p>"},{"location":"api-script-segment.html","title":"segment_images","text":"<p>Script to segment objects from images.</p> <p>For fiber-like objects, binarize and skeletonize the image, then use <code>skan</code> to extract branches coordinates. For polygon-like objects, binarize the image and detect objects and extract contours coordinates. For points, treat that as polygons then extract the centroids instead of contours. Finally, export the coordinates as collections in geojson files, importable in QuPath. Supports any number of channel of interest within the same image. One file output file per channel will be created.</p> <p>This script uses <code>cuisto.seg</code>. It is designed to work on probability maps generated from a pixel classifier in QuPath, but might work on raw images.</p> <p>Usage : fill-in the Parameters section of the script and run it. A \"geojson\" folder will be created in the parent directory of <code>IMAGES_DIR</code>. To exclude objects near the edges of an ROI, specify the path to masks stored as images with the same names as probabilities images (without their suffix).</p> <p>author : Guillaume Le Goc (g.legoc@posteo.org) @ NeuroPSI version : 2024.12.10</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.CHANNELS_PARAMS","title":"<code>CHANNELS_PARAMS = [{'name': 'cy5', 'target_channel': 0, 'proba_threshold': 0.85, 'qp_class': 'Fibers: Cy5', 'qp_color': [164, 250, 120]}, {'name': 'dsred', 'target_channel': 1, 'proba_threshold': 0.65, 'qp_class': 'Fibers: DsRed', 'qp_color': [224, 153, 18]}, {'name': 'egfp', 'target_channel': 2, 'proba_threshold': 0.85, 'qp_class': 'Fibers: EGFP', 'qp_color': [135, 11, 191]}]</code>  <code>module-attribute</code>","text":"<p>This should be a list of dictionary (one per channel) with keys :</p> <ul> <li>name: str, used as suffix for output geojson files, not used if only one channel</li> <li>target_channel: int, index of the segmented channel of the image, 0-based</li> <li>proba_threshold: float &lt; 1, probability cut-off for that channel</li> <li>qp_class: str, name of QuPath classification</li> <li>qp_color: list of RGB values, associated color</li> </ul>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.EDGE_DIST","title":"<code>EDGE_DIST = 0</code>  <code>module-attribute</code>","text":"<p>Distance to brain edge to ignore, in \u00b5m. 0 to disable.</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.FILTERS","title":"<code>FILTERS = {'length_low': 1.5, 'area_low': 10, 'area_high': 1000, 'ecc_low': 0.0, 'ecc_high': 0.9, 'dist_thresh': 30}</code>  <code>module-attribute</code>","text":"<p>Dictionary with keys :</p> <ul> <li>length_low: minimal length in microns - for lines</li> <li>area_low: minimal area in \u00b5m\u00b2 - for polygons and points</li> <li>area_high: maximal area in \u00b5m\u00b2 - for polygons and points</li> <li>ecc_low: minimal eccentricity - for polygons  and points (0 = circle)</li> <li>ecc_high: maximal eccentricity - for polygons and points (1 = line)</li> <li>dist_thresh: maximal inter-point distance in \u00b5m - for points</li> </ul>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.IMAGES_DIR","title":"<code>IMAGES_DIR = '/path/to/images'</code>  <code>module-attribute</code>","text":"<p>Full path to the images to segment.</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.IMG_SUFFIX","title":"<code>IMG_SUFFIX = '_Probabilities.tiff'</code>  <code>module-attribute</code>","text":"<p>Images suffix, including extension. Masks must be the same name without the suffix.</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.MASKS_DIR","title":"<code>MASKS_DIR = 'path/to/corresponding/masks'</code>  <code>module-attribute</code>","text":"<p>Full path to the masks, to exclude objects near the brain edges (set to None or empty string to disable this feature).</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.MASKS_EXT","title":"<code>MASKS_EXT = 'tiff'</code>  <code>module-attribute</code>","text":"<p>Masks files extension.</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.MAX_PIX_VALUE","title":"<code>MAX_PIX_VALUE = 255</code>  <code>module-attribute</code>","text":"<p>Maximum pixel possible value to adjust <code>proba_threshold</code>.</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.ORIGINAL_PIXELSIZE","title":"<code>ORIGINAL_PIXELSIZE = 0.45</code>  <code>module-attribute</code>","text":"<p>Original images pixel size in microns. This is in case the pixel classifier uses a lower resolution, yielding smaller probability maps, so output objects coordinates need to be rescaled to the full size images. The pixel size is written in the \"Image\" tab in QuPath.</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.QUPATH_TYPE","title":"<code>QUPATH_TYPE = 'detection'</code>  <code>module-attribute</code>","text":"<p>QuPath object type.</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.SEGTYPE","title":"<code>SEGTYPE = 'boutons'</code>  <code>module-attribute</code>","text":"<p>Type of segmentation.</p>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.get_geojson_dir","title":"<code>get_geojson_dir(images_dir)</code>","text":"<p>Get the directory of geojson files, which will be in the parent directory of <code>images_dir</code>.</p> <p>If the directory does not exist, create it.</p> <p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>geojson_dir</code> <code>str</code> Source code in <code>scripts/segmentation/segment_images.py</code> <pre><code>def get_geojson_dir(images_dir: str):\n    \"\"\"\n    Get the directory of geojson files, which will be in the parent directory\n    of `images_dir`.\n\n    If the directory does not exist, create it.\n\n    Parameters\n    ----------\n    images_dir : str\n\n    Returns\n    -------\n    geojson_dir : str\n\n    \"\"\"\n\n    geojson_dir = os.path.join(Path(images_dir).parent, \"geojson\")\n\n    if not os.path.isdir(geojson_dir):\n        os.mkdir(geojson_dir)\n\n    return geojson_dir\n</code></pre>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.get_geojson_properties","title":"<code>get_geojson_properties(name, color, objtype='detection')</code>","text":"<p>Return geojson objects properties as a dictionnary, ready to be used in geojson.Feature.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Classification name.</p> required <code>color</code> <code>tuple or list</code> <p>Classification color in RGB (3-elements vector).</p> required <code>objtype</code> <code>str</code> <p>Object type (\"detection\" or \"annotation\"). Default is \"detection\".</p> <code>'detection'</code> <p>Returns:</p> Name Type Description <code>props</code> <code>dict</code> Source code in <code>scripts/segmentation/segment_images.py</code> <pre><code>def get_geojson_properties(name: str, color: tuple | list, objtype: str = \"detection\"):\n    \"\"\"\n    Return geojson objects properties as a dictionnary, ready to be used in geojson.Feature.\n\n    Parameters\n    ----------\n    name : str\n        Classification name.\n    color : tuple or list\n        Classification color in RGB (3-elements vector).\n    objtype : str, optional\n        Object type (\"detection\" or \"annotation\"). Default is \"detection\".\n\n    Returns\n    -------\n    props : dict\n\n    \"\"\"\n\n    return {\n        \"objectType\": objtype,\n        \"classification\": {\"name\": name, \"color\": color},\n        \"isLocked\": \"true\",\n    }\n</code></pre>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.get_seg_method","title":"<code>get_seg_method(segtype)</code>","text":"<p>Determine what kind of segmentation is performed.</p> <p>Segmentation kind are, for now, lines, polygons or points. We detect that based on hardcoded keywords.</p> <p>Parameters:</p> Name Type Description Default <code>segtype</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>seg_method</code> <code>str</code> Source code in <code>scripts/segmentation/segment_images.py</code> <pre><code>def get_seg_method(segtype: str):\n    \"\"\"\n    Determine what kind of segmentation is performed.\n\n    Segmentation kind are, for now, lines, polygons or points. We detect that based on\n    hardcoded keywords.\n\n    Parameters\n    ----------\n    segtype : str\n\n    Returns\n    -------\n    seg_method : str\n\n    \"\"\"\n\n    line_list = [\"fibers\", \"axons\", \"fiber\", \"axon\"]\n    point_list = [\"synapto\", \"synaptophysin\", \"syngfp\", \"boutons\", \"points\"]\n    polygon_list = [\"cells\", \"polygon\", \"polygons\", \"polygon\", \"cell\"]\n\n    if segtype in line_list:\n        seg_method = \"lines\"\n    elif segtype in polygon_list:\n        seg_method = \"polygons\"\n    elif segtype in point_list:\n        seg_method = \"points\"\n    else:\n        raise ValueError(\n            f\"Could not determine method to use based on segtype : {segtype}.\"\n        )\n\n    return seg_method\n</code></pre>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.parameters_as_dict","title":"<code>parameters_as_dict(images_dir, masks_dir, segtype, name, proba_threshold, edge_dist)</code>","text":"<p>Get information as a dictionnary.</p> <p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Path to images to be segmented.</p> required <code>masks_dir</code> <code>str</code> <p>Path to images masks.</p> required <code>segtype</code> <code>str</code> <p>Segmentation type (eg. \"fibers\").</p> required <code>name</code> <code>str</code> <p>Name of the segmentation (eg. \"green\").</p> required <code>proba_threshold</code> <code>float &lt; 1</code> <p>Probability threshold.</p> required <code>edge_dist</code> <code>float</code> <p>Distance in \u00b5m to the brain edge that is ignored.</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> Source code in <code>scripts/segmentation/segment_images.py</code> <pre><code>def parameters_as_dict(\n    images_dir: str,\n    masks_dir: str,\n    segtype: str,\n    name: str,\n    proba_threshold: float,\n    edge_dist: float,\n):\n    \"\"\"\n    Get information as a dictionnary.\n\n    Parameters\n    ----------\n    images_dir : str\n        Path to images to be segmented.\n    masks_dir : str\n        Path to images masks.\n    segtype : str\n        Segmentation type (eg. \"fibers\").\n    name : str\n        Name of the segmentation (eg. \"green\").\n    proba_threshold : float &lt; 1\n        Probability threshold.\n    edge_dist : float\n        Distance in \u00b5m to the brain edge that is ignored.\n\n    Returns\n    -------\n    params : dict\n\n    \"\"\"\n\n    return {\n        \"images_location\": images_dir,\n        \"masks_location\": masks_dir,\n        \"type\": segtype,\n        \"probability threshold\": proba_threshold,\n        \"name\": name,\n        \"edge distance\": edge_dist,\n    }\n</code></pre>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.process_directory","title":"<code>process_directory(images_dir, img_suffix='', segtype='', original_pixelsize=1.0, target_channel=0, proba_threshold=0.0, qupath_class='Object', qupath_color=[0, 0, 0], channel_suffix='', edge_dist=0.0, filters={}, masks_dir='', masks_ext='')</code>","text":"<p>Main function, processes the .ome.tiff files in the input directory.</p> <p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Animal ID to process.</p> required <code>img_suffix</code> <code>str</code> <p>Images suffix, including extension.</p> <code>''</code> <code>segtype</code> <code>str</code> <p>Segmentation type.</p> <code>''</code> <code>original_pixelsize</code> <code>float</code> <p>Original images pixel size in microns.</p> <code>1.0</code> <code>target_channel</code> <code>int</code> <p>Index of the channel containning the objects of interest (eg. not the background), in the probability map (not the original images channels).</p> <code>0</code> <code>proba_threshold</code> <code>float &lt; 1</code> <p>Probability below this value will be discarded (multiplied by <code>MAX_PIXEL_VALUE</code>)</p> <code>0.0</code> <code>qupath_class</code> <code>str</code> <p>Name of the QuPath classification.</p> <code>'Object'</code> <code>qupath_color</code> <code>list of three elements</code> <p>Color associated to that classification in RGB.</p> <code>[0, 0, 0]</code> <code>channel_suffix</code> <code>str</code> <p>Channel name, will be used as a suffix in output geojson files.</p> <code>''</code> <code>edge_dist</code> <code>float</code> <p>Distance to the edge of the brain masks that will be ignored, in microns. Set to 0 to disable this feature.</p> <code>0.0</code> <code>filters</code> <code>dict</code> <p>Filters values to include or excludes objects. See the top of the script.</p> <code>{}</code> <code>masks_dir</code> <code>str</code> <p>Path to images masks, to exclude objects found near the edges. The masks must be with the same name as the corresponding image to be segmented, without its suffix. Default is \"\", which disables this feature.</p> <code>''</code> <code>masks_ext</code> <code>str</code> <p>Masks files extension, without leading \".\". Default is \"\"</p> <code>''</code> Source code in <code>scripts/segmentation/segment_images.py</code> <pre><code>def process_directory(\n    images_dir: str,\n    img_suffix: str = \"\",\n    segtype: str = \"\",\n    original_pixelsize: float = 1.0,\n    target_channel: int = 0,\n    proba_threshold: float = 0.0,\n    qupath_class: str = \"Object\",\n    qupath_color: list = [0, 0, 0],\n    channel_suffix: str = \"\",\n    edge_dist: float = 0.0,\n    filters: dict = {},\n    masks_dir: str = \"\",\n    masks_ext: str = \"\",\n):\n    \"\"\"\n    Main function, processes the .ome.tiff files in the input directory.\n\n    Parameters\n    ----------\n    images_dir : str\n        Animal ID to process.\n    img_suffix : str\n        Images suffix, including extension.\n    segtype : str\n        Segmentation type.\n    original_pixelsize : float\n        Original images pixel size in microns.\n    target_channel : int\n        Index of the channel containning the objects of interest (eg. not the\n        background), in the probability map (*not* the original images channels).\n    proba_threshold : float &lt; 1\n        Probability below this value will be discarded (multiplied by `MAX_PIXEL_VALUE`)\n    qupath_class : str\n        Name of the QuPath classification.\n    qupath_color : list of three elements\n        Color associated to that classification in RGB.\n    channel_suffix : str\n        Channel name, will be used as a suffix in output geojson files.\n    edge_dist : float\n        Distance to the edge of the brain masks that will be ignored, in microns. Set to\n        0 to disable this feature.\n    filters : dict\n        Filters values to include or excludes objects. See the top of the script.\n    masks_dir : str, optional\n        Path to images masks, to exclude objects found near the edges. The masks must be\n        with the same name as the corresponding image to be segmented, without its\n        suffix. Default is \"\", which disables this feature.\n    masks_ext : str, optional\n        Masks files extension, without leading \".\". Default is \"\"\n\n    \"\"\"\n\n    # -- Preparation\n    # get segmentation type\n    seg_method = get_seg_method(segtype)\n\n    # get output directory path\n    geojson_dir = get_geojson_dir(images_dir)\n\n    # get images list\n    images_list = [\n        os.path.join(images_dir, filename)\n        for filename in os.listdir(images_dir)\n        if filename.endswith(img_suffix)\n    ]\n\n    # write parameters\n    parameters = parameters_as_dict(\n        images_dir, masks_dir, segtype, channel_suffix, proba_threshold, edge_dist\n    )\n    param_file = os.path.join(geojson_dir, \"parameters\" + channel_suffix + \".txt\")\n    if os.path.isfile(param_file):\n        raise FileExistsError(\"Parameters file already exists.\")\n    else:\n        write_parameters(param_file, parameters, filters, original_pixelsize)\n\n    # convert parameters to pixels in probability map\n    pixelsize = hq.seg.get_pixelsize(images_list[0])  # get pixel size\n    edge_dist = int(edge_dist / pixelsize)\n    filters = hq.seg.convert_to_pixels(filters, pixelsize)\n\n    # get rescaling factor\n    rescale_factor = pixelsize / original_pixelsize\n\n    # get GeoJSON properties\n    geojson_props = get_geojson_properties(\n        qupath_class, qupath_color, objtype=QUPATH_TYPE\n    )\n\n    # -- Processing\n    pbar = tqdm(images_list)\n    for imgpath in pbar:\n        # build file names\n        imgname = os.path.basename(imgpath)\n        geoname = imgname.replace(img_suffix, \"\")\n        geojson_file = os.path.join(\n            geojson_dir, geoname + \"_segmentation\" + channel_suffix + \".geojson\"\n        )\n\n        # checks if output file already exists\n        if os.path.isfile(geojson_file):\n            continue\n\n        # read images\n        pbar.set_description(f\"{geoname}: Loading...\")\n        img = tifffile.imread(imgpath, key=target_channel)\n        if (edge_dist &gt; 0) &amp; (len(masks_dir) != 0):\n            mask = tifffile.imread(os.path.join(masks_dir, geoname + \".\" + masks_ext))\n            mask = hq.seg.pad_image(mask, img.shape)  # resize mask\n            # apply mask, eroding from the edges\n            img = img * hq.seg.erode_mask(mask, edge_dist)\n\n        # image processing\n        pbar.set_description(f\"{geoname}: IP...\")\n\n        # threshold probability and binarization\n        img = img &gt;= proba_threshold * MAX_PIX_VALUE\n\n        # segmentation\n        pbar.set_description(f\"{geoname}: Segmenting...\")\n\n        if seg_method == \"lines\":\n            collection = hq.seg.segment_lines(\n                img,\n                geojson_props,\n                minsize=filters[\"length_low\"],\n                rescale_factor=rescale_factor,\n            )\n\n        elif seg_method == \"polygons\":\n            collection = hq.seg.segment_polygons(\n                img,\n                geojson_props,\n                area_min=filters[\"area_low\"],\n                area_max=filters[\"area_high\"],\n                ecc_min=filters[\"ecc_low\"],\n                ecc_max=filters[\"ecc_high\"],\n                rescale_factor=rescale_factor,\n            )\n\n        elif seg_method == \"points\":\n            collection = hq.seg.segment_points(\n                img,\n                geojson_props,\n                area_min=filters[\"area_low\"],\n                area_max=filters[\"area_high\"],\n                ecc_min=filters[\"ecc_low\"],\n                ecc_max=filters[\"ecc_high\"],\n                dist_thresh=filters[\"dist_thresh\"],\n                rescale_factor=rescale_factor,\n            )\n        else:\n            # we already printed an error message\n            return\n\n        # save geojson\n        pbar.set_description(f\"{geoname}: Saving...\")\n        with open(geojson_file, \"w\") as fid:\n            fid.write(geojson.dumps(collection))\n</code></pre>"},{"location":"api-script-segment.html#scripts.segmentation.segment_images.write_parameters","title":"<code>write_parameters(outfile, parameters, filters, original_pixelsize)</code>","text":"<p>Write parameters to <code>outfile</code>.</p> <p>A timestamp will be added. Parameters are written as key = value, and a [filters] is added before filters parameters.</p> <p>Parameters:</p> Name Type Description Default <code>outfile</code> <code>str</code> <p>Full path to the output file.</p> required <code>parameters</code> <code>dict</code> <p>General parameters.</p> required <code>filters</code> <code>dict</code> <p>Filters parameters.</p> required <code>original_pixelsize</code> <code>float</code> <p>Size of pixels in original image.</p> required Source code in <code>scripts/segmentation/segment_images.py</code> <pre><code>def write_parameters(\n    outfile: str, parameters: dict, filters: dict, original_pixelsize: float\n):\n    \"\"\"\n    Write parameters to `outfile`.\n\n    A timestamp will be added. Parameters are written as key = value,\n    and a [filters] is added before filters parameters.\n\n    Parameters\n    ----------\n    outfile : str\n        Full path to the output file.\n    parameters : dict\n        General parameters.\n    filters : dict\n        Filters parameters.\n    original_pixelsize : float\n        Size of pixels in original image.\n\n    \"\"\"\n\n    with open(outfile, \"w\") as fid:\n        fid.writelines(f\"date = {datetime.now().strftime('%d-%B-%Y %H:%M:%S')}\\n\")\n\n        fid.writelines(f\"original_pixelsize = {original_pixelsize}\\n\")\n\n        for key, value in parameters.items():\n            fid.writelines(f\"{key} = {value}\\n\")\n\n        fid.writelines(\"[filters]\\n\")\n\n        for key, value in filters.items():\n            fid.writelines(f\"{key} = {value}\\n\")\n</code></pre>"},{"location":"api-seg.html","title":"cuisto.seg","text":"<p>seg module, part of cuisto.</p> <p>Functions for segmentating probability map stored as an image.</p>"},{"location":"api-seg.html#cuisto.seg.convert_to_pixels","title":"<code>convert_to_pixels(filters, pixelsize)</code>","text":"<p>Convert some values in <code>filters</code> in pixels.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>Must contain the keys used below.</p> required <code>pixelsize</code> <code>float</code> <p>Pixel size in microns.</p> required <p>Returns:</p> Name Type Description <code>filters</code> <code>dict</code> <p>Same as input, with values in pixels.</p> Source code in <code>cuisto/seg.py</code> <pre><code>def convert_to_pixels(filters, pixelsize):\n    \"\"\"\n    Convert some values in `filters` in pixels.\n\n    Parameters\n    ----------\n    filters : dict\n        Must contain the keys used below.\n    pixelsize : float\n        Pixel size in microns.\n\n    Returns\n    -------\n    filters : dict\n        Same as input, with values in pixels.\n\n    \"\"\"\n\n    filters[\"area_low\"] = filters[\"area_low\"] / pixelsize**2\n    filters[\"area_high\"] = filters[\"area_high\"] / pixelsize**2\n    filters[\"length_low\"] = filters[\"length_low\"] / pixelsize\n    filters[\"dist_thresh\"] = int(filters[\"dist_thresh\"] / pixelsize)\n\n    return filters\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.erode_mask","title":"<code>erode_mask(mask, edge_dist)</code>","text":"<p>Erode the mask outline so that is is <code>edge_dist</code> smaller from the border.</p> <p>This allows discarding the edges.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> required <code>edge_dist</code> <code>float</code> <p>Distance to edges, in pixels.</p> required <p>Returns:</p> Name Type Description <code>eroded_mask</code> <code>ndarray of bool</code> Source code in <code>cuisto/seg.py</code> <pre><code>def erode_mask(mask: np.ndarray, edge_dist: float) -&gt; np.ndarray:\n    \"\"\"\n    Erode the mask outline so that is is `edge_dist` smaller from the border.\n\n    This allows discarding the edges.\n\n    Parameters\n    ----------\n    mask : ndarray\n    edge_dist : float\n        Distance to edges, in pixels.\n\n    Returns\n    -------\n    eroded_mask : ndarray of bool\n\n    \"\"\"\n\n    if edge_dist % 2 == 0:\n        edge_dist += 1  # decomposition requires even number\n\n    footprint = morphology.square(edge_dist, decomposition=\"sequence\")\n\n    return mask * morphology.binary_erosion(mask, footprint=footprint)\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.get_collection_from_points","title":"<code>get_collection_from_points(coords, properties, rescale_factor=1.0, offset=0.5)</code>","text":"<p>Gather coordinates from <code>coords</code> and put them in GeoJSON format.</p> <p>An entry in <code>coords</code> are pairs of (x, y) coordinates defining the point. <code>properties</code> is a dictionnary with QuPath properties of each detections.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> required <code>properties</code> <code>dict</code> required <code>rescale_factor</code> <code>float</code> <p>Rescale output coordinates by this factor.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>collection</code> <code>FeatureCollection</code> Source code in <code>cuisto/seg.py</code> <pre><code>def get_collection_from_points(\n    coords: list, properties: dict, rescale_factor: float = 1.0, offset: float = 0.5\n) -&gt; geojson.FeatureCollection:\n    \"\"\"\n    Gather coordinates from `coords` and put them in GeoJSON format.\n\n    An entry in `coords` are pairs of (x, y) coordinates defining the point.\n    `properties` is a dictionnary with QuPath properties of each detections.\n\n    Parameters\n    ----------\n    coords : list\n    properties : dict\n    rescale_factor : float\n        Rescale output coordinates by this factor.\n\n    Returns\n    -------\n    collection : geojson.FeatureCollection\n\n    \"\"\"\n\n    collection = [\n        geojson.Feature(\n            geometry=shapely.Point(\n                np.flip((coord + offset) * rescale_factor)\n            ),  # shape object\n            properties=properties,  # object properties\n            id=str(uuid.uuid4()),  # object uuid\n        )\n        for coord in coords\n    ]\n\n    return geojson.FeatureCollection(collection)\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.get_collection_from_poly","title":"<code>get_collection_from_poly(contours, properties, rescale_factor=1.0, offset=0.5)</code>","text":"<p>Gather coordinates in the list and put them in GeoJSON format as Polygons.</p> <p>An entry in <code>contours</code> must define a closed polygon. <code>properties</code> is a dictionnary with QuPath properties of each detections.</p> <p>Parameters:</p> Name Type Description Default <code>contours</code> <code>list</code> required <code>properties</code> <code>dict</code> <p>QuPatj objects' properties.</p> required <code>rescale_factor</code> <code>float</code> <p>Rescale output coordinates by this factor.</p> <code>1.0</code> <code>offset</code> <code>float</code> <p>Shift coordinates by this amount, typically to get pixel centers or edges. Default is 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>collection</code> <code>FeatureCollection</code> <p>A FeatureCollection ready to be written as geojson.</p> Source code in <code>cuisto/seg.py</code> <pre><code>def get_collection_from_poly(\n    contours: list, properties: dict, rescale_factor: float = 1.0, offset: float = 0.5\n) -&gt; geojson.FeatureCollection:\n    \"\"\"\n    Gather coordinates in the list and put them in GeoJSON format as Polygons.\n\n    An entry in `contours` must define a closed polygon. `properties` is a dictionnary\n    with QuPath properties of each detections.\n\n    Parameters\n    ----------\n    contours : list\n    properties : dict\n        QuPatj objects' properties.\n    rescale_factor : float\n        Rescale output coordinates by this factor.\n    offset : float\n        Shift coordinates by this amount, typically to get pixel centers or edges.\n        Default is 0.5.\n\n    Returns\n    -------\n    collection : geojson.FeatureCollection\n        A FeatureCollection ready to be written as geojson.\n\n    \"\"\"\n    collection = [\n        geojson.Feature(\n            geometry=shapely.Polygon(\n                np.fliplr((contour + offset) * rescale_factor)\n            ),  # shape object\n            properties=properties,  # object properties\n            id=str(uuid.uuid4()),  # object uuid\n        )\n        for contour in contours\n    ]\n\n    return geojson.FeatureCollection(collection)\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.get_collection_from_skel","title":"<code>get_collection_from_skel(skeleton, properties, rescale_factor=1.0, offset=0.5)</code>","text":"<p>Get the coordinates of each skeleton path as a GeoJSON Features in a FeatureCollection. <code>properties</code> is a dictionnary with QuPath properties of each detections.</p> <p>Parameters:</p> Name Type Description Default <code>skeleton</code> <code>Skeleton</code> required <code>properties</code> <code>dict</code> <p>QuPatj objects' properties.</p> required <code>rescale_factor</code> <code>float</code> <p>Rescale output coordinates by this factor.</p> <code>1.0</code> <code>offset</code> <code>float</code> <p>Shift coordinates by this amount, typically to get pixel centers or edges. Default is 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>collection</code> <code>FeatureCollection</code> <p>A FeatureCollection ready to be written as geojson.</p> Source code in <code>cuisto/seg.py</code> <pre><code>def get_collection_from_skel(\n    skeleton: Skeleton, properties: dict, rescale_factor: float = 1.0, offset=0.5\n) -&gt; geojson.FeatureCollection:\n    \"\"\"\n    Get the coordinates of each skeleton path as a GeoJSON Features in a\n    FeatureCollection.\n    `properties` is a dictionnary with QuPath properties of each detections.\n\n    Parameters\n    ----------\n    skeleton : skan.Skeleton\n    properties : dict\n        QuPatj objects' properties.\n    rescale_factor : float\n        Rescale output coordinates by this factor.\n    offset : float\n        Shift coordinates by this amount, typically to get pixel centers or edges.\n        Default is 0.5.\n\n    Returns\n    -------\n    collection : geojson.FeatureCollection\n        A FeatureCollection ready to be written as geojson.\n\n    \"\"\"\n\n    branch_data = summarize(skeleton, separator=\"_\")\n\n    collection = []\n    for ind in range(skeleton.n_paths):\n        prop = properties.copy()\n        prop[\"measurements\"] = {\"skeleton_id\": int(branch_data.loc[ind, \"skeleton_id\"])}\n        collection.append(\n            geojson.Feature(\n                geometry=shapely.LineString(\n                    (skeleton.path_coordinates(ind)[:, ::-1] + offset) * rescale_factor\n                ),  # shape object\n                properties=prop,  # object properties\n                id=str(uuid.uuid4()),  # object uuid\n            )\n        )\n\n    return geojson.FeatureCollection(collection)\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.get_image_skeleton","title":"<code>get_image_skeleton(img, minsize=0)</code>","text":"<p>Get the image skeleton.</p> <p>Computes the image skeleton and removes objects smaller than <code>minsize</code>.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray of bool</code> required <code>minsize</code> <code>number</code> <p>Min. size the object can have, as a number of pixels. Default is 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>skel</code> <code>ndarray of bool</code> <p>Binary image with 1-pixel wide skeleton.</p> Source code in <code>cuisto/seg.py</code> <pre><code>def get_image_skeleton(img: np.ndarray, minsize=0) -&gt; np.ndarray:\n    \"\"\"\n    Get the image skeleton.\n\n    Computes the image skeleton and removes objects smaller than `minsize`.\n\n    Parameters\n    ----------\n    img : ndarray of bool\n    minsize : number, optional\n        Min. size the object can have, as a number of pixels. Default is 0.\n\n    Returns\n    -------\n    skel : ndarray of bool\n        Binary image with 1-pixel wide skeleton.\n\n    \"\"\"\n\n    skel = morphology.skeletonize(img)\n\n    return morphology.remove_small_objects(skel, min_size=minsize, connectivity=2)\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.get_pixelsize","title":"<code>get_pixelsize(image_name)</code>","text":"<p>Get pixel size recorded in <code>image_name</code> TIFF metadata.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>Full path to image.</p> required <p>Returns:</p> Name Type Description <code>pixelsize</code> <code>float</code> <p>Pixel size in microns.</p> Source code in <code>cuisto/seg.py</code> <pre><code>def get_pixelsize(image_name: str) -&gt; float:\n    \"\"\"\n    Get pixel size recorded in `image_name` TIFF metadata.\n\n    Parameters\n    ----------\n    image_name : str\n        Full path to image.\n\n    Returns\n    -------\n    pixelsize : float\n        Pixel size in microns.\n\n    \"\"\"\n\n    with tifffile.TiffFile(image_name) as tif:\n        # XResolution is a tuple, numerator, denomitor. The inverse is the pixel size\n        return (\n            tif.pages[0].tags[\"XResolution\"].value[1]\n            / tif.pages[0].tags[\"XResolution\"].value[0]\n        )\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.pad_image","title":"<code>pad_image(img, finalsize)</code>","text":"<p>Pad image with zeroes to match expected final size.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> required <code>finalsize</code> <code>tuple or list</code> <p>nrows, ncolumns</p> required <p>Returns:</p> Name Type Description <code>imgpad</code> <code>ndarray</code> <p>img with black borders.</p> Source code in <code>cuisto/seg.py</code> <pre><code>def pad_image(img: np.ndarray, finalsize: tuple | list) -&gt; np.ndarray:\n    \"\"\"\n    Pad image with zeroes to match expected final size.\n\n    Parameters\n    ----------\n    img : ndarray\n    finalsize : tuple or list\n        nrows, ncolumns\n\n    Returns\n    -------\n    imgpad : ndarray\n        img with black borders.\n\n    \"\"\"\n\n    final_h = finalsize[0]  # requested number of rows (height)\n    final_w = finalsize[1]  # requested number of columns (width)\n    original_h = img.shape[0]  # input number of rows\n    original_w = img.shape[1]  # input number of columns\n\n    a = (final_h - original_h) // 2  # vertical padding before\n    aa = final_h - a - original_h  # vertical padding after\n    b = (final_w - original_w) // 2  # horizontal padding before\n    bb = final_w - b - original_w  # horizontal padding after\n\n    return np.pad(img, pad_width=((a, aa), (b, bb)), mode=\"constant\")\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.segment_lines","title":"<code>segment_lines(img, geojson_props, minsize=0.0, rescale_factor=1.0)</code>","text":"<p>Wraps skeleton analysis to get paths coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray of bool</code> <p>Binary image to segment as lines.</p> required <code>geojson_props</code> <code>dict</code> <p>GeoJSON properties of objects.</p> required <code>minsize</code> <code>float</code> <p>Minimum size in pixels for an object.</p> <code>0.0</code> <code>rescale_factor</code> <code>float</code> <p>Rescale output coordinates by this factor.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>collection</code> <code>FeatureCollection</code> <p>A FeatureCollection ready to be written as geojson.</p> Source code in <code>cuisto/seg.py</code> <pre><code>def segment_lines(\n    img: np.ndarray, geojson_props: dict, minsize=0.0, rescale_factor=1.0\n) -&gt; geojson.FeatureCollection:\n    \"\"\"\n    Wraps skeleton analysis to get paths coordinates.\n\n    Parameters\n    ----------\n    img : ndarray of bool\n        Binary image to segment as lines.\n    geojson_props : dict\n        GeoJSON properties of objects.\n    minsize : float\n        Minimum size in pixels for an object.\n    rescale_factor : float\n        Rescale output coordinates by this factor.\n\n    Returns\n    -------\n    collection : geojson.FeatureCollection\n        A FeatureCollection ready to be written as geojson.\n\n    \"\"\"\n\n    skel = get_image_skeleton(img, minsize=minsize)\n\n    # get paths coordinates as FeatureCollection\n    skeleton = Skeleton(skel, keep_images=False)\n    return get_collection_from_skel(\n        skeleton, geojson_props, rescale_factor=rescale_factor\n    )\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.segment_points","title":"<code>segment_points(img, geojson_props, area_min=0.0, area_max=np.inf, ecc_min=0, ecc_max=1, dist_thresh=0, rescale_factor=1)</code>","text":"<p>Point segmentation.</p> <p>First, segment polygons to apply shape filters, then extract their centroids, and remove isolated points as defined by <code>dist_thresh</code>.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray of bool</code> <p>Binary image to segment as points.</p> required <code>geojson_props</code> <code>dict</code> <p>GeoJSON properties of objects.</p> required <code>area_min</code> <code>float</code> <p>Minimum and maximum area in pixels for an object.</p> <code>0.0</code> <code>area_max</code> <code>float</code> <p>Minimum and maximum area in pixels for an object.</p> <code>0.0</code> <code>ecc_min</code> <code>float</code> <p>Minimum and maximum eccentricity for an object.</p> <code>0</code> <code>ecc_max</code> <code>float</code> <p>Minimum and maximum eccentricity for an object.</p> <code>0</code> <code>dist_thresh</code> <code>float</code> <p>Maximal distance in pixels between objects before considering them as isolated and remove them. 0 disables it.</p> <code>0</code> <code>rescale_factor</code> <code>float</code> <p>Rescale output coordinates by this factor.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>collection</code> <code>FeatureCollection</code> <p>A FeatureCollection ready to be written as geojson.</p> Source code in <code>cuisto/seg.py</code> <pre><code>def segment_points(\n    img: np.ndarray,\n    geojson_props: dict,\n    area_min: float = 0.0,\n    area_max: float = np.inf,\n    ecc_min: float = 0,\n    ecc_max: float = 1,\n    dist_thresh: float = 0,\n    rescale_factor: float = 1,\n) -&gt; geojson.FeatureCollection:\n    \"\"\"\n    Point segmentation.\n\n    First, segment polygons to apply shape filters, then extract their centroids,\n    and remove isolated points as defined by `dist_thresh`.\n\n    Parameters\n    ----------\n    img : ndarray of bool\n        Binary image to segment as points.\n    geojson_props : dict\n        GeoJSON properties of objects.\n    area_min, area_max : float\n        Minimum and maximum area in pixels for an object.\n    ecc_min, ecc_max : float\n        Minimum and maximum eccentricity for an object.\n    dist_thresh : float\n        Maximal distance in pixels between objects before considering them as isolated and remove them.\n        0 disables it.\n    rescale_factor : float\n        Rescale output coordinates by this factor.\n\n    Returns\n    -------\n    collection : geojson.FeatureCollection\n        A FeatureCollection ready to be written as geojson.\n\n    \"\"\"\n\n    # get objects properties\n    stats = pd.DataFrame(\n        measure.regionprops_table(\n            measure.label(img), properties=(\"label\", \"area\", \"eccentricity\", \"centroid\")\n        )\n    )\n\n    # keep objects matching filters\n    stats = stats[\n        (stats[\"area\"] &gt;= area_min)\n        &amp; (stats[\"area\"] &lt;= area_max)\n        &amp; (stats[\"eccentricity\"] &gt;= ecc_min)\n        &amp; (stats[\"eccentricity\"] &lt;= ecc_max)\n    ]\n\n    # create an image from centroids only\n    stats[\"centroid-0\"] = stats[\"centroid-0\"].astype(int)\n    stats[\"centroid-1\"] = stats[\"centroid-1\"].astype(int)\n    bw = np.zeros(img.shape, dtype=bool)\n    bw[stats[\"centroid-0\"], stats[\"centroid-1\"]] = True\n\n    # filter isolated objects\n    if dist_thresh:\n        # dilation of points\n        if dist_thresh % 2 == 0:\n            dist_thresh += 1  # decomposition requires even number\n\n        footprint = morphology.square(int(dist_thresh), decomposition=\"sequence\")\n        dilated = measure.label(morphology.binary_dilation(bw, footprint=footprint))\n        stats = pd.DataFrame(\n            measure.regionprops_table(dilated, properties=(\"label\", \"area\"))\n        )\n\n        # objects that did not merge are alone\n        toremove = stats[(stats[\"area\"] &lt;= dist_thresh**2)]\n        dilated[np.isin(dilated, toremove[\"label\"])] = 0  # remove them\n\n        # apply mask\n        bw = bw * dilated\n\n    # get points coordinates\n    coords = np.argwhere(bw)\n\n    return get_collection_from_points(\n        coords, geojson_props, rescale_factor=rescale_factor\n    )\n</code></pre>"},{"location":"api-seg.html#cuisto.seg.segment_polygons","title":"<code>segment_polygons(img, geojson_props, area_min=0.0, area_max=np.inf, ecc_min=0.0, ecc_max=1.0, rescale_factor=1.0)</code>","text":"<p>Polygon segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray of bool</code> <p>Binary image to segment as polygons.</p> required <code>geojson_props</code> <code>dict</code> <p>GeoJSON properties of objects.</p> required <code>area_min</code> <code>float</code> <p>Minimum and maximum area in pixels for an object.</p> <code>0.0</code> <code>area_max</code> <code>float</code> <p>Minimum and maximum area in pixels for an object.</p> <code>0.0</code> <code>ecc_min</code> <code>float</code> <p>Minimum and maximum eccentricity for an object.</p> <code>0.0</code> <code>ecc_max</code> <code>float</code> <p>Minimum and maximum eccentricity for an object.</p> <code>0.0</code> <code>rescale_factor</code> <code>float</code> <p>Rescale output coordinates by this factor.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>collection</code> <code>FeatureCollection</code> <p>A FeatureCollection ready to be written as geojson.</p> Source code in <code>cuisto/seg.py</code> <pre><code>def segment_polygons(\n    img: np.ndarray,\n    geojson_props: dict,\n    area_min: float = 0.0,\n    area_max: float = np.inf,\n    ecc_min: float = 0.0,\n    ecc_max: float = 1.0,\n    rescale_factor: float = 1.0,\n) -&gt; geojson.FeatureCollection:\n    \"\"\"\n    Polygon segmentation.\n\n    Parameters\n    ----------\n    img : ndarray of bool\n        Binary image to segment as polygons.\n    geojson_props : dict\n        GeoJSON properties of objects.\n    area_min, area_max : float\n        Minimum and maximum area in pixels for an object.\n    ecc_min, ecc_max : float\n        Minimum and maximum eccentricity for an object.\n    rescale_factor: float\n        Rescale output coordinates by this factor.\n\n    Returns\n    -------\n    collection : geojson.FeatureCollection\n        A FeatureCollection ready to be written as geojson.\n\n    \"\"\"\n\n    label_image = measure.label(img)\n\n    # get objects properties\n    stats = pd.DataFrame(\n        measure.regionprops_table(\n            label_image, properties=(\"label\", \"area\", \"eccentricity\")\n        )\n    )\n\n    # remove objects not matching filters\n    toremove = stats[\n        (stats[\"area\"] &lt; area_min)\n        | (stats[\"area\"] &gt; area_max)\n        | (stats[\"eccentricity\"] &lt; ecc_min)\n        | (stats[\"eccentricity\"] &gt; ecc_max)\n    ]\n\n    label_image[np.isin(label_image, toremove[\"label\"])] = 0\n\n    # find objects countours\n    label_image = label_image &gt; 0\n    contours = measure.find_contours(label_image)\n\n    return get_collection_from_poly(\n        contours, geojson_props, rescale_factor=rescale_factor\n    )\n</code></pre>"},{"location":"api-utils.html","title":"cuisto.utils","text":"<p>utils module, part of cuisto.</p> <p>Contains utilities functions.</p>"},{"location":"api-utils.html#cuisto.utils.add_brain_region","title":"<code>add_brain_region(df, atlas, col='Parent')</code>","text":"<p>Add brain region to a DataFrame with <code>Atlas_X</code>, <code>Atlas_Y</code> and <code>Atlas_Z</code> columns.</p> <p>This uses Brainglobe Atlas API to query the atlas. It does not use the structure_from_coords() method, instead it manually converts the coordinates in stack indices, then get the corresponding annotation id and query the corresponding acronym -- because brainglobe-atlasapi is not vectorized at all.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with atlas coordinates in microns.</p> required <code>atlas</code> <code>BrainGlobeAtlas</code> required <code>col</code> <code>str</code> <p>Column in which to put the regions acronyms. Default is \"Parent\".</p> <code>'Parent'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Same DataFrame with a new \"Parent\" column.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def add_brain_region(\n    df: pd.DataFrame, atlas: BrainGlobeAtlas, col=\"Parent\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add brain region to a DataFrame with `Atlas_X`, `Atlas_Y` and `Atlas_Z` columns.\n\n    This uses Brainglobe Atlas API to query the atlas. It does not use the\n    structure_from_coords() method, instead it manually converts the coordinates in\n    stack indices, then get the corresponding annotation id and query the corresponding\n    acronym -- because brainglobe-atlasapi is not vectorized at all.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame with atlas coordinates in microns.\n    atlas : BrainGlobeAtlas\n    col : str, optional\n        Column in which to put the regions acronyms. Default is \"Parent\".\n\n    Returns\n    -------\n    df : pd.DataFrame\n        Same DataFrame with a new \"Parent\" column.\n\n    \"\"\"\n    df_in = df.copy()\n\n    res = atlas.resolution  # microns &lt;-&gt; pixels conversion\n    lims = atlas.shape_um  # out of brain\n\n    # set out-of-brain objects at 0 so we get \"root\" as their parent\n    df_in.loc[(df_in[\"Atlas_X\"] &gt;= lims[0]) | (df_in[\"Atlas_X\"] &lt; 0), \"Atlas_X\"] = 0\n    df_in.loc[(df_in[\"Atlas_Y\"] &gt;= lims[1]) | (df_in[\"Atlas_Y\"] &lt; 0), \"Atlas_Y\"] = 0\n    df_in.loc[(df_in[\"Atlas_Z\"] &gt;= lims[2]) | (df_in[\"Atlas_Z\"] &lt; 0), \"Atlas_Z\"] = 0\n\n    # build the multi index, in pixels and integers\n    ixyz = (\n        df_in[\"Atlas_X\"].divide(res[0]).astype(int),\n        df_in[\"Atlas_Y\"].divide(res[1]).astype(int),\n        df_in[\"Atlas_Z\"].divide(res[2]).astype(int),\n    )\n    # convert i, j, k indices in raveled indices\n    linear_indices = np.ravel_multi_index(ixyz, dims=atlas.annotation.shape)\n    # get the structure id from the annotation stack\n    idlist = atlas.annotation.ravel()[linear_indices]\n    # replace 0 which does not exist to 997 (root)\n    idlist[idlist == 0] = 997\n\n    # query the corresponding acronyms\n    lookup = atlas.lookup_df.set_index(\"id\")\n    df.loc[:, col] = lookup.loc[idlist, \"acronym\"].values\n\n    return df\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.add_channel","title":"<code>add_channel(df, object_type, channel_names)</code>","text":"<p>Add channel as a measurement for detections DataFrame.</p> <p>The channel is read from the Classification column, the latter having to be formatted as \"object_type: channel\".</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with detections measurements.</p> required <code>object_type</code> <code>str</code> <p>Object type (primary classification).</p> required <code>channel_names</code> <code>dict</code> <p>Map between original channel names to something else.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Same DataFrame with a \"channel\" column.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def add_channel(\n    df: pd.DataFrame, object_type: str, channel_names: dict\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add channel as a measurement for detections DataFrame.\n\n    The channel is read from the Classification column, the latter having to be\n    formatted as \"object_type: channel\".\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame with detections measurements.\n    object_type : str\n        Object type (primary classification).\n    channel_names : dict\n        Map between original channel names to something else.\n\n    Returns\n    -------\n    pd.DataFrame\n        Same DataFrame with a \"channel\" column.\n\n    \"\"\"\n    # check if there is something to do\n    if \"channel\" in df.columns:\n        return df\n\n    kind = get_df_kind(df)\n    if kind == \"annotation\":\n        warnings.warn(\"Annotation DataFrame not supported.\")\n        return df\n\n    # add channel, from {class_name: channel} classification\n    df[\"channel\"] = (\n        df[\"Classification\"].str.replace(object_type + \": \", \"\").map(channel_names)\n    )\n\n    return df\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.add_hemisphere","title":"<code>add_hemisphere(df, hemisphere_names, midline=5700, col='Atlas_Z', atlas_type='brain')</code>","text":"<p>Add hemisphere (left/right) as a measurement for detections or annotations.</p> <p>The hemisphere is read in the \"Classification\" column for annotations. The latter needs to be in the form \"Right: Name\" or \"Left: Name\". For detections, the input <code>col</code> of <code>df</code> is compared to <code>midline</code> to assess if the object belong to the left or right hemispheres.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with detections or annotations measurements.</p> required <code>hemisphere_names</code> <code>dict</code> <p>Map between \"Left\" and \"Right\" to something else.</p> required <code>midline</code> <code>float</code> <p>Used only for \"detections\" <code>df</code>. Corresponds to the brain midline in microns, should be 5700 for CCFv3 and 1610 for spinal cord.</p> <code>5700</code> <code>col</code> <code>str</code> <p>Name of the column containing the Z coordinate (medio-lateral) in microns. Default is \"Atlas_Z\".</p> <code>'Atlas_Z'</code> <code>atlas_type</code> <code>(brain, cord)</code> <p>Type of atlas used for registration. Required because the brain atlas is swapped between left and right while the spinal cord atlas is not. Default is \"brain\".</p> <code>\"brain\"</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The same DataFrame with a new \"hemisphere\" column</p> Source code in <code>cuisto/utils.py</code> <pre><code>def add_hemisphere(\n    df: pd.DataFrame,\n    hemisphere_names: dict,\n    midline: float = 5700,\n    col: str = \"Atlas_Z\",\n    atlas_type: str = \"brain\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add hemisphere (left/right) as a measurement for detections or annotations.\n\n    The hemisphere is read in the \"Classification\" column for annotations. The latter\n    needs to be in the form \"Right: Name\" or \"Left: Name\". For detections, the input\n    `col` of `df` is compared to `midline` to assess if the object belong to the left or\n    right hemispheres.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame with detections or annotations measurements.\n    hemisphere_names : dict\n        Map between \"Left\" and \"Right\" to something else.\n    midline : float\n        Used only for \"detections\" `df`. Corresponds to the brain midline in microns,\n        should be 5700 for CCFv3 and 1610 for spinal cord.\n    col : str, optional\n        Name of the column containing the Z coordinate (medio-lateral) in microns.\n        Default is \"Atlas_Z\".\n    atlas_type : {\"brain\", \"cord\"}, optional\n        Type of atlas used for registration. Required because the brain atlas is swapped\n        between left and right while the spinal cord atlas is not. Default is \"brain\".\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The same DataFrame with a new \"hemisphere\" column\n\n    \"\"\"\n    # check if there is something to do\n    if \"hemisphere\" in df.columns:\n        return df\n\n    # get kind of DataFrame\n    kind = get_df_kind(df)\n\n    if kind == \"detection\":\n        # use midline\n        if atlas_type == \"brain\":\n            # brain atlas : beyond midline, it's left\n            df.loc[df[col] &gt;= midline, \"hemisphere\"] = hemisphere_names[\"Left\"]\n            df.loc[df[col] &lt; midline, \"hemisphere\"] = hemisphere_names[\"Right\"]\n        elif atlas_type == \"cord\":\n            # cord atlas : below midline, it's left\n            df.loc[df[col] &lt;= midline, \"hemisphere\"] = hemisphere_names[\"Left\"]\n            df.loc[df[col] &gt; midline, \"hemisphere\"] = hemisphere_names[\"Right\"]\n\n    elif kind == \"annotation\":\n        # use Classification name -- this does not depend on atlas type\n        df[\"hemisphere\"] = [name.split(\":\")[0] for name in df[\"Classification\"]]\n        df[\"hemisphere\"] = df[\"hemisphere\"].map(hemisphere_names)\n\n    return df\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.ccf_to_stereo","title":"<code>ccf_to_stereo(x_ccf, y_ccf, z_ccf=0)</code>","text":"<p>Convert X, Y, Z coordinates in CCFv3 to stereotaxis coordinates (as in Paxinos-Franklin atlas).</p> <p>Coordinates are shifted, rotated and squeezed, see (1) for more info. Input must be in mm. <code>x_ccf</code> corresponds to the anterio-posterior (rostro-caudal) axis. <code>y_ccf</code> corresponds to the dorso-ventral axis. <code>z_ccf</code> corresponds to the medio-lateral axis (left-right) axis.</p> <p>Warning : it is a rough estimation.</p> <p>(1) https://community.brain-map.org/t/how-to-transform-ccf-x-y-z-coordinates-into-stereotactic-coordinates/1858</p> <p>Parameters:</p> Name Type Description Default <code>x_ccf</code> <code>floats or ndarray</code> <p>Coordinates in CCFv3 space in mm.</p> required <code>y_ccf</code> <code>floats or ndarray</code> <p>Coordinates in CCFv3 space in mm.</p> required <code>z_ccf</code> <code>float or ndarray</code> <p>Coordinate in CCFv3 space in mm. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ap, dv, ml : floats or np.ndarray</code> <p>Stereotaxic coordinates in mm.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def ccf_to_stereo(\n    x_ccf: float | np.ndarray, y_ccf: float | np.ndarray, z_ccf: float | np.ndarray = 0\n) -&gt; tuple:\n    \"\"\"\n    Convert X, Y, Z coordinates in CCFv3 to stereotaxis coordinates (as in\n    Paxinos-Franklin atlas).\n\n    Coordinates are shifted, rotated and squeezed, see (1) for more info. Input must be\n    in mm.\n    `x_ccf` corresponds to the anterio-posterior (rostro-caudal) axis.\n    `y_ccf` corresponds to the dorso-ventral axis.\n    `z_ccf` corresponds to the medio-lateral axis (left-right) axis.\n\n    Warning : it is a rough estimation.\n\n    (1) https://community.brain-map.org/t/how-to-transform-ccf-x-y-z-coordinates-into-stereotactic-coordinates/1858\n\n    Parameters\n    ----------\n    x_ccf, y_ccf : floats or np.ndarray\n        Coordinates in CCFv3 space in mm.\n    z_ccf : float or np.ndarray, optional\n        Coordinate in CCFv3 space in mm. Default is 0.\n\n    Returns\n    -------\n    ap, dv, ml : floats or np.ndarray\n        Stereotaxic coordinates in mm.\n\n    \"\"\"\n    # Center CCF on Bregma\n    xstereo = -(x_ccf - 5.40)  # anterio-posterior coordinate (rostro-caudal)\n    ystereo = y_ccf - 0.44  # dorso-ventral coordinate\n    ml = z_ccf - 5.70  # medio-lateral coordinate (left-right)\n\n    # Rotate CCF of 5\u00b0\n    angle = np.deg2rad(5)\n    ap = xstereo * np.cos(angle) - ystereo * np.sin(angle)\n    dv = xstereo * np.sin(angle) + ystereo * np.cos(angle)\n\n    # Squeeze the dorso-ventral axis by 94.34%\n    dv *= 0.9434\n\n    return ap, dv, ml\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.filter_df_classifications","title":"<code>filter_df_classifications(df, filter_list, mode='keep', col='Classification')</code>","text":"<p>Filter a DataFrame whether specified <code>col</code> column entries contain elements in <code>filter_list</code>. Case insensitive.</p> <p>If <code>mode</code> is \"keep\", keep entries only if their <code>col</code> in is in the list (default). If <code>mode</code> is \"remove\", remove entries if their <code>col</code> is in the list.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <code>filter_list</code> <code>list | tuple | str</code> <p>List of words that should be present to trigger the filter.</p> required <code>mode</code> <code>keep or remove</code> <p>Keep or remove entries from the list. Default is \"keep\".</p> <code>'keep'</code> <code>col</code> <code>str</code> <p>Key in <code>df</code>. Default is \"Classification\".</p> <code>'Classification'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered DataFrame.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def filter_df_classifications(\n    df: pd.DataFrame, filter_list: list | tuple | str, mode=\"keep\", col=\"Classification\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter a DataFrame whether specified `col` column entries contain elements in\n    `filter_list`. Case insensitive.\n\n    If `mode` is \"keep\", keep entries only if their `col` in is in the list (default).\n    If `mode` is \"remove\", remove entries if their `col` is in the list.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    filter_list : list | tuple | str\n        List of words that should be present to trigger the filter.\n    mode : \"keep\" or \"remove\", optional\n        Keep or remove entries from the list. Default is \"keep\".\n    col : str, optional\n        Key in `df`. Default is \"Classification\".\n\n    Returns\n    -------\n    pd.DataFrame\n        Filtered DataFrame.\n\n    \"\"\"\n    # check input\n    if isinstance(filter_list, str):\n        filter_list = [filter_list]  # make sure it is a list\n\n    if col not in df.columns:\n        # might be because of 'Classification' instead of 'classification'\n        col = col.capitalize()\n        if col not in df.columns:\n            raise KeyError(f\"{col} not in DataFrame.\")\n\n    pattern = \"|\".join(f\".*{s}.*\" for s in filter_list)\n\n    if mode == \"keep\":\n        df_return = df[df[col].str.contains(pattern, case=False, regex=True)]\n    elif mode == \"remove\":\n        df_return = df[~df[col].str.contains(pattern, case=False, regex=True)]\n\n    # check\n    if len(df_return) == 0:\n        raise ValueError(\n            (\n                f\"Filtering '{col}' with {filter_list} resulted in an\"\n                + \" empty DataFrame, check your config file.\"\n            )\n        )\n    return df_return\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.filter_df_regions","title":"<code>filter_df_regions(df, filter_list, mode='keep', col='Parent')</code>","text":"<p>Filters entries in <code>df</code> based on wether their <code>col</code> is in <code>filter_list</code> or not.</p> <p>If <code>mode</code> is \"keep\", keep entries only if their <code>col</code> in is in the list (default). If <code>mode</code> is \"remove\", remove entries if their <code>col</code> is in the list.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <code>filter_list</code> <code>list - like</code> <p>List of regions to keep or remove from the DataFrame.</p> required <code>mode</code> <code>keep or remove</code> <p>Keep or remove entries from the list. Default is \"keep\".</p> <code>'keep'</code> <code>col</code> <code>str</code> <p>Key in <code>df</code>. Default is \"Parent\".</p> <code>'Parent'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Filtered DataFrame.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def filter_df_regions(\n    df: pd.DataFrame, filter_list: list | tuple, mode=\"keep\", col=\"Parent\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters entries in `df` based on wether their `col` is in `filter_list` or not.\n\n    If `mode` is \"keep\", keep entries only if their `col` in is in the list (default).\n    If `mode` is \"remove\", remove entries if their `col` is in the list.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n    filter_list : list-like\n        List of regions to keep or remove from the DataFrame.\n    mode : \"keep\" or \"remove\", optional\n        Keep or remove entries from the list. Default is \"keep\".\n    col : str, optional\n        Key in `df`. Default is \"Parent\".\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        Filtered DataFrame.\n\n    \"\"\"\n\n    if mode == \"keep\":\n        return df[df[col].isin(filter_list)]\n    if mode == \"remove\":\n        return df[~df[col].isin(filter_list)]\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.get_blacklist","title":"<code>get_blacklist(file, atlas)</code>","text":"<p>Build a list of regions to exclude from file.</p> <p>File must be a TOML with [WITH_CHILDS] and [EXACT] sections.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Full path the atlas_blacklist.toml file.</p> required <code>atlas</code> <code>BrainGlobeAtlas</code> <p>Atlas to extract regions from.</p> required <p>Returns:</p> Name Type Description <code>black_list</code> <code>list</code> <p>Full list of acronyms to discard.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def get_blacklist(file: str, atlas: BrainGlobeAtlas) -&gt; list:\n    \"\"\"\n    Build a list of regions to exclude from file.\n\n    File must be a TOML with [WITH_CHILDS] and [EXACT] sections.\n\n    Parameters\n    ----------\n    file : str\n        Full path the atlas_blacklist.toml file.\n    atlas : BrainGlobeAtlas\n        Atlas to extract regions from.\n\n    Returns\n    -------\n    black_list : list\n        Full list of acronyms to discard.\n\n    \"\"\"\n    with open(file, \"rb\") as fid:\n        content = tomllib.load(fid)\n\n    blacklist = []  # init. the list\n\n    # add regions and their descendants\n    for region in content[\"WITH_CHILDS\"][\"members\"]:\n        blacklist.extend(\n            [\n                atlas.structures[id][\"acronym\"]\n                for id in atlas.structures.tree.expand_tree(\n                    atlas.structures[region][\"id\"]\n                )\n            ]\n        )\n\n    # add regions specified exactly (no descendants)\n    blacklist.extend(content[\"EXACT\"][\"members\"])\n\n    return blacklist\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.get_data_coverage","title":"<code>get_data_coverage(df, col='Atlas_AP', by='animal')</code>","text":"<p>Get min and max in <code>col</code> for each <code>by</code>.</p> <p>Used to get data coverage for each animal to plot in distributions.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>description</p> required <code>col</code> <code>str</code> <p>Key in <code>df</code>, default is \"Atlas_X\".</p> <code>'Atlas_AP'</code> <code>by</code> <code>str</code> <p>Key in <code>df</code> , default is \"animal\".</p> <code>'animal'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>min and max of <code>col</code> for each <code>by</code>, named \"X_min\", and \"X_max\".</p> Source code in <code>cuisto/utils.py</code> <pre><code>def get_data_coverage(df: pd.DataFrame, col=\"Atlas_AP\", by=\"animal\") -&gt; pd.DataFrame:\n    \"\"\"\n    Get min and max in `col` for each `by`.\n\n    Used to get data coverage for each animal to plot in distributions.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        _description_\n    col : str, optional\n        Key in `df`, default is \"Atlas_X\".\n    by : str, optional\n        Key in `df` , default is \"animal\".\n\n    Returns\n    -------\n    pd.DataFrame\n        min and max of `col` for each `by`, named \"X_min\", and \"X_max\".\n\n    \"\"\"\n    df_group = df.groupby([by])\n    return pd.DataFrame(\n        [\n            df_group[col].min(),\n            df_group[col].max(),\n        ],\n        index=[\"X_min\", \"X_max\"],\n    )\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.get_df_kind","title":"<code>get_df_kind(df)</code>","text":"<p>Get DataFrame kind, eg. Annotations or Detections.</p> <p>It is based on reading the Object Type of the first entry, so the DataFrame must have only one kind of object.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <p>Returns:</p> Name Type Description <code>kind</code> <code>str</code> <p>\"detection\" or \"annotation\".</p> Source code in <code>cuisto/utils.py</code> <pre><code>def get_df_kind(df: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Get DataFrame kind, eg. Annotations or Detections.\n\n    It is based on reading the Object Type of the first entry, so the DataFrame must\n    have only one kind of object.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n\n    Returns\n    -------\n    kind : str\n        \"detection\" or \"annotation\".\n\n    \"\"\"\n    return df[\"Object type\"].iloc[0].lower()\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.get_injection_site","title":"<code>get_injection_site(animal, info_file, channel, stereo=False)</code>","text":"<p>Get the injection site coordinates associated with animal.</p> <p>Parameters:</p> Name Type Description Default <code>animal</code> <code>str</code> <p>Animal ID.</p> required <code>info_file</code> <code>str</code> <p>Path to TOML info file.</p> required <code>channel</code> <code>str</code> <p>Channel ID as in the TOML file.</p> required <code>stereo</code> <code>bool</code> <p>Wether to convert coordinates in stereotaxis coordinates. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>x, y, z : floats</code> <p>Injection site coordinates.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def get_injection_site(\n    animal: str, info_file: str, channel: str, stereo: bool = False\n) -&gt; tuple:\n    \"\"\"\n    Get the injection site coordinates associated with animal.\n\n    Parameters\n    ----------\n    animal : str\n        Animal ID.\n    info_file : str\n        Path to TOML info file.\n    channel : str\n        Channel ID as in the TOML file.\n    stereo : bool, optional\n        Wether to convert coordinates in stereotaxis coordinates. Default is False.\n\n    Returns\n    -------\n    x, y, z : floats\n        Injection site coordinates.\n\n    \"\"\"\n    with open(info_file, \"rb\") as fid:\n        info = tomllib.load(fid)\n\n    if channel in info[animal]:\n        x, y, z = info[animal][channel][\"injection_site\"]\n        if stereo:\n            x, y, z = ccf_to_stereo(x, y, z)\n    else:\n        x, y, z = None, None, None\n\n    return x, y, z\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.get_leaves_list","title":"<code>get_leaves_list(atlas)</code>","text":"<p>Get the list of leaf brain regions.</p> <p>Leaf brain regions are defined as regions without childs, eg. regions that are at the bottom of the hiearchy.</p> <p>Parameters:</p> Name Type Description Default <code>atlas</code> <code>BrainGlobeAtlas</code> <p>Atlas to extract regions from.</p> required <p>Returns:</p> Name Type Description <code>leaves_list</code> <code>list</code> <p>Acronyms of leaf brain regions.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def get_leaves_list(atlas: BrainGlobeAtlas) -&gt; list:\n    \"\"\"\n    Get the list of leaf brain regions.\n\n    Leaf brain regions are defined as regions without childs, eg. regions that are at\n    the bottom of the hiearchy.\n\n    Parameters\n    ----------\n    atlas : BrainGlobeAtlas\n        Atlas to extract regions from.\n\n    Returns\n    -------\n    leaves_list : list\n        Acronyms of leaf brain regions.\n\n    \"\"\"\n    leaves_list = []\n    for region in atlas.structures_list:\n        if atlas.structures.tree[region[\"id\"]].is_leaf():\n            leaves_list.append(region[\"acronym\"])\n\n    return leaves_list\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.get_mapping_fusion","title":"<code>get_mapping_fusion(fusion_file)</code>","text":"<p>Get mapping dictionnary between input brain regions and new regions defined in <code>atlas_fusion.toml</code> file.</p> <p>The returned dictionnary can be used in DataFrame.replace().</p> <p>Parameters:</p> Name Type Description Default <code>fusion_file</code> <code>str</code> <p>Path to the TOML file with the merging rules.</p> required <p>Returns:</p> Name Type Description <code>m</code> <code>dict</code> <p>Mapping as {old: new}.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def get_mapping_fusion(fusion_file: str) -&gt; dict:\n    \"\"\"\n    Get mapping dictionnary between input brain regions and new regions defined in\n    `atlas_fusion.toml` file.\n\n    The returned dictionnary can be used in DataFrame.replace().\n\n    Parameters\n    ----------\n    fusion_file : str\n        Path to the TOML file with the merging rules.\n\n    Returns\n    -------\n    m : dict\n        Mapping as {old: new}.\n\n    \"\"\"\n    with open(fusion_file, \"rb\") as fid:\n        df = pd.DataFrame.from_dict(tomllib.load(fid), orient=\"index\").set_index(\n            \"acronym\"\n        )\n\n    return (\n        df.drop(columns=\"name\")[\"members\"]\n        .explode()\n        .reset_index()\n        .set_index(\"members\")\n        .to_dict()[\"acronym\"]\n    )\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.get_starter_cells","title":"<code>get_starter_cells(animal, channel, info_file)</code>","text":"<p>Get the number of starter cells associated with animal.</p> <p>Parameters:</p> Name Type Description Default <code>animal</code> <code>str</code> <p>Animal ID.</p> required <code>channel</code> <code>str</code> <p>Channel ID.</p> required <code>info_file</code> <code>str</code> <p>Path to TOML info file.</p> required <p>Returns:</p> Name Type Description <code>n_starters</code> <code>int</code> <p>Number of starter cells.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def get_starter_cells(animal: str, channel: str, info_file: str) -&gt; int:\n    \"\"\"\n    Get the number of starter cells associated with animal.\n\n    Parameters\n    ----------\n    animal : str\n        Animal ID.\n    channel : str\n        Channel ID.\n    info_file : str\n        Path to TOML info file.\n\n    Returns\n    -------\n    n_starters : int\n        Number of starter cells.\n\n    \"\"\"\n    with open(info_file, \"rb\") as fid:\n        info = tomllib.load(fid)\n\n    return info[animal][channel][\"starter_cells\"]\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.merge_regions","title":"<code>merge_regions(df, col, fusion_file)</code>","text":"<p>Merge brain regions following rules in the <code>fusion_file.toml</code> file.</p> <p>Apply this merging on <code>col</code> of the input DataFrame. <code>col</code> whose value is found in the <code>members</code> sections in the file will be changed to the new acronym.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <code>col</code> <code>str</code> <p>Column of <code>df</code> on which to apply the mapping.</p> required <code>fusion_file</code> <code>str</code> <p>Path to the toml file with the merging rules.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Same DataFrame with regions renamed.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def merge_regions(df: pd.DataFrame, col: str, fusion_file: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Merge brain regions following rules in the `fusion_file.toml` file.\n\n    Apply this merging on `col` of the input DataFrame. `col` whose value is found in\n    the `members` sections in the file will be changed to the new acronym.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n    col : str\n        Column of `df` on which to apply the mapping.\n    fusion_file : str\n        Path to the toml file with the merging rules.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        Same DataFrame with regions renamed.\n\n    \"\"\"\n    df[col] = df[col].replace(get_mapping_fusion(fusion_file))\n\n    return df\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.renormalize_per_key","title":"<code>renormalize_per_key(df, by, on)</code>","text":"<p>Renormalize <code>on</code> column by its sum for each <code>by</code>.</p> <p>Use case : relative density is computed for both hemispheres, so if one wants to plot only one hemisphere, the sum of the bars corresponding to one channel (<code>by</code>) should be 1. So :</p> <p>df = df[df[\"hemisphere\"] == \"Ipsi.\"] df = renormalize_per_key(df, \"channel\", \"relative density\") Then, the sum of \"relative density\" for each \"channel\" equals 1.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> required <code>by</code> <code>str</code> <p>Key in <code>df</code>. <code>df</code> is normalized for each <code>by</code>.</p> required <code>on</code> <code>str</code> <p>Key in <code>df</code>. Measurement to be normalized.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Same DataFrame with normalized <code>on</code> column.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def renormalize_per_key(df: pd.DataFrame, by: str, on: str):\n    \"\"\"\n    Renormalize `on` column by its sum for each `by`.\n\n    Use case : relative density is computed for both hemispheres, so if one wants to\n    plot only one hemisphere, the sum of the bars corresponding to one channel (`by`)\n    should be 1. So :\n    &gt;&gt;&gt; df = df[df[\"hemisphere\"] == \"Ipsi.\"]\n    &gt;&gt;&gt; df = renormalize_per_key(df, \"channel\", \"relative density\")\n    Then, the sum of \"relative density\" for each \"channel\" equals 1.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    by : str\n        Key in `df`. `df` is normalized for each `by`.\n    on : str\n        Key in `df`. Measurement to be normalized.\n\n    Returns\n    -------\n    df : pd.DataFrame\n        Same DataFrame with normalized `on` column.\n\n    \"\"\"\n    norm = df.groupby(by)[on].sum()\n    bys = df[by].unique()\n    for key in bys:\n        df.loc[df[by] == key, on] = df.loc[df[by] == key, on].divide(norm[key])\n\n    return df\n</code></pre>"},{"location":"api-utils.html#cuisto.utils.select_hemisphere_channel","title":"<code>select_hemisphere_channel(df, hue, hue_filter, hue_mirror)</code>","text":"<p>Select relevant data given hue and filters.</p> <p>Returns the DataFrame with only things to be used.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to filter.</p> required <code>hue</code> <code>(hemisphere, channel)</code> <p>hue that will be used in seaborn plots.</p> <code>\"hemisphere\"</code> <code>hue_filter</code> <code>str</code> <p>Selected data.</p> required <code>hue_mirror</code> <code>bool</code> <p>Instead of keeping only hue_filter values, they will be plotted in mirror.</p> required <p>Returns:</p> Name Type Description <code>dfplt</code> <code>DataFrame</code> <p>DataFrame to be used in plots.</p> Source code in <code>cuisto/utils.py</code> <pre><code>def select_hemisphere_channel(\n    df: pd.DataFrame, hue: str, hue_filter: str, hue_mirror: bool\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Select relevant data given hue and filters.\n\n    Returns the DataFrame with only things to be used.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame to filter.\n    hue : {\"hemisphere\", \"channel\"}\n        hue that will be used in seaborn plots.\n    hue_filter : str\n        Selected data.\n    hue_mirror : bool\n        Instead of keeping only hue_filter values, they will be plotted in mirror.\n\n    Returns\n    -------\n    dfplt : pd.DataFrame\n        DataFrame to be used in plots.\n\n    \"\"\"\n    dfplt = df.copy()\n\n    if hue == \"hemisphere\":\n        # hue_filter is used to select channels\n        # keep only left and right hemispheres, not \"both\"\n        dfplt = dfplt[dfplt[\"hemisphere\"] != \"both\"]\n        if hue_filter == \"all\":\n            hue_filter = dfplt[\"channel\"].unique()\n        elif not isinstance(hue_filter, (list, tuple)):\n            # it is allowed to select several channels so handle lists\n            hue_filter = [hue_filter]\n        dfplt = dfplt[dfplt[\"channel\"].isin(hue_filter)]\n    elif hue == \"channel\":\n        # hue_filter is used to select hemispheres\n        # it can only be left, right, both or empty\n        if hue_filter == \"both\":\n            # handle if it's a coordinates DataFrame which doesn't have \"both\"\n            if \"both\" not in dfplt[\"hemisphere\"].unique():\n                # keep both hemispheres, don't do anything\n                pass\n            else:\n                if hue_mirror:\n                    # we need to keep both hemispheres to plot them in mirror\n                    dfplt = dfplt[dfplt[\"hemisphere\"] != \"both\"]\n                else:\n                    # we keep the metrics computed in both hemispheres\n                    dfplt = dfplt[dfplt[\"hemisphere\"] == \"both\"]\n        else:\n            # hue_filter should correspond to an hemisphere name\n            dfplt = dfplt[dfplt[\"hemisphere\"] == hue_filter]\n    else:\n        # not handled. Just return the DataFrame without filtering, maybe it'll make\n        # sense.\n        warnings.warn(f\"{hue} should be 'channel' or 'hemisphere'.\")\n\n    # check result\n    if len(dfplt) == 0:\n        warnings.warn(\n            f\"hue={hue} and hue_filter={hue_filter} resulted in an empty subset.\"\n        )\n\n    return dfplt\n</code></pre>"},{"location":"guide-create-pyramids.html","title":"Create pyramidal OME-TIFF","text":"<p>This page will guide you to use the <code>pyramid-creator</code> package, in the event the CZI file does not work directly in QuPath. The script will generate pyramids from OME-TIFF files exported from ZEN.</p> <p>Tip</p> <p><code>pyramid-creator</code> can also pyramidalize images using Python only with the <code>--no-use-qupath</code> option.</p> <p>This Python script uses QuPath under the hood, via a companion script called <code>createPyramids.groovy</code>. It will find the OME-TIFF files and make QuPath run the groovy script on it, in console mode (without graphical user interface).</p> <p>This script is standalone, eg. it does not rely on the <code>cuisto</code> package. But installing the later makes sure all dependencies are installed (namely <code>typer</code> and <code>tqdm</code> with the QuPath backend and quite a few more for the Python backend).</p> <p><code>pyramid-creator</code> moved to a standalone package that you can find here with installation and usage instructions.</p>"},{"location":"guide-create-pyramids.html#installation","title":"Installation","text":"<p>You will find instructions on the dedicated project page over at Github.</p> <p>For reference :</p> <p>You will need <code>conda</code>, follow those instructions to install it.</p> <p>Then, create a virtual environment if you didn't already (<code>pyramid-creator</code> can be installed in the environment for <code>cuisto</code>) and install the <code>pyramid-creator</code> package. <pre><code>conda create -c conda-forge -n cuisto-env python=3.12  # not required if you already create an environment\nconda activate cuisto-env\npip install pyramid-creator\n</code></pre> To use the Python backend (with <code>tifffile</code>), replace the last line with : <pre><code>pip install pyramid-creator[python-backend]\n</code></pre> To use the QuPath backend, a working QuPath installation is required, and the <code>pyramid-creator</code> command needs to be aware of its location.</p> <p>To do so, first, install QuPath. By default, it will install in <code>~\\AppData\\QuPath-0.X.Y</code>. In any case, note down the installation location.</p> <p>Then, you have several options : - Create a file in your user directory called \"QUPATH_PATH\" (without extension), containing the full path to the QuPath console executable. In my case, it reads : <code>C:\\Users\\glegoc\\AppData\\Local\\QuPath-0.5.1\\QuPath-0.5.1 (console).exe</code>. Then, the <code>pyramid-creator</code> script will read this file to find the QuPath executable. - Specify the QuPath path as an option when calling the command line interface (see the Usage section) : <pre><code>pyramid-creator /path/to/your/images --qupath-path \"C:\\Users\\glegoc\\AppData\\Local\\QuPath-0.5.1\\QuPath-0.5.1 (console).exe\"\n</code></pre> - Specify the QuPath path as an option when using the package in a Python script (see the Usage section) : <pre><code>from pyramid_creator import pyramidalize_directory\npyramidalize_directory(\"/path/to/your/images/\", qupath_path=\"C:\\Users\\glegoc\\AppData\\Local\\QuPath-0.5.1\\QuPath-0.5.1 (console).exe\")\n</code></pre> - If you're using Windows, using QuPath v0.6.0, v0.5.1 or v0.5.0 and chose the default installation location, <code>pyramid-creator</code> should find it automatically and write it down in the \"QUPATH_PATH\" file by itself.</p>"},{"location":"guide-create-pyramids.html#export-czi-to-ome-tiff","title":"Export CZI to OME-TIFF","text":"<p>OME-TIFF is a specification of the TIFF image format. It specifies how the metadata should be written to the file to be interoperable between softwares. ZEN can export to OME-TIFF so you don't need to pay attention to metadata. Therefore, you won't need to specify pixel size and channels names and colors as it will be read directly from the OME-TIFF files.</p> <ol> <li>Open your CZI file in ZEN.</li> <li>Open the \"Processing tab\" on the left panel.</li> <li>Under method, choose Export/Import &gt; OME TIFF-Export.</li> <li>In Parameters, make sure to tick the \"Show all\" tiny box on the right.</li> <li>The following parameters should be used (checked), the other should be unchecked :<ul> <li>Use Tiles</li> <li>Original data  \"Convert to 8 Bit\" should be UNCHECKED </li> <li>OME-XML Scheme : 2016-06</li> <li>Use full set of dimensions (unless you want to select slices and/or channels)</li> </ul> </li> <li>In Input, choose your file</li> <li>Go back to Parameters to choose the output directory and file prefix. \"_s1\", \"_s2\"... will be appended to the prefix.</li> <li>Back on the top, click the \"Apply\" button.</li> </ol> <p>The OME-TIFF files should be ready to be pyramidalized with the <code>create_pyramids.py</code> script.</p>"},{"location":"guide-create-pyramids.html#usage","title":"Usage","text":"<p>See the instructions on the dedicated project page over at Github.</p>"},{"location":"guide-install-abba.html","title":"Install ABBA","text":"<p>You can head to the ABBA documentation for installation instructions. You'll see that a Windows installer is available. While it might be working great, I prefer to do it manually step-by-step to make sure everything is going well.</p> <p>You will find below installation instructions for the regular ABBA Fiji plugin, which proposes only the mouse and rat brain atlases. To be able to use the Brainglobe atlases, you will need the Python version. The two can be installed alongside each other.</p>"},{"location":"guide-install-abba.html#abba-fiji","title":"ABBA Fiji","text":""},{"location":"guide-install-abba.html#install-fiji","title":"Install Fiji","text":"<p>Install the \"batteries-included\" distribution of ImageJ, Fiji, from the official website.  </p> <p>Warning</p> <p>Extract Fiji somewhere you have write access, otherwise Fiji will not be able to download and install plugins. In other words, put the folder in your User directory and not in C:\\, C:\\Program Files and the like.</p> <ol> <li>Download the zip archive and extract it somewhere relevant.</li> <li>Launch ImageJ.exe.</li> </ol>"},{"location":"guide-install-abba.html#install-the-abba-plugin","title":"Install the ABBA plugin","text":"<p>We need to add the PTBIOP update site, managed by the bio-imaging and optics facility at EPFL, that contains the ABBA plugin.</p> <ol> <li>In Fiji, head to <code>Help &gt; Update</code>...  </li> <li>In the ImageJ updater window, click on <code>Manage Update Sites</code>. Look up <code>PTBIOP</code>, and click on the check box. <code>Apply and Close</code>, and <code>Apply Changes</code>. This will download and install the required plugins. Restart ImageJ as suggested.  </li> <li>In Fiji, head to <code>Plugins &gt; BIOP &gt; Atlas &gt; ABBA - ABBA start</code>, or simply type <code>abba start</code> in the search box. Choose the \"Adult Mouse Brain - Allen Brain Atlas V3p1\". It will download this atlas and might take a while, depending on your Internet connection.</li> </ol>"},{"location":"guide-install-abba.html#install-the-automatic-registration-tools","title":"Install the automatic registration tools","text":"<p>ABBA can leverage the elastix toolbox for automatic 2D in-plane registration. </p> <ol> <li>You need to download it here, which will redirect you to the Github releases page (5.2.0 should work).  </li> <li>Download the zip archive and extract it somewhere relevant.  </li> <li>In Fiji, in the search box, type \"set and check\" and launch the \"Set and Check Wrappers\" command. Set the paths to \"elastix.exe\" and \"transformix.exe\" you just downloaded.</li> </ol> <p>ABBA should be installed and functional ! You can check the official documentation for usage instructions and some tips here.</p>"},{"location":"guide-install-abba.html#abba-python","title":"ABBA Python","text":"<p>Brainglobe is an initiative aiming at providing interoperable, model-agnostic Python-based tools for neuroanatomy. They package various published volumetric anatomical atlases of different species (check the list), including the Allen Mouse brain atlas (CCFv3, ref.) and a 3D version of the Allen mouse spinal cord atlas (ref).</p> <p>To be able to leverage those atlases, we need to make ImageJ and Python be able to talk to each other. This is the purpose of abba_python, that will install ImageJ and its ABBA plugins inside a python environment, with bindings between the two worlds.</p>"},{"location":"guide-install-abba.html#install-conda","title":"Install <code>conda</code>","text":"<p>If not done already, follow those instructions to install <code>conda</code>.</p>"},{"location":"guide-install-abba.html#install-abba_python-in-a-virtual-environment","title":"Install abba_python in a virtual environment","text":"<ol> <li>Open a terminal (PowerShell).</li> <li>Create a virtual environment with Python 3.10, OpenJDK and PyImageJ : <pre><code>conda create -c conda-forge -n abba_python python=3.10 openjdk=11 maven pyimagej notebook\n</code></pre></li> <li>Install the latest functional version of abba_python with pip : <pre><code>pip install abba-python==0.9.6.dev0\n</code></pre></li> <li>Restart the terminal and activate the new environment : <pre><code>conda activate abba_python\n</code></pre></li> <li>Download the Brainglobe atlas you want (eg. Allen mouse spinal cord) : <pre><code>brainglobe install -a allen_cord_20um\n</code></pre></li> <li>Launch an interactive Python shell : <pre><code>ipython\n</code></pre> You should see the IPython prompt, that looks like this : <pre><code>In [1]:\n</code></pre></li> <li>Import abba_python and launch ImageJ from Python : <pre><code>from abba_python import abba\nabba.start_imagej()\n</code></pre> The first launch needs to initialize ImageJ and install all required plugins, which takes a while (&gt;5min).</li> <li>Use ABBA as the regular Fiji version ! The main difference is that the dropdown menu to select which atlas to use is populated with the Brainglobe atlases.</li> </ol> <p>Tip</p> <p>Afterwards, to launch ImageJ from Python and do some registration work, you just need to launch a terminal (PowerShell), and do steps 4., 6., and 7.</p>"},{"location":"guide-install-abba.html#install-the-automatic-registration-tools_1","title":"Install the automatic registration tools","text":"<p>You can follow the same instructions as the regular Fiji version. You can do it from either the \"normal\" Fiji or the ImageJ instance launched from Python, they share the same configuration files. Therefore, if you already did it in regular Fiji, elastix should already be set up and ready to use in ImageJ from Python.</p>"},{"location":"guide-install-abba.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide-install-abba.html#java_home-errors","title":"JAVA_HOME errors","text":"<p>Unfortunately on some computers, Python does not find the Java virtual machine even though it should have been installed when installing OpenJDK with conda. This will result in an error mentionning \"java.dll\" and suggesting to check the <code>JAVA_HOME</code> environment variable.</p> <p>The only fix I could find is to install Java system-wide. You can grab a (free) installer on Adoptium, choosing JRE 17.X for your platform. During the installation :</p> <ul> <li>choose to install \"just for you\",</li> <li>enable \"Modify PATH variable\" as well as \"Set or override JAVA_HOME\" variable.</li> </ul> <p>Restart the terminal and try again. Now, ImageJ should use the system-wide Java and it should work.</p>"},{"location":"guide-install-abba.html#abba-qupath-extension","title":"ABBA QuPath extension","text":"<p>To import registered regions in your QuPath project and be able to convert objects' coordinates in atlas space, the ABBA QuPath extension is required.</p> <ol> <li>In QuPath, head to <code>Edit &gt; Preferences</code>. In the <code>Extension</code> tab, set your <code>QuPath user directory</code> to a local directory (usually <code>C:\\Users\\USERNAME\\QuPath\\v0.X.Y</code>).</li> <li>Create a folder named <code>extensions</code> in your QuPath user directory.</li> <li>Download the latest ABBA extension for QuPath from GitHub (choose the file <code>qupath-extension-abba-x.y.z.zip</code>).</li> <li>Uncompress the archive and copy all .jar files into the <code>extensions</code> folder in your QuPath user directory.</li> <li>Restart QuPath. Now, in <code>Extensions</code>, you should have an <code>ABBA</code> entry.</li> </ol>"},{"location":"guide-pipeline.html","title":"Pipeline","text":"<p>While you can use QuPath and <code>cuisto</code> functionalities as you see fit, there exists a pipeline version of those. It requires a specific structure to store files (so that the different scripts know where to look for data). It also requires that you have detections stored as geojson files, which can be achieved using a pixel classifier and further segmentation (see here) for example.</p>"},{"location":"guide-pipeline.html#purpose","title":"Purpose","text":"<p>This is especially useful to perform quantification for several animals at once, where you'll only need to specify the root directory and the animals identifiers that should be pooled together, instead of having to manually specify each detections and annotations files.</p> <p>Three main scripts and function are used within the pipeline :</p> <ul> <li><code>exportPixelClassifierProbabilities.groovy</code> to create prediction maps of objects of interest</li> <li><code>segment_image.py</code> to segment those maps and create geojson files to be imported back to QuPath as detections</li> <li><code>pipelineImportExport.groovy</code> to :<ul> <li>clear all objects</li> <li>import ABBA regions</li> <li>mirror regions names</li> <li>import geojson detections (from <code>$folderPrefix$segmentation/$segTag$/geojson</code>)</li> <li>add measurements to detections</li> <li>add atlas coordinates to detections</li> <li>add hemisphere to detections' parents</li> <li>add regions measurements <ul> <li>count for punctal objects</li> <li>cumulated length for lines objects</li> </ul> </li> <li>export detections measurements<ul> <li>as CSV for punctual objects</li> <li>as JSON for lines</li> </ul> </li> <li>export annotations as CSV</li> </ul> </li> </ul>"},{"location":"guide-pipeline.html#directory-structure","title":"Directory structure","text":"<p>Following a specific directory structure ensures subsequent scripts and functions can find required files. The good news is that this structure will mostly be created automatically using the segmentation scripts (from QuPath and Python), as long as you stay consistent filling the parameters of each script. The structure expected by the groovy all-in-one script and <code>cuisto</code> batch-process function is the following :</p> <pre><code>some_directory/\n    \u251c\u2500\u2500AnimalID0/  \n    \u2502   \u251c\u2500\u2500 animalid0_qupath/\n    \u2502   \u2514\u2500\u2500 animalid0_segmentation/  \n    \u2502       \u2514\u2500\u2500 segtag/  \n    \u2502           \u251c\u2500\u2500 annotations/  \n    \u2502           \u251c\u2500\u2500 detections/  \n    \u2502           \u251c\u2500\u2500 geojson/  \n    \u2502           \u2514\u2500\u2500 probabilities/  \n    \u251c\u2500\u2500AnimalID1/  \n    \u2502   \u251c\u2500\u2500 animalid1_qupath/\n    \u2502   \u2514\u2500\u2500 animalid1_segmentation/  \n    \u2502       \u2514\u2500\u2500 segtag/  \n    \u2502           \u251c\u2500\u2500 annotations/  \n    \u2502           \u251c\u2500\u2500 detections/  \n    \u2502           \u251c\u2500\u2500 geojson/  \n    \u2502           \u2514\u2500\u2500 probabilities/  \n</code></pre> <p>Info</p> <p>Except the root directory and the QuPath project, the rest is automatically created based on the parameters provided in the different scripts. Here's the description of the structure and the requirements :</p> <ul> <li><code>animalid0</code> should be a convenient animal identifier.</li> <li>The hierarchy must be followed.</li> <li>The experiment root directory, <code>AnimalID0</code>, can be anything but should correspond to one and only one animal.</li> <li>Subsequent <code>animalid0</code> should be lower case.</li> <li><code>animalid0_qupath</code> can be named as you wish in practice, but should be the QuPath project.</li> <li><code>animalid0_segmentation</code> should be called exactly like this -- replacing <code>animalid0</code> with the actual animal ID. It will be created automatically with the <code>exportPixelClassifierProbabilities.groovy</code> script.</li> <li><code>segtag</code> corresponds to the type of segmentation (cells, fibers...). It is specified in the <code>exportPixelClassifierProbabilities</code> script. It could be anything, but to recognize if the objects are polygons (and should be counted per regions) or polylines (and the cumulated length should be measured), there are some hardcoded keywords in the <code>segment_images.py</code> and <code>pipelineImportExport.groovy</code> scripts :<ul> <li>Cells-like when you need measurements related to its shape (area, circularity...) : <code>cells</code>, <code>cell</code>, <code>polygons</code>, <code>polygon</code></li> <li>Cells-like when you consider them as punctual : <code>synapto</code>, <code>synaptophysin</code>, <code>syngfp</code>, <code>boutons</code>, <code>points</code></li> <li>Fibers-like (polylines) : <code>fibers</code>, <code>fiber</code>, <code>axons</code>, <code>axon</code></li> </ul> </li> <li><code>annotations</code> contains the atlas regions measurements as TSV files.</li> <li><code>detections</code> contains the objects atlas coordinates and measurements as CSV files (for punctal objects) or JSON (for polylines objects).</li> <li><code>geojson</code> contains objects stored as geojson files. They could be generated with the pixel classifier prediction map segmentation.</li> <li><code>probabilities</code> contains the prediction maps to be segmented by the <code>segment_images.py</code> script.</li> </ul> <p>Tip</p> <p>You can see an example minimal directory structure with only annotations stored in <code>resources/multi</code>.</p>"},{"location":"guide-pipeline.html#usage","title":"Usage","text":"<p>Tip</p> <p>Remember that this is merely an example pipeline, you can shortcut it at any points, as long as you end up with TSV files following the requirements for <code>cuisto</code>.</p> <ol> <li>Create a QuPath project.</li> <li>Register your images on an atlas with ABBA and export the registration back to QuPath.</li> <li>Use a pixel classifier and export the prediction maps with the <code>exportPixelClassifierProbabilities.groovy</code> script. You need to get a pixel classifier or create one.</li> <li>Segment those maps with the <code>segment_images.py</code> script to generate the geojson files containing the objects of interest.</li> <li>Run the <code>pipelineImportExport.groovy</code> script on your QuPath project.</li> <li>Set up your configuration files.</li> <li>Then, analysing your data with any number of animals should be as easy as executing those lines in Python (either from IPython directly or in a script to easily run it later) :</li> </ol> <pre><code>import cuisto\n\n# Parameters\nwdir = \"/path/to/some_directory\"\nanimals = [\"AnimalID0\", \"AnimalID1\"]\nconfig_file = \"/path/to/your/config.toml\"\noutput_format = \"h5\"  # to save the quantification values as hdf5 file\n\n# Processing\ncfg = cuisto.Config(config_file)\ndf_regions, dfs_distributions, df_coordinates = cuisto.process.process_animals(\n    wdir, animals, cfg, out_fmt=output_format\n)\n\n# Display\ncuisto.display.plot_regions(df_regions, cfg)\ncuisto.display.plot_1D_distributions(dfs_distributions, cfg, df_coordinates=df_coordinates)\ncuisto.display.plot_2D_distributions(df_coordinates, cfg)\n</code></pre> <p>Tip</p> <p>You can see a live example in this demo notebook.</p>"},{"location":"guide-prepare-qupath.html","title":"Prepare QuPath data","text":"<p><code>cuisto</code> uses some QuPath classifications concepts, make sure to be familiar with them with the official documentation. Notably, we use the concept of primary classification and derived classification : an object classfied as <code>First: second</code> is of classification <code>First</code> and of derived classification <code>second</code>.</p>"},{"location":"guide-prepare-qupath.html#qupath-requirements","title":"QuPath requirements","text":"<p><code>cuisto</code> assumes a specific way of storing regions and objects information in the TSV files exported from QuPath. Note that only one primary classification is supported, but you can have any number of derived classifications.</p>"},{"location":"guide-prepare-qupath.html#detections","title":"Detections","text":"<p>Detections are the objects of interest. Their information must respect the following :</p> <ul> <li>Atlas coordinates should be in millimetres (mm) and stored as <code>Atlas_X</code>, <code>Atlas_Y</code>, <code>Atlas_Z</code>. They correspond, respectively, to the anterio-posterior (rostro-caudal) axis, the inferio-superior (dorso-ventral) axis and the left-right (medio-lateral) axis.</li> <li>They must have a derived classification, in the form <code>Primary: second</code>. Primary would be an object type (cells, fibers, ...), the second one would be a biological marker or a detection channel (fluorescence channel name), for instance : <code>Cells: some marker</code>, or <code>Fibers: EGFP</code>.</li> <li>The classification must match exactly the corresponding measurement in the annotations (see below).</li> </ul>"},{"location":"guide-prepare-qupath.html#annotations","title":"Annotations","text":"<p>Annotations correspond to the atlas regions. Their information must respect the following :</p> <ul> <li>They should be imported with the ABBA extension as acronyms and splitting left/right. Therefore, the annotation name should be the region acronym and its classification should be formatted as <code>Hemisphere: acronym</code> (for ex. <code>Left: PAG</code>).</li> <li>Measurements names should be formatted as : <code>Primary classification: derived classification measurement name</code>. For instance :  <ul> <li>if one has cells with some marker and count them in each atlas regions, the measurement name would be : <code>Cells: some marker Count</code>.</li> <li>if one segments fibers revealed in the EGFP channel and measures the cumulated length in \u00b5m in each atlas regions, the measurement name would be : <code>Fibers: EGFP Length \u00b5m</code>.</li> </ul> </li> <li>Any number of markers or channels are supported.</li> </ul>"},{"location":"guide-prepare-qupath.html#measurements","title":"Measurements","text":""},{"location":"guide-prepare-qupath.html#metrics-supported-by-cuisto","title":"Metrics supported by <code>cuisto</code>","text":"<p>While you're free to add any measurements as long as they follow the requirements, keep in mind that for atlas regions quantification, <code>cuisto</code> will only compute, pool and average the following metrics :</p> <ul> <li>the base measurement itself<ul> <li>if \"\u00b5m\" is contained in the measurement name, it will also be converted to mm (\\(\\div\\)1000)</li> </ul> </li> <li>the base measurement divided by the region area in \u00b5m\u00b2 (density in something/\u00b5m\u00b2)</li> <li>the base measurement divided by the region area in mm\u00b2 (density in something/mm\u00b2)</li> <li>the squared base measurement divided by the region area in \u00b5m\u00b2 (could be an index, in weird units...)</li> <li>the relative base measurement : the base measurement divided by the total base measurement across all regions in each hemisphere</li> <li>the relative density : density divided by total density across all regions in each hemisphere</li> </ul> <p>It is then up to you to select which metrics among those to compute and display and name them, via the configuration file.</p> <p>For punctal detections (eg. objects whose only the centroid is considered), only the atlas coordinates are used, to compute and display spatial distributions of objects across the brain (using their classifications to give each distributions different hues). For fibers-like objects, it requires to export the lines detections atlas coordinates as JSON files, with the <code>exportFibersAtlasCoordinates.groovy</code> script (this is done automatically when using the pipeline).</p>"},{"location":"guide-prepare-qupath.html#adding-measurements","title":"Adding measurements","text":""},{"location":"guide-prepare-qupath.html#count-for-cell-like-objects","title":"Count for cell-like objects","text":"<p>The groovy script under <code>scripts/qupath-utils/measurements/addRegionsCount.groovy</code> will add a properly formatted count of objects of selected classifications in all atlas regions. This is used for punctual objects (polygons or points), for example objects created in QuPath or with the segmentation script.</p>"},{"location":"guide-prepare-qupath.html#cumulated-length-for-fibers-like-objects","title":"Cumulated length for fibers-like objects","text":"<p>The groovy script under <code>scripts/qupath-utils/measurements/addRegionsLength.groovy</code> will add the properly formatted cumulated lenghth in microns of fibers-like objects in all atlas regions. This is used for polylines objects, for example generated with the segmentation script.</p>"},{"location":"guide-prepare-qupath.html#custom-measurements","title":"Custom measurements","text":"<p>Keeping in mind <code>cuisto</code> limitations, you can add any measurements you'd like.</p> <p>For example, you can run a pixel classifier in all annotations (eg. atlas regions). Using the <code>Measure</code> button, it will add a measurement of the area covered by classified pixels. Then, you can use the script located under <code>scripts/qupath-utils/measurements/renameMeasurements.groovy</code> to rename the generated measurements with a properly-formatted name. Finally, you can export regions measurements.</p> <p>Since <code>cuisto</code> will compute a \"density\", eg. the measurement divided by the region area, in this case, it will correspond to the fraction of surface occupied by classified pixels. This is showcased in the Examples.</p>"},{"location":"guide-prepare-qupath.html#qupath-export","title":"QuPath export","text":"<p>Once you imported atlas regions registered with ABBA, detected objects in your images and added properly formatted measurements to detections and annotations, you can : </p> <ul> <li>Head to <code>Measure &gt; Export measurements</code></li> <li>Select relevant images</li> <li>Choose the <code>Output file</code> (specify in the file name if it is a detections or annotations file)</li> <li>Chose either <code>Detections</code> or <code>Annoations</code> in <code>Export type</code></li> <li>Click <code>Export</code></li> </ul> <p>Do this for both Detections and Annotations, you can then use those files with <code>cuisto</code> (see the Examples).</p>"},{"location":"guide-qupath-objects.html","title":"Detect objects with QuPath","text":"<p>The QuPath documentation is quite extensive, detailed, very well explained and contains full guides on how to create a QuPath project and how to find objects of interests. It is therefore a highly recommended read, nevertheless, you will find below some quick reminders.</p>"},{"location":"guide-qupath-objects.html#qupath-project","title":"QuPath project","text":"<p>QuPath works with projects. It is basically a folder with a main <code>project.qproj</code> file, which is a JSON file that contains all the data about your images except the images themselves. Algonside, there is a <code>data</code> folder with an entry for each image, that stores the thumbnails, metadata about the image and detections and annotations but, again, not the image itself. The actual images can be stored anywhere (including a remote server), the QuPath project merely contains the information needed to fetch them and display them. QuPath will never modify your image data.</p> <p>This design makes the QuPath project itself lightweight (should never exceed 500MB even with millions of detections), and portable : upon opening, if QuPath is not able to find the images where they should be, it will ask for their new locations.</p> <p>Tip</p> <p>It is recommended to create the QuPath project locally on your computer, to avoid any risk of conflicts if two people open it at the same time. Nevertheless, you should backup the project regularly on a remote server.</p> <p>To create a new project, simply drag &amp; drop an empty folder into QuPath window and accept to create a new empty project. Then, add images :</p> <ul> <li>If you have a single file, just drag &amp; drop it in the main window.</li> <li>If you have several images, in the left panel, click <code>Add images</code>, then <code>Choose files</code> on the bottom. Drag &amp; drop does not really work as the images will not be sorted properly.</li> </ul> <p>Then, choose the following options :</p> <code>Image server</code> <p>Default (let QuPath decide)</p> <code>Set image type</code> <p>Most likely, fluorescence</p> <code>Rotate image</code> <p>No rotation (unless all your images should be rotated)</p> <code>Optional args</code> <p>Leave empty</p> <code>Auto-generate pyramids</code> <p>Uncheck</p> <code>Import objects</code> <p>Uncheck</p> <code>Show image selector</code> <p>Might be useful to check if the images are read correctly (mostly for CZI files).</p>"},{"location":"guide-qupath-objects.html#detect-objects","title":"Detect objects","text":""},{"location":"guide-qupath-objects.html#built-in-cell-detection","title":"Built-in cell detection","text":"<p>QuPath has a built-in cell detection feature, available in <code>Analyze &gt; Cell detection</code>. You hava a full tutorial in the official documentation.</p> <p>Briefly, this uses a watershed algorithm to find bright spots and can perform a cell expansion to estimate the full cell shape based on the detected nuclei. Therefore, this works best to segment nuclei but one can expect good performance for cells as well, depending on the imaging and staining conditions.</p> <p>Tip</p> <p>In <code>scripts/qupath-utils/segmentation</code>, there is <code>watershedDetectionFilters.groovy</code> which uses this feature from a script. It further allows you to filter out detected cells based on shape measurements as well as fluorescence itensity in several channels and cell compartments.</p>"},{"location":"guide-qupath-objects.html#pixel-classifier","title":"Pixel classifier","text":"<p>Another very powerful and versatile way to segment cells if through machine learning. Note the term \"machine\" and not \"deep\" as it relies on statistics theory from the 1980s. QuPath provides an user-friendly interface to that, similar to what ilastik provides.</p> <p>The general idea is to train a model to classify every pixel as a signal or as background. You can find good resources on how to procede in the official documentation and some additionnal tips and tutorials on Michael Neslon's blog (here and here).</p> <p>Specifically, you will manually annotate some pixels of objects of interest and background. Then, you will apply some image processing filters (gaussian blur, laplacian...) to reveal specific features in your images (shapes, textures...). Finally, the pixel classifier will fit a model on those pixel values, so that it will be able to predict if a pixel, given the values with the different filters you applied, belongs to an object of interest or to the background.</p> <p>This is done in an intuitive GUI with live predictions to get an instant feedback on the effects of the filters and manual annotations.</p>"},{"location":"guide-qupath-objects.html#train-a-model","title":"Train a model","text":"<p>First and foremost, you should use a QuPath project dedicated to the training of a pixel classifier, as it is the only way to be able to edit it later on.</p> <ol> <li>You should choose some images from different animals, with different imaging conditions (staining efficiency and LED intensity) in different regions (eg. with different objects' shape, size, sparsity...). The goal is to get the most diversity of objects you could encounter in your experiments. 10 images is more than enough !</li> <li>Import those images to the new, dedicated QuPath project.</li> <li>Create the classifications you'll need, \"Cells: marker+\" for example. The \"Ignore*\" classification is used for the background.</li> <li>Head to <code>Classify &gt; Pixel classification &gt; Train pixel classifier</code>, and turn on <code>Live prediction</code>.</li> <li>Load all your images in <code>Load training</code>.</li> <li>In <code>Advanced settings</code>, check <code>Reweight samples</code> to help make sure a classification is not over-represented.</li> <li>Modify the different parameters :<ul> <li><code>Classifier</code> : typically, <code>RTrees</code> or <code>ANN_MLP</code>. This can be changed dynamically afterwards to see which works best for you.</li> <li><code>Resolution</code> : this is the pixel size used. This is a trade-off between accuracy and speed. If your objects are only composed of a few pixels, you'll the full resolution, for big objects reducing the resolution will be faster.</li> <li><code>Features</code> : this is the core of the process -- where you choose the filters. In <code>Edit</code>, you'll need to choose :<ul> <li>The fluorescence channels</li> <li>The scales, eg. the size of the filters applied to the image. The bigger, the coarser the filter is. Again, this will depend on the size of the objects you want to segment.</li> <li>The features themselves, eg. the filters applied to your images before feeding the pixel values to the model. For starters, you can select them all to see what they look like.</li> </ul> </li> <li><code>Output</code> :<ul> <li><code>Classification</code> : QuPath will directly classify the pixels. Use that to create objects directly from the pixel classifier within QuPath.</li> <li><code>Probability</code> : this will output an image where each pixel is its probability to belong to each of the classifications. This is useful to create objects externally.</li> </ul> </li> </ul> </li> <li>In the bottom-right corner of the pixel classifier window, you can select to display each filters individually. Then in the QuPath main window, hitting C will switch the view to appreciate what the filter looks like. Identify the ones that makes your objects the most distinct from the background as possible. Switch back to <code>Show classification</code> once you begin to make annotations.</li> <li> <p>Begin to annotate ! Use the Polyline annotation tool (V) to classify some pixels belonging to an object and some pixels belonging to the background across your images.</p> <p>Tip</p> <p>You can select the <code>RTrees</code> Classifier, then <code>Edit</code> : check the <code>Calculate variable importance</code> checkbox. Then in the log (Ctrl+Shift+L), you can inspect the weight each features have. This can help discard some filters to keep only the ones most efficient to distinguish the objects of interest.</p> </li> <li> <p>See in live the effect of your annotations on the classification using C and continue until you're satisfied.</p> <p>Important</p> <p>This is machine learning. The lesser annotations, the better, as this will make your model more general and adapt to new images. The goal is to find the minimal number of annotations to make it work.</p> </li> <li> <p>Once you're done, give your classifier a name in the text box in the bottom and save it. It will be stored as a JSON file in the <code>classifiers</code> folder of the QuPath project. This file can be imported in your other QuPath projects.</p> </li> </ol>"},{"location":"guide-qupath-objects.html#built-in-create-objects","title":"Built-in create objects","text":"<p>Once you imported your model JSON file (<code>Classify &gt; Pixel classification &gt; Load pixel classifier</code>, three-dotted menu and <code>Import from file</code>), you can create objects out of it, measure the surface occupied by classified pixels in each annotation or classify existing detections based on the prediction at their centroid.</p> <p>In <code>scripts/qupath-utils/segmentation</code>, there is a <code>createDetectionsFromPixelClassifier.groovy</code> script to batch-process your project.</p>"},{"location":"guide-qupath-objects.html#probability-map-segmentation","title":"Probability map segmentation","text":"<p>Alternatively, a Python script provided with <code>cuisto</code> can be used to segment the probability map generated by the pixel classifier (the script is located in <code>scripts/segmentation</code>).</p> <p>You will first need to export those with the <code>exportPixelClassifierProbabilities.groovy</code> script (located in <code>scripts/qupath-utils</code>).</p> <p>Then the segmentation script can :</p> <ul> <li>find punctal objects as polygons (with a shape) or points (punctal) than can be counted.</li> <li>trace fibers with skeletonization to create lines whose lengths can be measured.</li> </ul> <p>Several parameters have to be specified by the user,  see the segmentation script API reference. This script will generate GeoJson files that can be imported back to QuPath with the <code>importGeojsonFiles.groovy</code> script.</p>"},{"location":"guide-qupath-objects.html#third-party-extensions","title":"Third-party extensions","text":"<p>QuPath being open-source and extensible, there are third-party extensions that implement popular deep learning segmentation algorithms directly in QuPath. They can be used to find objects of interest as detections in the QuPath project and thus integrate nicely with <code>cuisto</code> to quantify them afterwards.</p>"},{"location":"guide-qupath-objects.html#instanseg","title":"InstanSeg","text":"<p>QuPath extension : https://github.com/qupath/qupath-extension-instanseg Original repository : https://github.com/instanseg/instanseg Reference papers : doi:10.48550/arXiv.2408.15954, doi:10.1101/2024.09.04.611150</p>"},{"location":"guide-qupath-objects.html#stardist","title":"Stardist","text":"<p>QuPath extension : https://github.com/qupath/qupath-extension-stardist Original repository : https://github.com/stardist/stardist Reference paper : doi:10.48550/arXiv.1806.03535</p> <p>There is a <code>stardistDetectionFilter.groovy</code> script in <code>scripts/qupath-utils/segmentation</code> to use it from a script which further allows you to filter out detected cells based on shape measurements as well as fluorescence itensity in several channels and cell compartments.</p>"},{"location":"guide-qupath-objects.html#cellpose","title":"Cellpose","text":"<p>QuPath extension : https://github.com/BIOP/qupath-extension-cellpose Original repository : https://github.com/MouseLand/cellpose Reference papers : doi:10.1038/s41592-020-01018-x, doi:10.1038/s41592-022-01663-4, doi:10.1101/2024.02.10.579780</p> <p>There is a <code>cellposeDetectionFilter.groovy</code> script in <code>scripts/qupath-utils/segmentation</code> to use it from a script which further allows you to filter out detected cells based on shape measurements as well as fluorescence itensity in several channels and cell compartments.</p>"},{"location":"guide-qupath-objects.html#sam","title":"SAM","text":"<p>QuPath extension : https://github.com/ksugar/qupath-extension-sam Original repositories : samapi, SAM Reference papers : doi:10.1101/2023.06.13.544786, doi:10.48550/arXiv.2304.02643</p> <p>This is more an interactive annotation tool than a fully automatic segmentation algorithm.</p>"},{"location":"guide-register-abba.html","title":"Registration with ABBA","text":"<p>The ABBA documentation is quite extensive and contains guided tutorials and a video tutorial. You should therefore check it out ! Nevertheless, you will find below some quick reminders.</p>"},{"location":"guide-register-abba.html#import-a-qupath-project","title":"Import a QuPath project","text":"<p>Always use ABBA with a QuPath project, if you import the images directly it will not be possible to export the results back to QuPath. In the toolbar, head to <code>Import &gt; Import QuPath Project</code>.</p> <ul> <li>Select the .qproj file corresponding to the QuPath project to be aligned.</li> <li>Initial axis position : this is the initial position where to put your stack. It will be modified afterwards.</li> <li>Axis increment between slices : this is the spatial spacing, in mm, between two slices. This would correspond to the slice thickness multiplied by the number of set. If your images are ordered from rostral to caudal, set it negative.</li> </ul> <p>Warning</p> <p>ABBA is not the most stable software, it is highly recommended to save in a different file each time you do anything.</p>"},{"location":"guide-register-abba.html#navigation","title":"Navigation","text":""},{"location":"guide-register-abba.html#interface","title":"Interface","text":"<ul> <li>Left Button + drag to select slices</li> <li>Right Button for display options</li> <li>Right Button + drag to browse the view</li> <li>Middle Button to zoom in and or out</li> </ul>"},{"location":"guide-register-abba.html#right-panel","title":"Right panel","text":"<p>In the right panel, there is everything related to the images, both yours and the atlas.</p> <p>In the <code>Atlas Display</code> section, you can turn on and off different channels (the first is the reference image, the last is the regions outlines). The <code>Displayed slicing [atlas steps]</code> slider can increase or decrease the number of displayed 2D slices extracted from the 3D volume. It is comfortable to set to to the same spacing as your slices. Remember it is in \"altas steps\", so for an atlas imaged at 10\u00b5m, a 120\u00b5m spacing corresponds to 12 atlas steps.</p> <p>The <code>Slices Display</code> section lists all your slices. Ctrl+A to select all, and click on the <code>Vis.</code> header to make them visible. Then, you can turn on and off each channels (generally the NISSL channel and the ChAT channel will be used) by clicking on the corresponding header. Finally, set the display limits clicking on the empty header containing the colors.</p> <p>Right Button in the main view to <code>Change overlap mode</code> twice to get the slices right under the atlas slices.</p> <p>Tip</p> <p>Every action in ABBA are stored and are cancellable with Right Button+Z, except the Interactive transform.</p>"},{"location":"guide-register-abba.html#find-position-and-angle","title":"Find position and angle","text":"<p>This is the hardest task. You need to drag the slices along the rostro-caudal axis and modify the virtual slicing angle (<code>X Rotation [deg]</code> and <code>Y Rotation [deg]</code> sliders at the bottom of the right panel) until you match the brain structures observed in both your images and the atlas.</p> <p>Tip</p> <p>With a high number of slices, most likely, it will be impossible to find a position and slicing angle that works for all your slices. In that case, you should procede in batch, eg. sub-stack of images with a unique position and slicing angle that works for all images in the sub-stack. Then, remove the remaining slices (select them, Right Button <code>&gt; Remove Selected Slices</code>), but do not remove them from the QuPath project.</p> <p>Procede as usual, including saving (note the slices range it corresponds to) and exporting the registration back to QuPath. Then, reimport the project in a fresh ABBA instance, remove the slices that were already registered and redo the whole process with the next sub-stack and so on.</p> <p>Once you found the correct position and slicing angle, it must not change anymore, otherwise the registration operations you perform will not make any sense anymore.</p>"},{"location":"guide-register-abba.html#in-plane-registration","title":"In-plane registration","text":"<p>The next step is to deform your slices to match the corresponding atlas image, extracted from the 3D volume given the position and virtual slicing angle defined at the previous step.</p> <p>Info</p> <p>ABBA makes the choice to deform your slices to the atlas, but the transformations are invertible. This means that you will still be able to work on your raw data and deform the altas onto it instead.</p> <p>In image processing, there are two kinds of deformation one can apply on an image :</p> <ul> <li>Affine (or linear) : simple, image-wide, linear operations - translation, rotation, scaling, shearing.</li> <li>Spline (or non-linear) : complex non-linear operations that can allow for local deformation.</li> </ul> <p>Both can be applied manually or automatically (if the imaging quality allows it). You have different tools to achieve this, all of which can be combined in any order, except the Interactive transform tool (coarse, linear manual deformation).</p> <p>Change the overlap mode (Right Button) to overlay the slice onto the atlas regions borders. Select the slice you want to align.</p>"},{"location":"guide-register-abba.html#coarse-linear-manual-deformation","title":"Coarse, linear manual deformation","text":"<p>While not mandatory, if this tool shall be used, it must be before any operation as it is not cancellable. Head to <code>Register &gt; Affine &gt; Interactive transform</code>. This will open a box where you can rotate, translate and resize the image to make a first, coarse alignment.</p> <p>Close the box. Again, this is not cancellable. Afterwards, you're free to apply any numbers of transformations in any order.</p>"},{"location":"guide-register-abba.html#automatic-registration","title":"Automatic registration","text":"<p>This uses the elastix toolbox to compute the transformations needed to best match two images. It is available in both affine and spline mode, in the <code>Register &gt; Affine</code> and <code>Register &gt; Spline</code> menus respectively.</p> <p>In both cases, it will open a dialog where you need to choose :</p> <ul> <li>Atlas channels : the reference image of the atlas, usually channel number 0</li> <li>Slices channels : the fluorescence channel that looks like the most to the reference image, usually channel number 0</li> <li>Registration re-sampling (micrometers) : the pixel size to resize the images before registration, as it is a computationally intensive task. Going below 20\u00b5m won't help much.</li> </ul> <p>For the Spline mode, there an additional parameter :</p> <ul> <li>Number of control points along X : the algorithm will set points as a grid in the image and perform the transformations from those. The higher number of points, the more local transformations will be.</li> </ul>"},{"location":"guide-register-abba.html#manual-registration","title":"Manual registration","text":"<p>This uses BigWarp to manually deform the images with the mouse. It can be done from scratch (eg. you place the points yourself) or from a previous registration (either a previous BigWarp session or elastix in Spline mode).</p>"},{"location":"guide-register-abba.html#from-scratch","title":"From scratch","text":"<p><code>Register &gt; Spline &gt; BigWarp registration</code> to launch the tool. Choose the atlas that allows you to best see the brain structures (usually the regions outlines channels, the last one), and the reference fluorescence channel.</p> <p>It will open two viewers, called \"BigWarp moving image\" and \"BigWarp fixed image\". Briefly, they correspond to the two spaces you're working in, the \"Atlas space\" and the \"Slice space\".</p> <p>Tip</p> <p>Do not panick yet, while the explanations might be confusing (at least they were to me), in practice, it is easy, intuitive and can even be fun (sometimes, at small dose).</p> <p>To browse the viewer, use Right Button + drag (Left Button is used to rotate the viewer), Middle Button zooms in and out.</p> <p>The idea is to place points, called landmarks, that always go in pairs : one in the moving image and one where it corresponds to in the fixed image (or vice-versa). In practice, we will only work in the BigWarp fixed image viewer to place landmarks in both space in one click, then drag it to the corresponding location, with a live feedback of the transformation needed to go from one to another.</p> <p>To do so :</p> <ol> <li> <p>Press Space to switch to the \"Landmark mode\".</p> <p>Warning</p> <p>In \"Landmark mode\", Right Button can't be used to browse the view anymore. To do so, turn off the \"Landmark mode\" hitting Space again. </p> </li> <li> <p>Use Ctrl+Left Button to place a landmark.</p> <p>Info</p> <p>At least 4 landmarks are needed before activating the live-transform view.</p> </li> <li> <p>When there are at least 4 landmarks, hit T to activate the \"Transformed\" view. <code>Transformed</code> will be written at the bottom.</p> </li> <li>Hold Left Button on a landmark to drag it to deform the image onto the atlas.</li> <li>Add as many landmarks as needed, when you're done, find the Fiji window called \"Big Warp registration\" that opened at the beginning and click <code>OK</code>.</li> </ol> <p>Important remarks and tips</p> <ul> <li>A landmark is a location where you said \"this location correspond to this one\". Therefore, BigWarp is not allowed to move this particular location. Everywhere else, it is free to transform the image without any restrictions, including the borders. Thus, it is a good idea to delimit the coarse contour of the brain with landmarks to constrain the registration.</li> <li>Left Button without holding Ctrl will place a landmark in the fixed image only, without pair, and BigWarp won't like it. To delete landmarks, head to the \"Landmarks\" window that lists all of them. They highlight in the viewer upon selection. Hit Del to delete one. Alternatively, click on it on the viewer and hit Del.</li> </ul>"},{"location":"guide-register-abba.html#from-a-previous-registration","title":"From a previous registration","text":"<p>Head to <code>Register &gt; Edit last Registration</code> to work on a previous registration.</p> <p>If the previous registration was done with elastix (Spline) or BigWarp, it will launch the BigWarp interface exactly like above, but with landmarks already placed, either on a grid (elastix) or the one you manually placed (BigWarp).</p> <p>Tip</p> <p>It will ask which channels to use, you can modify the channel for your slices to work on two channels successively. For instance, one could make a first registration using the NISSL staining, then refine the motoneurons with the ChAT staining, if available.</p>"},{"location":"guide-register-abba.html#abba-state-file","title":"ABBA state file","text":"<p>ABBA can save the state you're in, from the <code>File &gt; Save State</code> menu. It will be saved as a <code>.abba</code> file, which is actually a zip archive containing a bunch of JSON, listing every actions you made and in which order, meaning you will stil be able to cancel actions after quitting ABBA.</p> <p>To load a state, quit ABBA, launch it again, then choose <code>File &gt; Load State</code> and select the <code>.abba</code> file to carry on with the registration.</p> <p>Save, save, save !</p> <p>Those state files are cheap, eg. they are lightweight (less than 200KB). You should save the state each time you finish a slice, and you can keep all your files, without overwritting the previous ones, appending a number to its file name. This will allow to roll back to the previous slice in the event of any problem you might face.</p>"},{"location":"guide-register-abba.html#export-registration-back-to-qupath","title":"Export registration back to QuPath","text":""},{"location":"guide-register-abba.html#export-the-registration-from-abba","title":"Export the registration from ABBA","text":"<p>Once you are satisfied with your registration, select the registered slices and head to <code>Export &gt; QuPath &gt; Export Registrations To QuPath Project</code>. Check the box to make sure to get the latest registered regions.</p> <p>It will export several files in the QuPath projects, including the transformed atlas regions ready to be imported in QuPath and the transformations parameters to be able to convert coordinates from the extension.</p>"},{"location":"guide-register-abba.html#import-the-registration-in-qupath","title":"Import the registration in QuPath","text":"<p>Make sure you installed the ABBA extension in QuPath.</p> <p>From your project with an image open, the basic usage is to head to <code>Extensions &gt; ABBA &gt; Load Atlas Annotations into Open Image</code>. Choose to <code>Split Left and Right Regions</code> to make the two hemispheres independent, and choose the \"acronym\" to name the regions. The registered regions should be imported as Annotations in the image.</p> <p>Tip</p> <p>With ABBA in regular Fiji using the CCFv3 Allen mouse brain atlas, the left and right regions are flipped, because ABBA considers the slices as backward facing. The <code>importAbba.groovy</code> script located in <code>scripts/qupath-utils-atlas</code> allows you to flip left/right regions names. This is OK because the Allen brain is symmetrical by construction.</p> <p>For more complex use, check the Groovy scripts in <code>scripts/qupath-utils/atlas</code>. ABBA registration is used throughout the guides, to either work with brain regions (and count objects for instance) or to get the detections' coordinates in the atlas space.</p>"},{"location":"main-citing.html","title":"Citing","text":"<p>While <code>cuisto</code> does not have a reference paper as of now, you can reference the GitHub repository.</p> <p>Please make sure to cite all the softwares used in your research. Citations are usually the only metric used by funding agencies, so citing properly the tools used in your research ensures the continuation of those projects.</p> <ul> <li>Fiji : https://imagej.net/software/fiji/#publication</li> <li>QuPath : https://qupath.readthedocs.io/en/stable/docs/intro/citing.html</li> <li>ABBA : doi:10.1101/2024.09.06.611625 </li> <li>Brainglobe :<ul> <li>AtlasAPI : https://brainglobe.info/documentation/brainglobe-atlasapi/index.html#citation</li> <li>Brainrender : https://brainglobe.info/documentation/brainrender/index.html#citation</li> </ul> </li> <li>Allen brain atlas (CCFv3) : doi:10.1016/j.cell.2020.04.007</li> <li>3D Allen spinal cord atlas : doi:10.1016/j.crmeth.2021.100074</li> <li>Skeleton analysis (for fibers-like segmentation) : doi:10.7717/peerj.4312</li> </ul>"},{"location":"main-configuration-files.html","title":"The configuration files","text":"<p>There are three configuration files : <code>altas_blacklist</code>, <code>atlas_fusion</code> and a modality-specific file, that we'll call <code>config</code> in this document. The former two are related to the atlas you're using, the latter is what is used by <code>cuisto</code> to know what and how to compute and display things. There is a fourth, optional, file, used to provide some information on a specific experiment, <code>info</code>.</p> <p>The configuration files are in the TOML file format, that are basically text files formatted in a way that is easy to parse in Python. See here for a basic explanation of the syntax.</p> <p>Most lines of each template file are commented to explain what each parameter do.</p>"},{"location":"main-configuration-files.html#atlas_blacklisttoml","title":"atlas_blacklist.toml","text":"Click to see an example file atlas_blacklist.toml<pre><code># TOML file to list Allen brain regions to ignore during analysis.\n# \n# It is used to blacklist regions and all descendants regions (\"WITH_CHILD\").\n# Objects belonging to those regions and their descendants will be discarded.\n# And you can specify an exact region where to remove objects (\"EXACT\"),\n# descendants won't be affected.\n# Use it to remove noise in CBX, ventricual systems and fiber tracts.\n# Regions are referenced by their exact acronym.\n#\n# Syntax :\n#   [WITH_CHILDS]\n#   members = [\"CBX\", \"fiber tracts\", \"VS\"]\n#\n#   [EXACT]\n#   members = [\"CB\"]\n\n\n[WITH_CHILDS]\nmembers = [\"CBX\", \"fiber tracts\", \"VS\"]\n\n[EXACT]\nmembers = [\"CB\"]\n</code></pre> <p>This file is used to filter out specified regions and objects belonging to them.</p> <ul> <li>The atlas regions present in the <code>members</code> keys will be ignored. Objects whose parents are in here will be ignored as well.</li> <li>In the <code>[WITH_CHILDS]</code> section, regions and objects belonging to those regions and all descending regions (child regions, as per the altas hierarchy) will be removed.</li> <li>In the <code>[EXACT]</code> section, only regions and objects belonging to those exact regions are removed. Descendants regions are not taken into account.</li> </ul>"},{"location":"main-configuration-files.html#atlas_fusiontoml","title":"atlas_fusion.toml","text":"Click to see an example file atlas_blacklist.toml<pre><code># TOML file to determine which brain regions should be merged together.\n# Regions are referenced by their exact acronym.\n# The syntax should be the following :\n# \n#   [MY]\n#   name = \"Medulla\"  # new or existing full name\n#   acronym = \"MY\"  # new or existing acronym\n#   members = [\"MY-mot\", \"MY-sat\"]  # existing Allen Brain acronyms that should belong to the new region\n#\n# Then, regions labelled \"MY-mot\" and \"MY-sat\" will be labelled \"MY\" and will join regions already labelled \"MY\".\n# What's in [] does not matter but must be unique and is used to group.\n# The new \"name\" and \"acronym\" can be existing Allen Brain regions or a new (meaningful) one.\n# Note that it is case sensitive.\n\n[PHY]\nname = \"Perihypoglossal nuclei\"\nacronym = \"PHY\"\nmembers = [\"NR\", \"PRP\"]\n\n[NTS]\nname = \"Nucleus of the solitary tract\"\nacronym = \"NTS\"\nmembers = [\"ts\", \"NTSce\", \"NTSco\", \"NTSge\", \"NTSl\", \"NTSm\"]\n\n[AMB]\nname = \"Nucleus ambiguus\"\nacronym = \"AMB\"\nmembers = [\"AMBd\", \"AMBv\"]\n\n[MY]\nname = \"Medulla undertermined\"\nacronym = \"MYu\"\nmembers = [\"MY-mot\", \"MY-sat\"]\n\n[IRN]\nname = \"Intermediate reticular nucleus\"\nacronym = \"IRN\"\nmembers = [\"IRN\", \"LIN\"]\n</code></pre> <p>This file is used to group regions together, to customize the atlas' hierarchy. It is particularly useful to group smalls brain regions that are impossible to register precisely. Keys <code>name</code>, <code>acronym</code> and <code>members</code> should belong to a <code>[section]</code>.</p> <ul> <li><code>[section]</code> is just for organizing, the name does not matter but should be unique.</li> <li><code>name</code> should be a human-readable name for your new region.</li> <li><code>acronym</code> is how the region will be refered to. It can be a new acronym, or an existing one.</li> <li><code>members</code> is a list of acronyms of atlas regions that should be part of the new one.</li> </ul>"},{"location":"main-configuration-files.html#configtoml","title":"config.toml","text":"Click to see an example file config_template.toml<pre><code>########################################################################################\n# Configuration file for cuisto package\n# -----------------------------------------\n# This is a TOML file. It maps a key to a value : `key = value`.\n# Each key must exist and be filled. The keys' names can't be modified, except:\n#   - entries in the [channels.names] section and its corresponding [channels.colors] section,\n#   - entries in the [regions.metrics] section.                                                                                   \n#\n# It is strongly advised to NOT modify this template but rather copy it and modify the copy.\n# Useful resources :\n#   - the TOML specification : https://toml.io/en/\n#   - matplotlib colors : https://matplotlib.org/stable/gallery/color/color_demo.html\n#\n# Configuration file part of the python cuisto package.\n# version : 2.1\n########################################################################################\n\nobject_type = \"Cells\"  # name of QuPath base classification (eg. without the \": subclass\" part)\nsegmentation_tag = \"cells\"  # type of segmentation, matches directory name, used only in the full pipeline\n\n[atlas]  # information related to the atlas used\nname = \"allen_mouse_10um\"  # brainglobe-atlasapi atlas name\ntype = \"brain\"  # brain or cord (eg. registration done in ABBA or abba_python)\nmidline = 5700  # midline Z coordinates (left/right limit) in microns\noutline_structures = [\"root\", \"CB\", \"MY\", \"P\"]  # structures to show an outline of in heatmaps\n\n[channels]  # information related to imaging channels\n[channels.names]  # must contain all classifications derived from \"object_type\"\n\"marker+\" = \"Positive\"  # classification name = name to display\n\"marker-\" = \"Negative\"\n[channels.colors]  # must have same keys as names' keys\n\"marker+\" = \"#96c896\"  # classification name = matplotlib color (either #hex, color name or RGB list)\n\"marker-\" = \"#688ba6\"\n\n[hemispheres]  # information related to hemispheres\n[hemispheres.names]\nLeft = \"Left\"  # Left = name to display\nRight = \"Right\"  # Right = name to display\n[hemispheres.colors]  # must have same keys as names' keys\nLeft = \"#ff516e\"  # Left = matplotlib color (either #hex, color name or RGB list)\nRight = \"#960010\"  # Right = matplotlib color\n\n[distributions]  # spatial distributions parameters\nstereo = true  # use stereotaxic coordinates (Paxinos, only for brain)\nap_lim = [-8.0, 0.0]  # bins limits for anterio-posterior\nap_nbins = 75  # number of bins for anterio-posterior\ndv_lim = [-1.0, 7.0]  # bins limits for dorso-ventral\ndv_nbins = 50  # number of bins for dorso-ventral\nml_lim = [-5.0, 5.0]  # bins limits for medio-lateral\nml_nbins = 50  # number of bins for medio-lateral\nhue = \"channel\"  # color curves with this parameter, must be \"hemisphere\" or \"channel\"\nhue_filter = \"Left\"  # use only a subset of data. If hue=hemisphere : channel name, list of such or \"all\". If hue=channel : hemisphere name or \"both\".\ncommon_norm = true  # use a global normalization for each hue (eg. the sum of areas under all curves is 1)\n[distributions.display]\nshow_injection = false  # add a patch showing the extent of injection sites. Uses corresponding channel colors\ncmap = \"OrRd\"  # matplotlib color map for heatmaps\ncmap_nbins = 50  # number of bins for heatmaps\ncmap_lim = [1, 50]  # color limits for heatmaps\n\n[regions]  # distributions per regions parameters\nbase_measurement = \"Count\"  # the name of the measurement in QuPath to derive others from\nhue = \"channel\"  # color bars with this parameter, must be \"hemisphere\" or \"channel\"\nhue_filter = \"Left\"  # use only a subset of data. If hue=hemisphere : channel name, list of such or \"all\". If hue=channel : hemisphere name or \"both\".\nhue_mirror = false  # plot two hue_filter in mirror instead of discarding the other\nnormalize_starter_cells = false  # normalize non-relative metrics by the number of starter cells\n[regions.metrics]  # names of metrics. Do not change the keys !\n\"density \u00b5m^-2\" = \"density \u00b5m^-2\"\n\"density mm^-2\" = \"density mm^-2\"\n\"coverage index\" = \"coverage index\"\n\"relative measurement\" = \"relative count\"\n\"relative density\" = \"relative density\"\n[regions.display]\nnregions = 18  # number of regions to display (sorted by max.)\norientation = \"h\"  # orientation of the bars (\"h\" or \"v\")\norder = \"max\"  # order the regions by \"ontology\" or by \"max\". Set to \"max\" to provide a custom order\ndodge = true  # enforce the bar not being stacked\nlog_scale = false  # use log. scale for metrics\n[regions.display.metrics]  # name of metrics to display\n\"count\" = \"count\"  # real_name = display_name, with real_name the \"values\" in [regions.metrics]\n\"density mm^-2\" = \"density (mm^-2)\"\n\n[files]  # full path to information TOML files\nblacklist = \"../../atlas/atlas_blacklist.toml\"\nfusion = \"../../atlas/atlas_fusion.toml\"\noutlines = \"/data/atlases/allen_mouse_10um_outlines.h5\"\ninfos = \"../../configs/infos_template.toml\"\n</code></pre> <p>This file is used to configure <code>cuisto</code> behavior. It specifies what to compute, how, and display parameters such as colors associated to each classifications, hemisphere names, distributions bins limits...</p> <p>Warning</p> <p>When editing your config.toml file, you're allowed to modify the keys only in the <code>[channels]</code> section.</p> Click for a more readable parameters explanation <p><code>object_type</code> : name of QuPath base classification (eg. without the \": subclass\" part) <code>segmentation_tag</code> : type of segmentation, matches directory name, used only in the full pipeline</p> <p>atlas Information related to the atlas used</p> <p><code>name</code> : brainglobe-atlasapi atlas name <code>type</code> : \"brain\" or \"cord\" (eg. registration done in ABBA or abba_python). This will determine whether to flip Left/Right when determining detections hemisphere based on their coordinates. Also adapts the axes in the 2D heatmaps. <code>midline</code> : midline Z coordinates (left/right limit) in microns to determine detections hemisphere based on their coordinates. <code>outline_structures</code> : structures to show an outline of in heatmaps  </p> <p>channels Information related to imaging channels</p> <p>names Must contain all classifications derived from \"object_type\" you want to process. In the form <code>subclassification name = name to display on the plots</code></p> <p><code>\"marker+\"</code> : classification name = name to display <code>\"marker-\"</code> : add any number of sub-classification</p> <p>colors Must have same keys as \"names\" keys, in the form <code>subclassification name = color</code>, with color specified as a matplotlib named color, an RGB list or an hex code.</p> <p><code>\"marker+\"</code> : classification name = matplotlib color <code>\"marker-\"</code> : must have the same entries as \"names\".</p> <p>hemispheres Information related to hemispheres, same structure as channels</p> <p>names</p> <p><code>Left</code> : Left = name to display <code>Right</code> : Right = name to display</p> <p>colors Must have same keys as names' keys</p> <p><code>Left</code> : ff516e\"  # Left = matplotlib color (either #hex, color name or RGB list) <code>Right</code> : 960010\"  # Right = matplotlib color</p> <p>distributions Spatial distributions parameters</p> <p><code>stereo</code> : use stereotaxic coordinates (as in Paxinos, only for mouse brain CCFv3) <code>ap_lim</code> : bins limits for anterio-posterior in mm <code>ap_nbins</code> : number of bins for anterio-posterior <code>dv_lim</code> : bins limits for dorso-ventral in mm <code>dv_nbins</code> : number of bins for dorso-ventral <code>ml_lim</code> : bins limits for medio-lateral in mm <code>ml_nbins</code> : number of bins for medio-lateral <code>hue</code> : color curves with this parameter, must be \"hemisphere\" or \"channel\" <code>hue_filter</code> : use only a subset of data</p> <ul> <li>If hue=hemisphere : it should be a channel name, a list of such or \"all\"  </li> <li>If hue=channel : it should be a hemisphere name or \"both\"</li> </ul> <p><code>common_norm</code> : use a global normalization (eg. the sum of areas under all curves is 1). Otherwise, normalize each hue individually</p> <p>display Display parameters</p> <p><code>show_injection</code> : add a patch showing the extent of injection sites. Uses corresponding channel colors. Requires the information TOML configuration file set up <code>cmap</code> : matplotlib color map for 2D heatmaps <code>cmap_nbins</code> : number of bins for 2D heatmaps <code>cmap_lim</code> : color limits for 2D heatmaps</p> <p>regions Distributions per regions parameters</p> <p><code>base_measurement</code> : the name of the measurement in QuPath to derive others from. Usually \"Count\" or \"Length \u00b5m\" <code>hue</code> : color bars with this parameter, must be \"hemisphere\" or \"channel\" <code>hue_filter</code> : use only a subset of data</p> <ul> <li>If hue=hemisphere : it should be a channel name, a list of such or \"all\"</li> <li>If hue=channel : it should be a hemisphere name or \"both\"</li> </ul> <p><code>hue_mirror</code> : plot two hue_filter in mirror instead of discarding the others. For example, if hue=channel and hue_filter=\"both\", plots the two hemisphere in mirror. <code>normalize_starter_cells</code> : normalize non-relative metrics by the number of starter cells</p> <p>metrics Names of metrics. The keys are used internally in cuisto as is so should NOT be modified. The values will only chang etheir names in the ouput file</p> <p><code>\"density \u00b5m^-2\"</code> : relevant name <code>\"density mm^-2\"</code> : relevant name <code>\"coverage index\"</code> : relevant name <code>\"relative measurement\"</code> : relevant name <code>\"relative density\"</code> : relevant name</p> <p>display</p> <p><code>nregions</code> : number of regions to display (sorted by max.) <code>orientation</code> : orientation of the bars (\"h\" or \"v\") <code>order</code> : order the regions by \"ontology\" or by \"max\". Set to \"max\" to provide a custom order <code>dodge</code> : enforce the bar not being stacked <code>log_scale</code> : use log. scale for metrics</p> <p>metrics name of metrics to display</p> <p><code>\"count\"</code> : real_name = display_name, with real_name the \"values\" in [regions.metrics] <code>\"density mm^-2\"</code></p> <p>files Full path to information TOML files and atlas outlines for 2D heatmaps.</p> <p><code>blacklist</code> <code>fusion</code> <code>outlines</code> <code>infos</code> </p>"},{"location":"main-configuration-files.html#infotoml","title":"info.toml","text":"Click to see an example file info_template.toml<pre><code># TOML file to specify experimental settings of each animals.\n# Syntax should be :\n#   [animalid0]  # animal ID\n#   slice_thickness = 30  # slice thickness in microns\n#   slice_spacing = 60  # spacing between two slices in microns\n#   [animalid0.marker-name]  # [{Animal id}.{segmented channel name}]\n#   starter_cells = 190  # number of starter cells\n#   injection_site = [x, y, z]  # approx. injection site in CCFv3 coordinates\n#\n# --------------------------------------------------------------------------\n[animalid0]\nslice_thickness = 30\nslice_spacing = 60\n[animalid0.\"marker+\"]\nstarter_cells = 150\ninjection_site = [ 10.8937328, 6.18522070, 6.841855301 ]\n[animalid0.\"marker-\"]\nstarter_cells = 175\ninjection_site = [ 10.7498512, 6.21545461, 6.815487203 ]\n# --------------------------------------------------------------------------\n[animalid1-SC]\nslice_thickness = 30\nslice_spacing = 120\n[animalid1-SC.EGFP]\nstarter_cells = 250\ninjection_site = [ 10.9468211, 6.3479642, 6.0061113 ]\n[animalid1-SC.DsRed]\nstarter_cells = 275\ninjection_site = [ 10.9154874, 6.2954872, 8.1587125 ]\n# --------------------------------------------------------------------------\n</code></pre> <p>This file is used to specify injection sites for each animal and each channel, to display it in distributions.</p>"},{"location":"main-getting-help.html","title":"Getting help","text":"<p>For help in QuPath, ABBA, Fiji or any image processing-related questions, your one stop is the image.sc forum. There, you can search with specific tags (<code>#qupath</code>, <code>#abba</code>, ...). You can also ask questions or even answer to some by creating an account !</p> <p>For help with <code>cuisto</code> in particular, you can open an issue in Github (which requires an account as well), or send an email to me or Antoine Lesage.</p>"},{"location":"main-getting-started.html","title":"Getting started","text":""},{"location":"main-getting-started.html#quick-start","title":"Quick start","text":"<ol> <li>Install QuPath, ABBA and conda.</li> <li>Create an environment : <pre><code>conda create -c conda-forge -n cuisto-env python=3.12\n</code></pre></li> <li>Activate it : <pre><code>conda activate cuisto-env\n</code></pre></li> <li>Install <code>cuisto</code> <pre><code>pip install cuisto\n</code></pre> If you want to build the doc locally : <pre><code>pip install cuisto[doc]\n</code></pre></li> <li>Check the Examples section !</li> </ol>"},{"location":"main-getting-started.html#slow-start","title":"Slow start","text":"<p>Tip</p> <p>If all goes well, you shouldn't need any admin rights to install the various pieces of software used before <code>cuisto</code>.</p> <p>Important</p> <p>Remember to cite all softwares you use ! See Citing.</p>"},{"location":"main-getting-started.html#qupath","title":"QuPath","text":"<p>QuPath is an \"open source software for bioimage analysis\". You can install it from the official website : https://qupath.github.io/. The documentation is quite clear and comprehensive : https://qupath.readthedocs.io/en/stable/index.html.</p> <p>This is where you'll create QuPath projects, in which you'll be able to browse your images, annotate them, import registered brain regions and find objects of interests (via automatic segmentation, thresholding, pixel classification, ...). Then, those annotations and detections can be exported to be processed by <code>cuisto</code>.</p>"},{"location":"main-getting-started.html#aligning-big-brain-and-atlases-abba","title":"Aligning Big Brain and Atlases (ABBA)","text":"<p>This is the tool you'll use to register 2D histological sections to 3D atlases. See the dedicated page.</p>"},{"location":"main-getting-started.html#python-virtual-environment-manager-conda","title":"Python virtual environment manager (<code>conda</code>)","text":"<p>The <code>cuisto</code> package is written in Python. It depends on scientific libraries (such as NumPy, pandas and many more). Those libraries need to be installed in versions that are compatible with each other and with <code>cuisto</code>. To make sure those versions do not conflict with other Python tools you might be using (<code>deeplabcut</code>, <code>abba_python</code>, ...), we will install <code>cuisto</code> and its dependencies in a dedicated virtual environment.</p> <p><code>conda</code> is a software that takes care of this. It comes with a \"base\" environment, from which we will create and manage other, project-specific environments. It is also used to download and install python in each of those environments, as well as third-party libraries. <code>conda</code> in itself is free and open-source and can be used freely by anyone.</p> <p>It is included with the Anaconda distribution, which is subject to specific terms of service, which state that unless you're an individual, a member of a company with less than 200 employees or a member of an university (but not a national research lab) it's free to use, otherwise, you need to pay a licence. <code>conda</code>, while being free, is by default configured to use the \"defaults\" channel to fetch the packages (including Python itself), a repository operated by Anaconda, which is, itself, subject to the Anaconda terms of service.</p> <p>In contrast, conda-forge is a community-run repository that contains more numerous and more update-to-date packages. This is free to use for anyone. The idea is to use <code>conda</code> directly (instead of Anaconda graphical interface) and download packages from conda-forge (instead of the Anaconda-run defaults). To try to decipher this mess, Anaconda provides this figure :</p> <p></p> <p>Furthermore, the \"base\" conda environment installed with the Anaconda distribution is bloated and already contains tons of libraries, and tends to self-destruct at some point (eg. becomes unable to resolve the inter-dependencies), which makes you unable to install new libraries nor create new environments.</p> <p>This is why it is highly recommended to install Miniconda instead, a minimal installer for conda, and configure it to use the free, community-run channel conda-forge, or, even better, use Miniforge which is basically the same but pre-configured to use conda-forge. The only downside is that will not get the Anaconda graphical user interface and you'll need to use the terminal instead, but worry not ! We got you covered.</p> <ol> <li>Download and install Miniforge (choose the latest release for your system). During the installation, choose to install for the current user, add conda to PATH and make python the default interpreter.</li> <li>Open a terminal (PowerShell in Windows). Run : <pre><code>conda init\n</code></pre> This will activate conda and its base environment whenever you open a new PowerShell window. Now, when opening a new PowerShell (or terminal), you should see a prompt like this : <pre><code>(base) PS C:\\Users\\myname&gt;\n</code></pre></li> </ol> <p>Tip</p> <p>If Anaconda is already installed and you don't have the rights to uninstall it, you'll have to use it instead. You can launch the \"Anaconda Prompt (PowerShell)\", run <code>conda init</code>. Open a regular PowerShell window and run <code>conda config --add channels conda-forge</code>, so that subsequent installations and environments creation will fetch required dependencies from conda-forge.</p>"},{"location":"main-getting-started.html#installation","title":"Installation","text":"<p>This section explains how to actually install the <code>cuisto</code> package. The following commands should be run from a terminal (PowerShell). Remember that the <code>-c conda-forge</code> bits are not necessary if you installed conda with the miniforge distribution.</p> <ol> <li>Create a virtual environment with python 3.12 : <pre><code>conda create -c conda-forge -n cuisto-env python=3.12\n</code></pre></li> <li>We need to install it inside the <code>cuisto-env</code> environment we just created. First, you need to activate the <code>cuisto-env</code> environment : <pre><code>conda activate cuisto-env\n</code></pre> Now, the prompt should look like this : <pre><code>(cuisto-env) PS C:\\Users\\myname&gt;\n</code></pre> This means that Python packages will now be installed in the <code>cuisto-env</code> environment and won't conflict with other toolboxes you might be using.</li> <li>Then, we use <code>pip</code> to install <code>cuisto</code>. <code>pip</code> was installed with Python, and will fetch the latest release from PyPI. <pre><code>pip install cuisto\n</code></pre></li> <li>Get a copy of the <code>cuisto</code> Source code .zip package, from the Releases page, to have access to all the utility scripts in the <code>scripts/</code> folder.</li> </ol> <p><code>cuisto</code> is now installed inside the <code>cuisto-env</code> environment and will be available in Python from that environment !</p> <p>Tip</p> <p>You can run <code>pip install cuisto --upgrade</code> from the <code>cuisto-env</code> environment to update the package.</p> <p>If you already have registered data and cells in QuPath, you can export Annotations and Detections as TSV files and head to the Example section. Otherwise, check out the Guides section on the left panel.</p>"},{"location":"main-using-notebooks.html","title":"Using notebooks","text":"<p>A Jupyter notebook is a way to use Python in an interactive manner. It uses cells that contain Python code, and that are to be executed to immediately see the output, including figures.</p> <p>You can see some rendered notebooks in the examples here, but you can also download them (downward arrow button on the top right corner of each notebook) and run them locally with your own data.</p> <p>To do so, you can either use an integrated development environment (basically a supercharged text editor) that supports Jupyter notebooks, or directly the Jupyter web interface.</p> IDEJupyter web interface <p>You can use for instance Visual Studio Code, also known as vscode.</p> <ol> <li>Download it and install it.</li> <li>Launch vscode.</li> <li>Follow or skip tutorials.</li> <li>In the left panel, open Extension (squared pieces).</li> <li>Install the \"Python\" and \"Jupyter\" extensions (by Microsoft).</li> <li>You now should be able to open .ipynb (notebooks) files with vscode. On the top right, you should be able to Select kernel : choose \"cuisto-env\".</li> </ol> <ol> <li>Create a folder dedicated to working with notebooks, for example \"Documents\\notebooks\".</li> <li>Copy the notebooks you're interested in in this folder.</li> <li>Open a terminal inside this folder (by either using <code>cd Documents\\notebooks</code> or, in the file explorer in your \"notebooks\" folder, Shift+Right Button to \"Open PowerShell window here\")</li> <li>Activate the conda environment : <pre><code>conda activate cuisto-env\n</code></pre></li> <li>Launch the Jupyter Lab web interface : <pre><code>jupyter lab\n</code></pre> This should open a web page where you can open the ipynb files.</li> </ol>"},{"location":"tips-abba.html","title":"ABBA","text":""},{"location":"tips-brain-contours.html","title":"Brain contours","text":"<p>With <code>cuisto</code>, it is possible to plot 2D heatmaps on brain contours.</p> <p>All the detections are projected in a single plane, thus it is up to you to select a relevant data range. It is primarily intended to give a quick, qualitative overview of the spreading of your data.</p> <p>To do so, it requires the brain regions outlines, stored in a hdf5 file. This can be generated with <code>brainglobe-atlasapi</code>. The <code>generate_atlas_outlines.py</code> located in <code>scripts/atlas</code> will show you how to make such a file, that the <code>cuisto.display</code> module can use.</p> <p>Alternatively it is possible to directly plot density maps without <code>cuisto</code>, using <code>brainglobe-heatmap</code>. An example is shown here.</p>"},{"location":"tips-formats.html","title":"Data format","text":""},{"location":"tips-formats.html#some-concepts","title":"Some concepts","text":""},{"location":"tips-formats.html#tiles","title":"Tiles","text":"<p>The representation of an image in a computer is basically a table where each element represents the pixel value (see more here). It can be n-dimensional, where the typical dimensions would be \\((x, y, z)\\), time and the fluorescence channels.</p> <p>In large images, such as histological slices that are more than 10000\\(\\times\\)10000 pixels, a strategy called tiling is used to optimize access to specific regions in the image. Storing the whole image at once in a file would imply to load the whole thing at once in the memory (RAM), even though one would only need to access a given rectangular region with a given zoom. Instead, the image is stored as tiles, small squares (512--2048 pixels) that pave the whole image and are used to reconstruct the original image. Therefore, when zooming-in, only the relevant tiles are loaded and displayed, allowing for smooth large image navigation. This process is done seamlessly by software like QuPath and BigDataViewer (the Fiji plugin ABBA is based on) when loading tiled images. This is also leveraged for image processing in QuPath, which will work on tiles instead of the whole image to not saturate your computer RAM.</p> <p>Most images are already tiled, including Zeiss CZI images. Note that those tiles do not necessarily correspond to the actual, real-world, tiles the microscope did to image the whole slide.</p>"},{"location":"tips-formats.html#pyramids","title":"Pyramids","text":"<p>In the same spirit as tiles, it would be a waste to have to load the entire image (and all the tiles) at once when viewing the image at max zoom-out, as your monitor nor your eyes would handle it. Instead, smaller, rescaled versions of the original image are stored alongside it, and depending on the zoom you are using, the sub-resolution version is displayed. Again, this is done seamlessly by QuPath and ABBA, allowing you to quickly switch from an image to another, without having to load the GB-sized image. Also, for image processing that does not require the original pixel size, QuPath can also leverage pyramids to go faster.</p> <p>Usually, upon openning a CZI file in ZEN, there is a pop-up suggesting you to generate pyramids. It is a very good idea to say yes, wait a bit and save the file so that the pyramidal levels are saved within the file.</p>"},{"location":"tips-formats.html#metadata","title":"Metadata","text":"<p>Metadata, while often overlooked, are of paramount importance in microscopy data. It allows both softwares and users to interpret the raw data of images, eg. the values of each pixels. Most image file formats support this, including the microcope manufacturer file formats. Metadata may include :</p> <ul> <li>Pixel size. Usually expressed in \u00b5m for microscopy, this maps computer pixel units into real world distance. QuPath and ABBA uses that calibration to scale your image properly, so that it match the atlas you'll register your slices on,</li> <li>Channels colors and names,</li> <li>Image type (fluorescence, brightfield, ...),</li> <li>Dimensions,</li> <li>Magnification...</li> </ul> <p>Pixel size is the parameter that is absolutely necessary. Channel names and colors are more a quality of life feature, to make sure not to mix your difference fluorescence channels. CZI files or exported OME-TIFF files include this out of the box so you don't really need to pay attention.</p>"},{"location":"tips-formats.html#bio-formats","title":"Bio-formats","text":"<p>Bio-formats is an initiative of the Open Microscopy Environment (OME) consortium, aiming at being able to read proprietary microscopy image data and metadata. It is used in QuPath, Fiji and ABBA.</p> <p>This page summarizes the level of support of numerous file formats. You can see that Zeiss CZI files and Leica LIF are quite well supported, and should therefore work out of the box in QuPath.</p>"},{"location":"tips-formats.html#zeiss-czi-files","title":"Zeiss CZI files","text":"<p>QuPath and ABBA supports any Bio-formats supported, tiled, pyramidal images.</p> <p>If you're in luck, adding the pyramidal CZI file to your QuPath project will just work. If it doesn't, you'll notice immediately : the tiles will be shuffled and you'll see only a part of the image instead of the whole one. Unfortunately I was not able to determine why this happens and did not find a way to even predict if a file will or will not work.</p> <p>In the event you experience this bug, you'll need to export the CZI files to OME-TIFF files from ZEN, then generate tiled pyramidal images with the <code>pyramid-creator</code> package that you can find here.</p>"},{"location":"tips-formats.html#markdown-md-files","title":"Markdown (.md) files","text":"<p>Markdown is a markup language to create formatted text. It is basically a simple text file that could be opened with any text editor software (notepad and the like), but features specific tags to format the text with heading levels, typesetting (bold, itallic), links, lists... This very page is actually written in markdown, and the engine that builds it renders the text in a nicely formatted manner.</p> <p>If you open a .md file with vscode for example, you'll get a magnigying glass on the top right corner to switch to the rendered version of the file.</p>"},{"location":"tips-formats.html#toml-toml-files","title":"TOML (.toml) files","text":"<p>TOML, or Tom's Obvious Minimal Language, is a configuration file format (similar to YAML). Again, it is basically a simple text file that can be opened with any text editor and is human-readable, but also computer-readable. This means that it is easy for most software and programming language to parse the file to associate a variable (or \"key\") to a value, thus making it a good file format for configuration. It is used in <code>cuisto</code> (see The configuration files page).</p> <p>The syntax looks like this : <pre><code># a comment, ignored by the computer\nkey1 = 10  # the key \"key1\" is mapped to the number 10\nkey2 = \"something\"  # \"key2\" is mapped to the string \"something\"\nkey3 = [\"something else\", 1.10, -25]  # \"key3\" is mapped to a list with 3 elements\n[section]  # we can declare sections\nkey1 = 5  # this is not \"key1\", it actually is section.key1\n[section.example]  # we can have nested sections\nkey1 = true  # this is section.example.key1, mapped to the boolean True\n</code></pre></p> <p>You can check the full specification of this language here.</p>"},{"location":"tips-formats.html#csv-csv-tsv-files","title":"CSV (.csv, .tsv) files","text":"<p>CSV (or TSV) stands for Comma-Separated Values (or Tab-Separated Values) and is, once again, a simple text file formatted in a way that allows LibreOffice Calc (or Excel) to open them as a table. Lines of the table are delimited with new lines, and columns are separated with commas (<code>,</code>) or tabulations. Those files are easily parsed by programming languages (including Python). QuPath can export annotations and detections measurements in TSV format.</p>"},{"location":"tips-formats.html#json-and-geojson-files","title":"JSON and GeoJSON files","text":"<p>JSON is a \"data-interchange format\". It is used to store data, very much like toml, but supports more complex data and is more efficient to read and write, but is less human-readable. It is used in <code>cuisto</code> to store fibers-like objects coordinates, as they can contain several millions of points (making CSV not usable).</p> <p>GeoJson is a file format used to store geographic data structures, basically objects coordinates with various shapes. It is based on and compatible with JSON, which makes it easy to parse in numerous programming language. It used in QuPath to import and export objects, that can be point, line, polygons...</p>"},{"location":"tips-qupath.html","title":"QuPath","text":""},{"location":"tips-qupath.html#custom-scripts","title":"Custom scripts","text":"<p>While QuPath graphical user interface (GUI) should meet a lot of your needs, it is very convenient to use scripting to automate certain tasks, execute them in batch (on all your images) and do things you couldn't do otherwise. QuPath uses the Groovy programming language, which is mostly Java.</p> <p>Warning</p> <p>Not all commands will appear in the history.</p> <p>In QuPath, in the left panel in the \"Workflow\" tab, there is an history of most of the commands you used during the session. On the bottom, you can click on <code>Create workflow</code> to select the relevant commands and create a script. This will open the built-in script editor that will contain the groovy version of what you did graphically.</p> <p>Tip</p> <p>The <code>scripts/qupath-utils</code> folder contains a bunch of utility scripts.</p> <p>They can be run in batch with the three-dotted menu on the bottom right corner of the script editor : <code>Run for project</code>, then choose the images you want the script to run on.</p>"},{"location":"demo_notebooks/cells_distributions.html","title":"Cells distributions","text":"<p>This notebook shows how to load data exported from QuPath, compute metrics and display them, according to the configuration file. This is meant for a single-animal.</p> <p>There are some conventions that need to be met in the QuPath project so that the measurements are usable with <code>cuisto</code>:</p> <ul> <li>Objects' classifications must be derived, eg. be in the form \"something: else\". The primary classification (\"something\") will be refered to \"object_type\" and the secondary classification (\"else\") to \"channel\" in the configuration file.</li> <li>Only one \"object_type\" can be processed at once, but supports any numbers of channels.</li> <li>Annotations (brain regions) must have properly formatted measurements. For punctual objects, it would be the count. Run the \"add_regions_count.groovy\" script to add them. The measurements names must be in the form \"something: else name\", for instance, \"something: else Count\". \"name\" is refered to \"base_measurement\" in the configuration file.</li> </ul> <p>You should copy this notebook, the configuration file and the atlas-related configuration files (blacklist and fusion) elsewhere and edit them according to your need.</p> <p>The data was generated from QuPath with stardist cell detection on toy data.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\n\nimport cuisto\n</pre> import pandas as pd  import cuisto In\u00a0[2]: Copied! <pre># Full path to your configuration file, edited according to your need beforehand\nconfig_file = \"../../resources/demo_config_cells.toml\"\n</pre> # Full path to your configuration file, edited according to your need beforehand config_file = \"../../resources/demo_config_cells.toml\" In\u00a0[3]: Copied! <pre># - Files\n# animal identifier\nanimal = \"animalid0\"\n# set the full path to the annotations tsv file from QuPath\nannotations_file = \"../../resources/cells_measurements_annotations.tsv\"\n# set the full path to the detections tsv file from QuPath\ndetections_file = \"../../resources/cells_measurements_detections.tsv\"\n</pre> # - Files # animal identifier animal = \"animalid0\" # set the full path to the annotations tsv file from QuPath annotations_file = \"../../resources/cells_measurements_annotations.tsv\" # set the full path to the detections tsv file from QuPath detections_file = \"../../resources/cells_measurements_detections.tsv\" In\u00a0[4]: Copied! <pre># get configuration\ncfg = cuisto.config.Config(config_file)\n</pre> # get configuration cfg = cuisto.config.Config(config_file) In\u00a0[5]: Copied! <pre># read data\ndf_annotations = pd.read_csv(annotations_file, index_col=\"Object ID\", sep=\"\\t\")\ndf_detections = pd.read_csv(detections_file, index_col=\"Object ID\", sep=\"\\t\")\n\n# remove annotations that are not brain regions\ndf_annotations = df_annotations[df_annotations[\"Classification\"] != \"Region*\"]\ndf_annotations = df_annotations[df_annotations[\"ROI\"] != \"Rectangle\"]\n\n# convert atlas coordinates from mm to microns\ndf_detections[[\"Atlas_X\", \"Atlas_Y\", \"Atlas_Z\"]] = df_detections[\n    [\"Atlas_X\", \"Atlas_Y\", \"Atlas_Z\"]\n].multiply(1000)\n\n# have a look\ndisplay(df_annotations.head())\ndisplay(df_detections.head())\n</pre> # read data df_annotations = pd.read_csv(annotations_file, index_col=\"Object ID\", sep=\"\\t\") df_detections = pd.read_csv(detections_file, index_col=\"Object ID\", sep=\"\\t\")  # remove annotations that are not brain regions df_annotations = df_annotations[df_annotations[\"Classification\"] != \"Region*\"] df_annotations = df_annotations[df_annotations[\"ROI\"] != \"Rectangle\"]  # convert atlas coordinates from mm to microns df_detections[[\"Atlas_X\", \"Atlas_Y\", \"Atlas_Z\"]] = df_detections[     [\"Atlas_X\", \"Atlas_Y\", \"Atlas_Z\"] ].multiply(1000)  # have a look display(df_annotations.head()) display(df_detections.head()) Image Object type Name Classification Parent ROI Centroid X \u00b5m Centroid Y \u00b5m Cells: marker+ Count Cells: marker- Count ID Side Parent ID Num Detections Num Cells: marker+ Num Cells: marker- Area \u00b5m^2 Perimeter \u00b5m Object ID 4781ed63-0d8e-422e-aead-b685fbe20eb5 animalid0_030.ome.tiff Annotation Root NaN Root object (Image) Geometry 5372.5 3922.1 0 0 NaN NaN NaN 2441 136 2305 31666431.6 37111.9 aa4b133d-13f9-42d9-8c21-45f143b41a85 animalid0_030.ome.tiff Annotation root Right: root Root Polygon 7094.9 4085.7 0 0 997 0.0 NaN 1284 41 1243 15882755.9 18819.5 42c3b914-91c5-4b65-a603-3f9431717d48 animalid0_030.ome.tiff Annotation grey Right: grey root Geometry 7256.8 4290.6 0 0 8 0.0 997.0 1009 24 985 12026268.7 49600.3 887af3eb-4061-4f8a-aa4c-fe9b81184061 animalid0_030.ome.tiff Annotation CB Right: CB grey Geometry 7778.7 3679.2 0 16 512 0.0 8.0 542 5 537 6943579.0 30600.2 adaabc05-36d1-4aad-91fe-2e904adc574f animalid0_030.ome.tiff Annotation CBN Right: CBN CB Geometry 6790.5 3567.9 0 0 519 0.0 512.0 55 1 54 864212.3 7147.4 Image Object type Name Classification Parent ROI Atlas_X Atlas_Y Atlas_Z Object ID 5ff386a8-5abd-46d1-8e0d-f5c5365457c1 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11523.0 4272.4 4276.7 9a2a9a8c-acbe-4308-bc5e-f3c9fd1754c0 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11520.2 4278.4 4418.6 481a519b-8b40-4450-9ec6-725181807d72 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11506.0 4317.2 4356.3 fd28e09c-2c64-4750-b026-cd99e3526a57 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11528.4 4257.4 4336.4 3d9ce034-f2ed-4c73-99be-f782363cf323 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11548.7 4203.3 4294.3 In\u00a0[6]: Copied! <pre># get distributions per regions, spatial distributions and coordinates\ndf_regions, dfs_distributions, df_coordinates = cuisto.process.process_animal(\n    animal, df_annotations, df_detections, cfg, compute_distributions=True\n)\n\n# have a look\ndisplay(df_regions.head())\ndisplay(df_coordinates.head())\n</pre> # get distributions per regions, spatial distributions and coordinates df_regions, dfs_distributions, df_coordinates = cuisto.process.process_animal(     animal, df_annotations, df_detections, cfg, compute_distributions=True )  # have a look display(df_regions.head()) display(df_coordinates.head()) Name hemisphere Area \u00b5m^2 Area mm^2 count density \u00b5m^-2 density mm^-2 coverage index relative count relative density channel animal 0 ACVII Left 8307.1 0.008307 1 0.00012 120.378953 0.00012 0.002132 0.205275 Positive animalid0 0 ACVII Left 8307.1 0.008307 1 0.00012 120.378953 0.00012 0.000189 0.020671 Negative animalid0 1 ACVII Right 7061.4 0.007061 0 0.0 0.0 0.0 0.0 0.0 Positive animalid0 1 ACVII Right 7061.4 0.007061 1 0.000142 141.614977 0.000142 0.000144 0.021646 Negative animalid0 2 ACVII both 15368.5 0.015369 1 0.000065 65.068159 0.000065 0.001362 0.153797 Positive animalid0 Image Object type Name Classification Parent ROI Atlas_X Atlas_Y Atlas_Z hemisphere channel Atlas_AP Atlas_DV Atlas_ML animal Object ID 5ff386a8-5abd-46d1-8e0d-f5c5365457c1 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11.5230 4.2724 4.2767 Right Negative -6.433716 3.098278 -1.4233 animalid0 9a2a9a8c-acbe-4308-bc5e-f3c9fd1754c0 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11.5202 4.2784 4.4186 Right Negative -6.431449 3.104147 -1.2814 animalid0 481a519b-8b40-4450-9ec6-725181807d72 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11.5060 4.3172 4.3563 Right Negative -6.420685 3.141780 -1.3437 animalid0 fd28e09c-2c64-4750-b026-cd99e3526a57 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11.5284 4.2574 4.3364 Right Negative -6.437788 3.083737 -1.3636 animalid0 3d9ce034-f2ed-4c73-99be-f782363cf323 animalid0_030.ome.tiff Detection NaN Cells: marker- VeCB Polygon 11.5487 4.2033 4.2943 Right Negative -6.453296 3.031224 -1.4057 animalid0 In\u00a0[7]: Copied! <pre># plot distributions per regions\nfigs_regions = cuisto.display.plot_regions(df_regions, cfg)\n# specify which regions to plot\n# figs_regions = cuisto.display.plot_regions(df_regions, cfg, names_list=[\"GRN\", \"IRN\", \"MDRNv\"])\n\n# save as svg\n# figs_regions[0].savefig(r\"C:\\Users\\glegoc\\Downloads\\regions_count.svg\")\n# figs_regions[1].savefig(r\"C:\\Users\\glegoc\\Downloads\\regions_density.svg\")\n</pre> # plot distributions per regions figs_regions = cuisto.display.plot_regions(df_regions, cfg) # specify which regions to plot # figs_regions = cuisto.display.plot_regions(df_regions, cfg, names_list=[\"GRN\", \"IRN\", \"MDRNv\"])  # save as svg # figs_regions[0].savefig(r\"C:\\Users\\glegoc\\Downloads\\regions_count.svg\") # figs_regions[1].savefig(r\"C:\\Users\\glegoc\\Downloads\\regions_density.svg\") In\u00a0[8]: Copied! <pre># plot 1D distributions\nfig_distrib = cuisto.display.plot_1D_distributions(\n    dfs_distributions, cfg, df_coordinates=df_coordinates\n)\n</pre> # plot 1D distributions fig_distrib = cuisto.display.plot_1D_distributions(     dfs_distributions, cfg, df_coordinates=df_coordinates ) <p>If there were several <code>animal</code> in the measurement file, it would be displayed as mean +/- sem instead.</p> In\u00a0[9]: Copied! <pre># plot heatmap (all types of cells pooled)\nfig_heatmap = cuisto.display.plot_2D_distributions(df_coordinates, cfg)\n</pre> # plot heatmap (all types of cells pooled) fig_heatmap = cuisto.display.plot_2D_distributions(df_coordinates, cfg)"},{"location":"demo_notebooks/density_map.html","title":"Density map","text":"<p>Draw 2D heatmaps as density isolines.</p> <p>This notebook does not actually use <code>histoquant</code> and relies only on brainglobe-heatmap to extract brain structures outlines.</p> <p>Only the detections measurements with atlas coordinates exported from QuPath are used.</p> <p>You need to select the range of data to be used, the regions outlines will be extracted at the centroid of that range. Therefore, a range that is too large will be misleading and irrelevant.</p> In\u00a0[1]: Copied! <pre>import brainglobe_heatmap as bgh\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n</pre> import brainglobe_heatmap as bgh import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns In\u00a0[2]: Copied! <pre># path to the exported measurements from QuPath\nfilename = \"../../resources/cells_measurements_detections.tsv\"\n</pre> # path to the exported measurements from QuPath filename = \"../../resources/cells_measurements_detections.tsv\" <p>Settings</p> In\u00a0[3]: Copied! <pre># atlas to use\natlas_name = \"allen_mouse_10um\"\n# brain regions whose outlines will be plotted\nregions = [\"root\", \"CB\", \"MY\", \"GRN\", \"IRN\"]\n# range to include, in Allen coordinates, in microns\nap_lims = [9800, 10000]  # lims : [0, 13200] for coronal\nml_lims = [5600, 5800]  # lims : [0, 11400] for sagittal\ndv_lims = [3900, 4100]  # lims : [0, 8000] for top\n# number of isolines\nnlevels = 5\n# color mapping between classification and matplotlib color\npalette = {\"Cells: marker-\": \"#d8782f\", \"Cells: marker+\": \"#8ccb73\"}\n</pre> # atlas to use atlas_name = \"allen_mouse_10um\" # brain regions whose outlines will be plotted regions = [\"root\", \"CB\", \"MY\", \"GRN\", \"IRN\"] # range to include, in Allen coordinates, in microns ap_lims = [9800, 10000]  # lims : [0, 13200] for coronal ml_lims = [5600, 5800]  # lims : [0, 11400] for sagittal dv_lims = [3900, 4100]  # lims : [0, 8000] for top # number of isolines nlevels = 5 # color mapping between classification and matplotlib color palette = {\"Cells: marker-\": \"#d8782f\", \"Cells: marker+\": \"#8ccb73\"} In\u00a0[4]: Copied! <pre>df = pd.read_csv(filename, sep=\"\\t\")\ndisplay(df.head())\n</pre> df = pd.read_csv(filename, sep=\"\\t\") display(df.head()) <pre></pre> Image Object ID Object type Name Classification Parent ROI Atlas_X Atlas_Y Atlas_Z 0 animalid0_030.ome.tiff 5ff386a8-5abd-46d1-8e0d-f5c5365457c1 Detection NaN Cells: marker- VeCB Polygon 11.5230 4.2724 4.2767 1 animalid0_030.ome.tiff 9a2a9a8c-acbe-4308-bc5e-f3c9fd1754c0 Detection NaN Cells: marker- VeCB Polygon 11.5202 4.2784 4.4186 2 animalid0_030.ome.tiff 481a519b-8b40-4450-9ec6-725181807d72 Detection NaN Cells: marker- VeCB Polygon 11.5060 4.3172 4.3563 3 animalid0_030.ome.tiff fd28e09c-2c64-4750-b026-cd99e3526a57 Detection NaN Cells: marker- VeCB Polygon 11.5284 4.2574 4.3364 4 animalid0_030.ome.tiff 3d9ce034-f2ed-4c73-99be-f782363cf323 Detection NaN Cells: marker- VeCB Polygon 11.5487 4.2033 4.2943 <p>Here we can filter out classifications we don't wan't to display.</p> In\u00a0[5]: Copied! <pre># select objects\n# df = df[df[\"Classification\"] == \"example: classification\"]\n</pre> # select objects # df = df[df[\"Classification\"] == \"example: classification\"] In\u00a0[6]: Copied! <pre># get outline coordinates in coronal (=frontal) orientation\ncoords_coronal = bgh.get_structures_slice_coords(\n    regions,\n    orientation=\"frontal\",\n    atlas_name=atlas_name,\n    position=(np.mean(ap_lims), 0, 0),\n)\n# get outline coordinates in sagittal orientation\ncoords_sagittal = bgh.get_structures_slice_coords(\n    regions,\n    orientation=\"sagittal\",\n    atlas_name=atlas_name,\n    position=(0, 0, np.mean(ml_lims)),\n)\n# get outline coordinates in top (=horizontal) orientation\ncoords_top = bgh.get_structures_slice_coords(\n    regions,\n    orientation=\"horizontal\",\n    atlas_name=atlas_name,\n    position=(0, np.mean(dv_lims), 0),\n)\n</pre> # get outline coordinates in coronal (=frontal) orientation coords_coronal = bgh.get_structures_slice_coords(     regions,     orientation=\"frontal\",     atlas_name=atlas_name,     position=(np.mean(ap_lims), 0, 0), ) # get outline coordinates in sagittal orientation coords_sagittal = bgh.get_structures_slice_coords(     regions,     orientation=\"sagittal\",     atlas_name=atlas_name,     position=(0, 0, np.mean(ml_lims)), ) # get outline coordinates in top (=horizontal) orientation coords_top = bgh.get_structures_slice_coords(     regions,     orientation=\"horizontal\",     atlas_name=atlas_name,     position=(0, np.mean(dv_lims), 0), ) In\u00a0[7]: Copied! <pre># Coronal projection\n# select objects within the rostro-caudal range\ndf_coronal = df[\n    (df[\"Atlas_X\"] &gt;= ap_lims[0] / 1000) &amp; (df[\"Atlas_X\"] &lt;= ap_lims[1] / 1000)\n]\n\nplt.figure()\n\nfor struct_name, contours in coords_coronal.items():\n    for cont in contours:\n        plt.fill(cont[:, 0] / 1000, cont[:, 1] / 1000, lw=1, fc=\"none\", ec=\"k\")\n\n# see https://seaborn.pydata.org/generated/seaborn.kdeplot.html to customize\nax = sns.kdeplot(\n    df_coronal,\n    x=\"Atlas_Z\",\n    y=\"Atlas_Y\",\n    hue=\"Classification\",\n    levels=nlevels,\n    common_norm=False,\n    palette=palette,\n)\nax.invert_yaxis()\nsns.despine(left=True, bottom=True)\nplt.axis(\"equal\")\nplt.xlabel(None)\nplt.ylabel(None)\nplt.xticks([])\nplt.yticks([])\nplt.plot([2, 3], [8, 8], \"k\", linewidth=3)\nplt.text(2, 7.9, \"1 mm\")\n</pre> # Coronal projection # select objects within the rostro-caudal range df_coronal = df[     (df[\"Atlas_X\"] &gt;= ap_lims[0] / 1000) &amp; (df[\"Atlas_X\"] &lt;= ap_lims[1] / 1000) ]  plt.figure()  for struct_name, contours in coords_coronal.items():     for cont in contours:         plt.fill(cont[:, 0] / 1000, cont[:, 1] / 1000, lw=1, fc=\"none\", ec=\"k\")  # see https://seaborn.pydata.org/generated/seaborn.kdeplot.html to customize ax = sns.kdeplot(     df_coronal,     x=\"Atlas_Z\",     y=\"Atlas_Y\",     hue=\"Classification\",     levels=nlevels,     common_norm=False,     palette=palette, ) ax.invert_yaxis() sns.despine(left=True, bottom=True) plt.axis(\"equal\") plt.xlabel(None) plt.ylabel(None) plt.xticks([]) plt.yticks([]) plt.plot([2, 3], [8, 8], \"k\", linewidth=3) plt.text(2, 7.9, \"1 mm\") <pre></pre> Out[7]: <pre>Text(2, 7.9, '1 mm')</pre> <pre></pre> In\u00a0[8]: Copied! <pre># Sagittal projection\n# select objects within the medio-lateral range\ndf_sagittal = df[\n    (df[\"Atlas_Z\"] &gt;= ml_lims[0] / 1000) &amp; (df[\"Atlas_Z\"] &lt;= ml_lims[1] / 1000)\n]\n\nplt.figure()\n\nfor struct_name, contours in coords_sagittal.items():\n    for cont in contours:\n        plt.fill(cont[:, 0] / 1000, cont[:, 1] / 1000, lw=1, fc=\"none\", ec=\"k\")\n\n# see https://seaborn.pydata.org/generated/seaborn.kdeplot.html to customize\nax = sns.kdeplot(\n    df_sagittal,\n    x=\"Atlas_X\",\n    y=\"Atlas_Y\",\n    hue=\"Classification\",\n    levels=nlevels,\n    common_norm=False,\n    palette=palette,\n)\nax.invert_yaxis()\nsns.despine(left=True, bottom=True)\nplt.axis(\"equal\")\nplt.xlabel(None)\nplt.ylabel(None)\nplt.xticks([])\nplt.yticks([])\nplt.plot([2, 3], [7.1, 7.1], \"k\", linewidth=3)\nplt.text(2, 7, \"1 mm\")\n</pre> # Sagittal projection # select objects within the medio-lateral range df_sagittal = df[     (df[\"Atlas_Z\"] &gt;= ml_lims[0] / 1000) &amp; (df[\"Atlas_Z\"] &lt;= ml_lims[1] / 1000) ]  plt.figure()  for struct_name, contours in coords_sagittal.items():     for cont in contours:         plt.fill(cont[:, 0] / 1000, cont[:, 1] / 1000, lw=1, fc=\"none\", ec=\"k\")  # see https://seaborn.pydata.org/generated/seaborn.kdeplot.html to customize ax = sns.kdeplot(     df_sagittal,     x=\"Atlas_X\",     y=\"Atlas_Y\",     hue=\"Classification\",     levels=nlevels,     common_norm=False,     palette=palette, ) ax.invert_yaxis() sns.despine(left=True, bottom=True) plt.axis(\"equal\") plt.xlabel(None) plt.ylabel(None) plt.xticks([]) plt.yticks([]) plt.plot([2, 3], [7.1, 7.1], \"k\", linewidth=3) plt.text(2, 7, \"1 mm\") <pre></pre> Out[8]: <pre>Text(2, 7, '1 mm')</pre> <pre></pre> In\u00a0[9]: Copied! <pre># Top projection\n# select objects within the dorso-ventral range\ndf_top = df[(df[\"Atlas_Y\"] &gt;= dv_lims[0] / 1000) &amp; (df[\"Atlas_Y\"] &lt;= dv_lims[1] / 1000)]\n\nplt.figure()\n\nfor struct_name, contours in coords_top.items():\n    for cont in contours:\n        plt.fill(-cont[:, 0] / 1000, cont[:, 1] / 1000, lw=1, fc=\"none\", ec=\"k\")\n\n# see https://seaborn.pydata.org/generated/seaborn.kdeplot.html to customize\nax = sns.kdeplot(\n    df_top,\n    x=\"Atlas_Z\",\n    y=\"Atlas_X\",\n    hue=\"Classification\",\n    levels=nlevels,\n    common_norm=False,\n    palette=palette,\n)\nax.invert_yaxis()\nsns.despine(left=True, bottom=True)\nplt.axis(\"equal\")\nplt.xlabel(None)\nplt.ylabel(None)\nplt.xticks([])\nplt.yticks([])\nplt.plot([0.5, 1.5], [0.5, 0.5], \"k\", linewidth=3)\nplt.text(0.5, 0.4, \"1 mm\")\n</pre> # Top projection # select objects within the dorso-ventral range df_top = df[(df[\"Atlas_Y\"] &gt;= dv_lims[0] / 1000) &amp; (df[\"Atlas_Y\"] &lt;= dv_lims[1] / 1000)]  plt.figure()  for struct_name, contours in coords_top.items():     for cont in contours:         plt.fill(-cont[:, 0] / 1000, cont[:, 1] / 1000, lw=1, fc=\"none\", ec=\"k\")  # see https://seaborn.pydata.org/generated/seaborn.kdeplot.html to customize ax = sns.kdeplot(     df_top,     x=\"Atlas_Z\",     y=\"Atlas_X\",     hue=\"Classification\",     levels=nlevels,     common_norm=False,     palette=palette, ) ax.invert_yaxis() sns.despine(left=True, bottom=True) plt.axis(\"equal\") plt.xlabel(None) plt.ylabel(None) plt.xticks([]) plt.yticks([]) plt.plot([0.5, 1.5], [0.5, 0.5], \"k\", linewidth=3) plt.text(0.5, 0.4, \"1 mm\") <pre></pre> Out[9]: <pre>Text(0.5, 0.4, '1 mm')</pre> <pre></pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demo_notebooks/fibers_coverage.html","title":"Fibers coverage","text":"<p>Plot regions coverage percentage in the spinal cord.</p> <p>This showcases that any brainglobe atlases should be supported.</p> <p>Here we're going to quantify the percentage of area of each spinal cord regions innervated by axons.</p> <p>The \"area \u00b5m^2\" measurement for each annotations can be created in QuPath with a pixel classifier, using the Measure button.</p> <p>We're going to consider that the \"area \u00b5m^2\" measurement generated by the pixel classifier is an object count. <code>histoquant</code> computes a density, which is the count in each region divided by its aera. Therefore, in  this case, it will be actually the fraction of area covered by fibers in a given color.</p> <p>The data was generated using QuPath with a pixel classifier on toy data.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\n\nimport cuisto\n</pre> import pandas as pd  import cuisto In\u00a0[2]: Copied! <pre># Full path to your configuration file, edited according to your need beforehand\nconfig_file = \"../../resources/demo_config_fibers.toml\"\n</pre> # Full path to your configuration file, edited according to your need beforehand config_file = \"../../resources/demo_config_fibers.toml\" In\u00a0[3]: Copied! <pre># - Files\n# not important if only one animal\nanimal = \"animalid1-SC\"\n# set the full path to the annotations tsv file from QuPath\nannotations_file = \"../../resources/fibers_measurements_annotations.tsv\"\n</pre> # - Files # not important if only one animal animal = \"animalid1-SC\" # set the full path to the annotations tsv file from QuPath annotations_file = \"../../resources/fibers_measurements_annotations.tsv\" In\u00a0[4]: Copied! <pre># get configuration\ncfg = cuisto.config.Config(config_file)\n</pre> # get configuration cfg = cuisto.config.Config(config_file) In\u00a0[5]: Copied! <pre># read data\ndf_annotations = pd.read_csv(annotations_file, index_col=\"Object ID\", sep=\"\\t\")\ndf_detections = pd.DataFrame()  # empty DataFrame\n\n# remove annotations that are not brain regions\ndf_annotations = df_annotations[df_annotations[\"Classification\"] != \"Region*\"]\ndf_annotations = df_annotations[df_annotations[\"ROI\"] != \"Rectangle\"]\n\n# have a look\ndisplay(df_annotations.head())\n</pre> # read data df_annotations = pd.read_csv(annotations_file, index_col=\"Object ID\", sep=\"\\t\") df_detections = pd.DataFrame()  # empty DataFrame  # remove annotations that are not brain regions df_annotations = df_annotations[df_annotations[\"Classification\"] != \"Region*\"] df_annotations = df_annotations[df_annotations[\"ROI\"] != \"Rectangle\"]  # have a look display(df_annotations.head()) Image Object type Name Classification Parent ROI Centroid X \u00b5m Centroid Y \u00b5m Fibers: EGFP area \u00b5m^2 Fibers: DsRed area \u00b5m^2 ID Side Parent ID Area \u00b5m^2 Perimeter \u00b5m Object ID dcfe5196-4e8d-4126-b255-a9ea393c383a animalid1-SC_s1.ome.tiff Annotation Root NaN Root object (Image) Geometry 1353.70 1060.00 108993.1953 15533.3701 NaN NaN NaN 3172474.0 9853.3 acc74bc0-3dd0-4b3e-86e3-e6c7b681d544 animalid1-SC_s1.ome.tiff Annotation root Right: root Root Polygon 864.44 989.95 39162.8906 5093.2798 250.0 0.0 NaN 1603335.7 4844.2 94571cf9-f22b-453f-860c-eb13d0e72440 animalid1-SC_s1.ome.tiff Annotation WM Right: WM root Geometry 791.00 1094.60 20189.0469 2582.4824 130.0 0.0 250.0 884002.0 7927.8 473d65fb-fda4-4721-ba6f-cc659efc1d5a animalid1-SC_s1.ome.tiff Annotation vf Right: vf WM Polygon 984.31 1599.00 6298.3574 940.4100 70.0 0.0 130.0 281816.9 2719.5 449e2cd1-eca2-4708-83fe-651f378c3a14 animalid1-SC_s1.ome.tiff Annotation df Right: df WM Polygon 1242.90 401.26 1545.0750 241.3800 74.0 0.0 130.0 152952.8 1694.4 In\u00a0[6]: Copied! <pre># get distributions per regions, spatial distributions and coordinates\ndf_regions, dfs_distributions, df_coordinates = cuisto.process.process_animal(\n    animal, df_annotations, df_detections, cfg, compute_distributions=False\n)\n\n# convert the \"density \u00b5m^-2\" column, which is actually the coverage fraction, to a percentage\ndf_regions[\"density \u00b5m^-2\"] = df_regions[\"density \u00b5m^-2\"] * 100\n\n# have a look\ndisplay(df_regions.head())\n</pre> # get distributions per regions, spatial distributions and coordinates df_regions, dfs_distributions, df_coordinates = cuisto.process.process_animal(     animal, df_annotations, df_detections, cfg, compute_distributions=False )  # convert the \"density \u00b5m^-2\" column, which is actually the coverage fraction, to a percentage df_regions[\"density \u00b5m^-2\"] = df_regions[\"density \u00b5m^-2\"] * 100  # have a look display(df_regions.head()) Name hemisphere Area \u00b5m^2 Area mm^2 area \u00b5m^2 area mm^2 density \u00b5m^-2 density mm^-2 coverage index relative count relative density channel animal 0 10Sp Contra. 1749462.18 1.749462 53117.3701 53.11737 3.036211 30362.113973 1612.755645 0.036535 0.033062 Negative animalid1-SC 0 10Sp Contra. 1749462.18 1.749462 5257.1025 5.257103 0.300498 3004.98208 15.797499 0.030766 0.02085 Positive animalid1-SC 1 10Sp Ipsi. 1439105.93 1.439106 64182.9823 64.182982 4.459921 44599.206328 2862.51007 0.023524 0.023265 Negative animalid1-SC 1 10Sp Ipsi. 1439105.93 1.439106 8046.3375 8.046337 0.559121 5591.205854 44.988729 0.028911 0.022984 Positive animalid1-SC 2 10Sp both 3188568.11 3.188568 117300.3524 117.300352 3.678778 36787.783216 4315.219935 0.028047 0.025734 Negative animalid1-SC In\u00a0[7]: Copied! <pre># plot distributions per regions\nfig_regions = cuisto.display.plot_regions(df_regions, cfg)\n# specify which regions to plot\n# fig_regions = hq.display.plot_regions(df_regions, cfg, names_list=[\"Rh9\", \"Sr9\", \"8Sp\"])\n\n# save as svg\n# fig_regions[0].savefig(r\"C:\\Users\\glegoc\\Downloads\\nice_figure.svg\")\n</pre> # plot distributions per regions fig_regions = cuisto.display.plot_regions(df_regions, cfg) # specify which regions to plot # fig_regions = hq.display.plot_regions(df_regions, cfg, names_list=[\"Rh9\", \"Sr9\", \"8Sp\"])  # save as svg # fig_regions[0].savefig(r\"C:\\Users\\glegoc\\Downloads\\nice_figure.svg\")"},{"location":"demo_notebooks/fibers_length_multi.html","title":"Fibers length in multi animals","text":"In\u00a0[1]: Copied! <pre>import cuisto\n</pre> import cuisto In\u00a0[2]: Copied! <pre># Full path to your configuration file, edited according to your need beforehand\nconfig_file = \"../../resources/demo_config_multi.toml\"\n</pre> # Full path to your configuration file, edited according to your need beforehand config_file = \"../../resources/demo_config_multi.toml\" In\u00a0[3]: Copied! <pre># Files\nwdir = \"../../resources/multi\"\nanimals = [\"mouse0\", \"mouse1\"]\n</pre> # Files wdir = \"../../resources/multi\" animals = [\"mouse0\", \"mouse1\"] In\u00a0[4]: Copied! <pre># get configuration\ncfg = cuisto.Config(config_file)\n</pre> # get configuration cfg = cuisto.Config(config_file) In\u00a0[5]: Copied! <pre># get distributions per regions\ndf_regions, _, _ = cuisto.process.process_animals(\n    wdir, animals, cfg, compute_distributions=False\n)\n\n# have a look\ndisplay(df_regions.head(10))\n</pre> # get distributions per regions df_regions, _, _ = cuisto.process.process_animals(     wdir, animals, cfg, compute_distributions=False )  # have a look display(df_regions.head(10)) <pre>Processing mouse1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00, 15.66it/s]\n</pre> Name hemisphere Area \u00b5m^2 Area mm^2 length \u00b5m length mm density \u00b5m^-1 density mm^-1 coverage index relative count relative density channel animal 0 ACVII Contra. 9099.04 0.009099 468.0381 0.468038 0.051438 51438.184688 24.07503 0.00064 0.022168 marker3 mouse0 1 ACVII Contra. 9099.04 0.009099 4260.4844 4.260484 0.468234 468234.495068 1994.905762 0.0019 0.056502 marker2 mouse0 2 ACVII Contra. 9099.04 0.009099 5337.7103 5.33771 0.586623 586623.45698 3131.226069 0.010104 0.242734 marker1 mouse0 3 ACVII Ipsi. 4609.90 0.004610 0.0 0.0 0.0 0.0 0.0 0.0 0.0 marker3 mouse0 4 ACVII Ipsi. 4609.90 0.004610 0.0 0.0 0.0 0.0 0.0 0.0 0.0 marker2 mouse0 5 ACVII Ipsi. 4609.90 0.004610 0.0 0.0 0.0 0.0 0.0 0.0 0.0 marker1 mouse0 6 ACVII both 13708.94 0.013709 468.0381 0.468038 0.034141 34141.086036 15.979329 0.000284 0.011001 marker3 mouse0 7 ACVII both 13708.94 0.013709 4260.4844 4.260484 0.310781 310781.460857 1324.079566 0.000934 0.030688 marker2 mouse0 8 ACVII both 13708.94 0.013709 5337.7103 5.33771 0.38936 389359.811918 2078.289878 0.00534 0.142623 marker1 mouse0 9 AMB Contra. 122463.80 0.122464 30482.7815 30.482782 0.248913 248912.588863 7587.548059 0.041712 0.107271 marker3 mouse0 In\u00a0[6]: Copied! <pre>figs_regions = cuisto.display.plot_regions(df_regions, cfg)\n</pre> figs_regions = cuisto.display.plot_regions(df_regions, cfg)"},{"location":"demo_notebooks/fibers_length_multi.html#fibers-length-in-multi-animals","title":"Fibers length in multi animals\u00b6","text":"<p>This example uses synthetic data to showcase how <code>histoquant</code> can be used in a pipeline.</p> <p>Annotations measurements should be exported from QuPath, following the required directory structure.</p> <p>Alternatively, you can merge all your CSV files yourself, one per animal, adding an animal ID to each table. Those can be processed with the <code>histoquant.process.process_animal()</code> function, in a loop, collecting the results at each iteration and finally concatenating the results. Finally, those can be used with <code>display</code> module. See the API reference for the <code>process</code> module.</p>"}]}